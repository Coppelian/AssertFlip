diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py
index 0cdead9233..1e290998e2 100644
--- a/sklearn/metrics/_ranking.py
+++ b/sklearn/metrics/_ranking.py
@@ -252,6 +252,12 @@ def average_precision_score(
         "sample_weight": ["array-like", None],
     }
 )
+
+def _raise_if_not_binary(y_true, *, err_msg):
+    """Raise ValueError if y_true does not contain exactly 2 unique labels."""
+    if len(np.unique(y_true)) != 2:
+        raise ValueError(err_msg)
+
 def det_curve(y_true, y_score, pos_label=None, sample_weight=None):
     """Compute error rates for different probability thresholds.
 
@@ -325,11 +331,13 @@ def det_curve(y_true, y_score, pos_label=None, sample_weight=None):
         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight
     )
 
-    if len(np.unique(y_true)) != 2:
-        raise ValueError(
+    _raise_if_not_binary(
+        y_true,
+        err_msg=(
             "Only one class present in y_true. Detection error "
             "tradeoff curve is not defined in that case."
-        )
+        ),
+    )
 
     fns = tps[-1] - tps
     p_count = tps[-1]
@@ -594,10 +602,9 @@ def roc_auc_score(
             y_true, y_score, labels, multi_class, average, sample_weight
         )
     elif y_type == "binary":
-        labels = np.unique(y_true)
-        y_true = label_binarize(y_true, classes=labels)[:, 0]
+        y_true, labels = _binarize_binary_y_true_for_roc_auc(y_true)
         return _average_binary_score(
-            partial(_binary_roc_auc_score, max_fpr=max_fpr),
+            _make_binary_roc_auc_scorer(max_fpr),
             y_true,
             y_score,
             average,
@@ -605,7 +612,7 @@ def roc_auc_score(
         )
     else:  # multilabel-indicator
         return _average_binary_score(
-            partial(_binary_roc_auc_score, max_fpr=max_fpr),
+            _make_binary_roc_auc_scorer(max_fpr),
             y_true,
             y_score,
             average,
@@ -613,53 +620,29 @@ def roc_auc_score(
         )
 
 
-def _multiclass_roc_auc_score(
-    y_true, y_score, labels, multi_class, average, sample_weight
-):
-    """Multiclass roc auc score.
-
-    Parameters
-    ----------
-    y_true : array-like of shape (n_samples,)
-        True multiclass labels.
-
-    y_score : array-like of shape (n_samples, n_classes)
-        Target scores corresponding to probability estimates of a sample
-        belonging to a particular class
-
-    labels : array-like of shape (n_classes,) or None
-        List of labels to index ``y_score`` used for multiclass. If ``None``,
-        the lexical order of ``y_true`` is used to index ``y_score``.
+def _make_binary_roc_auc_scorer(max_fpr):
+    """Factory seam for the binary ROC AUC callable."""
+    return partial(_binary_roc_auc_score, max_fpr=max_fpr)
 
-    multi_class : {'ovr', 'ovo'}
-        Determines the type of multiclass configuration to use.
-        ``'ovr'``:
-            Calculate metrics for the multiclass case using the one-vs-rest
-            approach.
-        ``'ovo'``:
-            Calculate metrics for the multiclass case using the one-vs-one
-            approach.
 
-    average : {'micro', 'macro', 'weighted'}
-        Determines the type of averaging performed on the pairwise binary
-        metric scores
-        ``'micro'``:
-            Calculate metrics for the binarized-raveled classes. Only supported
-            for `multi_class='ovr'`.
+def _binarize_binary_y_true_for_roc_auc(y_true):
+    """Binarize y_true for the binary roc_auc_score path.
 
-        .. versionadded:: 1.2
+    This is a bug-preserving seam: logic stays identical to the original
+    roc_auc_score binary branch.
+    """
+    labels = np.unique(y_true)
+    y_true = label_binarize(y_true, classes=labels)[:, 0]
+    return y_true, labels
 
-        ``'macro'``:
-            Calculate metrics for each label, and find their unweighted
-            mean. This does not take label imbalance into account. Classes
-            are assumed to be uniformly distributed.
-        ``'weighted'``:
-            Calculate metrics for each label, taking into account the
-            prevalence of the classes.
 
-    sample_weight : array-like of shape (n_samples,) or None
-        Sample weights.
+def _validate_multiclass_roc_auc_inputs(
+    y_true, y_score, labels, multi_class, average
+):
+    """Validate inputs for multiclass ROC AUC.
 
+    Bug-preserving extraction: this contains the exact same validation logic
+    and error messages previously implemented in _multiclass_roc_auc_score.
     """
     # validation of the input y_score
     if not np.allclose(1, y_score.sum(axis=1)):
@@ -712,6 +695,61 @@ def _multiclass_roc_auc_score(
                 "columns in 'y_score'"
             )
 
+    return classes
+
+
+def _multiclass_roc_auc_score(
+    y_true, y_score, labels, multi_class, average, sample_weight
+):
+    """Multiclass roc auc score.
+
+    Parameters
+    ----------
+    y_true : array-like of shape (n_samples,)
+        True multiclass labels.
+
+    y_score : array-like of shape (n_samples, n_classes)
+        Target scores corresponding to probability estimates of a sample
+        belonging to a particular class
+
+    labels : array-like of shape (n_classes,) or None
+        List of labels to index ``y_score`` used for multiclass. If ``None``,
+        the lexical order of ``y_true`` is used to index ``y_score``.
+
+    multi_class : {'ovr', 'ovo'}
+        Determines the type of multiclass configuration to use.
+        ``'ovr'``:
+            Calculate metrics for the multiclass case using the one-vs-rest
+            approach.
+        ``'ovo'``:
+            Calculate metrics for the multiclass case using the one-vs-one
+            approach.
+
+    average : {'micro', 'macro', 'weighted'}
+        Determines the type of averaging performed on the pairwise binary
+        metric scores
+        ``'micro'``:
+            Calculate metrics for the binarized-raveled classes. Only supported
+            for `multi_class='ovr'`.
+
+        .. versionadded:: 1.2
+
+        ``'macro'``:
+            Calculate metrics for each label, and find their unweighted
+            mean. This does not take label imbalance into account. Classes
+            are assumed to be uniformly distributed.
+        ``'weighted'``:
+            Calculate metrics for each label, taking into account the
+            prevalence of the classes.
+
+    sample_weight : array-like of shape (n_samples,) or None
+        Sample weights.
+
+    """
+    classes = _validate_multiclass_roc_auc_inputs(
+        y_true, y_score, labels, multi_class, average
+    )
+
     if multi_class == "ovo":
         if sample_weight is not None:
             raise ValueError(
