{
  "instance_id": "django__django-12741",
  "problem_statement": "Simplify signature of `DatabaseOperations.execute_sql_flush()`\nDescription\n\t\nThe current signature is:\ndef execute_sql_flush(self, using, sql_list):\nThe using argument can be dropped and inferred by the calling instance: self.connection.alias.\ndef execute_sql_flush(self, sql_list):\nSome internal ises of this method are already doing:\nconnection.ops.execute_sql_flush(connection.alias, sql_flush)\n",
  "localized_code": "[start of django/core/management/commands/flush.py]\n1: from importlib import import_module\n2: \n3: from django.apps import apps\n4: from django.core.management.base import BaseCommand, CommandError\n5: from django.core.management.color import no_style\n6: from django.core.management.sql import emit_post_migrate_signal, sql_flush\n7: from django.db import DEFAULT_DB_ALIAS, connections\n8: \n9: \n10: class Command(BaseCommand):\n11:     help = (\n12:         'Removes ALL DATA from the database, including data added during '\n13:         'migrations. Does not achieve a \"fresh install\" state.'\n14:     )\n15:     stealth_options = ('reset_sequences', 'allow_cascade', 'inhibit_post_migrate')\n16: \n17:     def add_arguments(self, parser):\n18:         parser.add_argument(\n19:             '--noinput', '--no-input', action='store_false', dest='interactive',\n20:             help='Tells Django to NOT prompt the user for input of any kind.',\n21:         )\n22:         parser.add_argument(\n23:             '--database', default=DEFAULT_DB_ALIAS,\n24:             help='Nominates a database to flush. Defaults to the \"default\" database.',\n25:         )\n26: \n27:     def handle(self, **options):\n28:         database = options['database']\n29:         connection = connections[database]\n30:         verbosity = options['verbosity']\n31:         interactive = options['interactive']\n32:         # The following are stealth options used by Django's internals.\n33:         reset_sequences = options.get('reset_sequences', True)\n34:         allow_cascade = options.get('allow_cascade', False)\n35:         inhibit_post_migrate = options.get('inhibit_post_migrate', False)\n36: \n37:         self.style = no_style()\n38: \n39:         # Import the 'management' module within each installed app, to register\n40:         # dispatcher events.\n41:         for app_config in apps.get_app_configs():\n42:             try:\n43:                 import_module('.management', app_config.name)\n44:             except ImportError:\n45:                 pass\n46: \n47:         sql_list = sql_flush(self.style, connection, only_django=True,\n48:                              reset_sequences=reset_sequences,\n49:                              allow_cascade=allow_cascade)\n50: \n51:         if interactive:\n52:             confirm = input(\"\"\"You have requested a flush of the database.\n53: This will IRREVERSIBLY DESTROY all data currently in the %r database,\n54: and return each table to an empty state.\n55: Are you sure you want to do this?\n56: \n57:     Type 'yes' to continue, or 'no' to cancel: \"\"\" % connection.settings_dict['NAME'])\n58:         else:\n59:             confirm = 'yes'\n60: \n61:         if confirm == 'yes':\n62:             try:\n63:                 connection.ops.execute_sql_flush(database, sql_list)\n64:             except Exception as exc:\n... Code Truncated ...\n\n[start of django/db/backends/base/operations.py]\n1: import datetime\n2: import decimal\n3: from importlib import import_module\n4: \n5: import sqlparse\n6: \n7: from django.conf import settings\n8: from django.db import NotSupportedError, transaction\n9: from django.db.backends import utils\n10: from django.utils import timezone\n11: from django.utils.encoding import force_str\n12: \n13: \n14: class BaseDatabaseOperations:\n15:     \"\"\"\n16:     Encapsulate backend-specific differences, such as the way a backend\n17:     performs ordering or calculates the ID of a recently-inserted row.\n18:     \"\"\"\n19:     compiler_module = \"django.db.models.sql.compiler\"\n20: \n21:     # Integer field safe ranges by `internal_type` as documented\n22:     # in docs/ref/models/fields.txt.\n23:     integer_field_ranges = {\n24:         'SmallIntegerField': (-32768, 32767),\n25:         'IntegerField': (-2147483648, 2147483647),\n26:         'BigIntegerField': (-9223372036854775808, 9223372036854775807),\n27:         'PositiveBigIntegerField': (0, 9223372036854775807),\n28:         'PositiveSmallIntegerField': (0, 32767),\n29:         'PositiveIntegerField': (0, 2147483647),\n30:         'SmallAutoField': (-32768, 32767),\n31:         'AutoField': (-2147483648, 2147483647),\n32:         'BigAutoField': (-9223372036854775808, 9223372036854775807),\n33:     }\n34:     set_operators = {\n35:         'union': 'UNION',\n36:         'intersection': 'INTERSECT',\n37:         'difference': 'EXCEPT',\n38:     }\n39:     # Mapping of Field.get_internal_type() (typically the model field's class\n40:     # name) to the data type to use for the Cast() function, if different from\n41:     # DatabaseWrapper.data_types.\n42:     cast_data_types = {}\n43:     # CharField data type if the max_length argument isn't provided.\n44:     cast_char_field_without_max_length = None\n45: \n46:     # Start and end points for window expressions.\n47:     PRECEDING = 'PRECEDING'\n48:     FOLLOWING = 'FOLLOWING'\n49:     UNBOUNDED_PRECEDING = 'UNBOUNDED ' + PRECEDING\n50:     UNBOUNDED_FOLLOWING = 'UNBOUNDED ' + FOLLOWING\n51:     CURRENT_ROW = 'CURRENT ROW'\n52: \n53:     # Prefix for EXPLAIN queries, or None EXPLAIN isn't supported.\n54:     explain_prefix = None\n55: \n56:     def __init__(self, connection):\n57:         self.connection = connection\n58:         self._cache = None\n59: \n60:     def autoinc_sql(self, table, column):\n61:         \"\"\"\n62:         Return any SQL needed to support auto-incrementing primary keys, or\n63:         None if no SQL is necessary.\n64: \n65:         This SQL is executed when a table is created.\n66:         \"\"\"\n67:         return None\n68: \n69:     def bulk_batch_size(self, fields, objs):\n70:         \"\"\"\n71:         Return the maximum allowed batch size for the backend. The fields\n72:         are the fields going to be inserted in the batch, the objs contains\n73:         all the objects to be inserted.\n74:         \"\"\"\n75:         return len(objs)\n76: \n77:     def cache_key_culling_sql(self):\n78:         \"\"\"\n79:         Return an SQL query that retrieves the first cache key greater than the\n80:         n smallest.\n81: \n82:         This is used by the 'db' cache backend to determine where to start\n83:         culling.\n84:         \"\"\"\n85:         return \"SELECT cache_key FROM %s ORDER BY cache_key LIMIT 1 OFFSET %%s\"\n86: \n87:     def unification_cast_sql(self, output_field):\n88:         \"\"\"\n89:         Given a field instance, return the SQL that casts the result of a union\n90:         to that type. The resulting string should contain a '%s' placeholder\n91:         for the expression being cast.\n92:         \"\"\"\n93:         return '%s'\n94: \n95:     def date_extract_sql(self, lookup_type, field_name):\n96:         \"\"\"\n97:         Given a lookup_type of 'year', 'month', or 'day', return the SQL that\n98:         extracts a value from the given date field field_name.\n99:         \"\"\"\n100:         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a date_extract_sql() method')\n101: \n102:     def date_interval_sql(self, timedelta):\n103:         \"\"\"\n104:         Implement the date interval functionality for expressions.\n105:         \"\"\"\n106:         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a date_interval_sql() method')\n107: \n108:     def date_trunc_sql(self, lookup_type, field_name):\n109:         \"\"\"\n110:         Given a lookup_type of 'year', 'month', or 'day', return the SQL that\n111:         truncates the given date field field_name to a date object with only\n112:         the given specificity.\n113:         \"\"\"\n114:         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a date_trunc_sql() method.')\n115: \n116:     def datetime_cast_date_sql(self, field_name, tzname):\n117:         \"\"\"\n118:         Return the SQL to cast a datetime value to date value.\n119:         \"\"\"\n120:         raise NotImplementedError(\n121:             'subclasses of BaseDatabaseOperations may require a '\n122:             'datetime_cast_date_sql() method.'\n123:         )\n124: \n125:     def datetime_cast_time_sql(self, field_name, tzname):\n126:         \"\"\"\n127:         Return the SQL to cast a datetime value to time value.\n128:         \"\"\"\n129:         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a datetime_cast_time_sql() method')\n130: \n131:     def datetime_extract_sql(self, lookup_type, field_name, tzname):\n132:         \"\"\"\n133:         Given a lookup_type of 'year', 'month', 'day', 'hour', 'minute', or\n134:         'second', return the SQL that extracts a value from the given\n135:         datetime field field_name.\n136:         \"\"\"\n137:         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a datetime_extract_sql() method')\n138: \n139:     def datetime_trunc_sql(self, lookup_type, field_name, tzname):\n140:         \"\"\"\n141:         Given a lookup_type of 'year', 'month', 'day', 'hour', 'minute', or\n142:         'second', return the SQL that truncates the given datetime field\n143:         field_name to a datetime object with only the given specificity.\n144:         \"\"\"\n145:         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a datetime_trunc_sql() method')\n146: \n147:     def time_trunc_sql(self, lookup_type, field_name):\n148:         \"\"\"\n149:         Given a lookup_type of 'hour', 'minute' or 'second', return the SQL\n150:         that truncates the given time field field_name to a time object with\n151:         only the given specificity.\n152:         \"\"\"\n153:         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a time_trunc_sql() method')\n154: \n155:     def time_extract_sql(self, lookup_type, field_name):\n156:         \"\"\"\n157:         Given a lookup_type of 'hour', 'minute', or 'second', return the SQL\n158:         that extracts a value from the given time field field_name.\n159:         \"\"\"\n160:         return self.date_extract_sql(lookup_type, field_name)\n161: \n162:     def deferrable_sql(self):\n163:         \"\"\"\n164:         Return the SQL to make a constraint \"initially deferred\" during a\n165:         CREATE TABLE statement.\n166:         \"\"\"\n167:         return ''\n168: \n169:     def distinct_sql(self, fields, params):\n170:         \"\"\"\n171:         Return an SQL DISTINCT clause which removes duplicate rows from the\n172:         result set. If any fields are given, only check the given fields for\n173:         duplicates.\n174:         \"\"\"\n175:         if fields:\n176:             raise NotSupportedError('DISTINCT ON fields is not supported by this database backend')\n177:         else:\n178:             return ['DISTINCT'], []\n179: \n180:     def fetch_returned_insert_columns(self, cursor, returning_params):\n181:         \"\"\"\n182:         Given a cursor object that has just performed an INSERT...RETURNING\n183:         statement into a table, return the newly created data.\n184:         \"\"\"\n185:         return cursor.fetchone()\n186: \n187:     def field_cast_sql(self, db_type, internal_type):\n188:         \"\"\"\n189:         Given a column type (e.g. 'BLOB', 'VARCHAR') and an internal type\n190:         (e.g. 'GenericIPAddressField'), return the SQL to cast it before using\n191:         it in a WHERE statement. The resulting string should contain a '%s'\n192:         placeholder for the column being searched against.\n193:         \"\"\"\n194:         return '%s'\n195: \n196:     def force_no_ordering(self):\n197:         \"\"\"\n198:         Return a list used in the \"ORDER BY\" clause to force no ordering at\n199:         all. Return an empty list to include nothing in the ordering.\n200:         \"\"\"\n201:         return []\n202: \n203:     def for_update_sql(self, nowait=False, skip_locked=False, of=()):\n204:         \"\"\"\n205:         Return the FOR UPDATE SQL clause to lock rows for an update operation.\n206:         \"\"\"\n207:         return 'FOR UPDATE%s%s%s' % (\n208:             ' OF %s' % ', '.join(of) if of else '',\n209:             ' NOWAIT' if nowait else '',\n210:             ' SKIP LOCKED' if skip_locked else '',\n211:         )\n212: \n213:     def _get_limit_offset_params(self, low_mark, high_mark):\n214:         offset = low_mark or 0\n215:         if high_mark is not None:\n216:             return (high_mark - offset), offset\n217:         elif offset:\n218:             return self.connection.ops.no_limit_value(), offset\n219:         return None, offset\n220: \n221:     def limit_offset_sql(self, low_mark, high_mark):\n222:         \"\"\"Return LIMIT/OFFSET SQL clause.\"\"\"\n223:         limit, offset = self._get_limit_offset_params(low_mark, high_mark)\n224:         return ' '.join(sql for sql in (\n225:             ('LIMIT %d' % limit) if limit else None,\n226:             ('OFFSET %d' % offset) if offset else None,\n227:         ) if sql)\n228: \n229:     def last_executed_query(self, cursor, sql, params):\n230:         \"\"\"\n231:         Return a string of the query last executed by the given cursor, with\n232:         placeholders replaced with actual values.\n233: \n234:         `sql` is the raw query containing placeholders and `params` is the\n235:         sequence of parameters. These are used by default, but this method\n236:         exists for database backends to provide a better implementation\n237:         according to their own quoting schemes.\n238:         \"\"\"\n239:         # Convert params to contain string values.\n240:         def to_string(s):\n241:             return force_str(s, strings_only=True, errors='replace')\n242:         if isinstance(params, (list, tuple)):\n243:             u_params = tuple(to_string(val) for val in params)\n244:         elif params is None:\n245:             u_params = ()\n246:         else:\n247:             u_params = {to_string(k): to_string(v) for k, v in params.items()}\n248: \n249:         return \"QUERY = %r - PARAMS = %r\" % (sql, u_params)\n250: \n251:     def last_insert_id(self, cursor, table_name, pk_name):\n252:         \"\"\"\n253:         Given a cursor object that has just performed an INSERT statement into\n254:         a table that has an auto-incrementing ID, return the newly created ID.\n255: \n256:         `pk_name` is the name of the primary-key column.\n257:         \"\"\"\n258:         return cursor.lastrowid\n259: \n260:     def lookup_cast(self, lookup_type, internal_type=None):\n261:         \"\"\"\n262:         Return the string to use in a query when performing lookups\n263:         (\"contains\", \"like\", etc.). It should contain a '%s' placeholder for\n264:         the column being searched against.\n265:         \"\"\"\n266:         return \"%s\"\n267: \n268:     def max_in_list_size(self):\n269:         \"\"\"\n270:         Return the maximum number of items that can be passed in a single 'IN'\n271:         list condition, or None if the backend does not impose a limit.\n272:         \"\"\"\n273:         return None\n274: \n275:     def max_name_length(self):\n276:         \"\"\"\n277:         Return the maximum length of table and column names, or None if there\n278:         is no limit.\n279:         \"\"\"\n280:         return None\n281: \n282:     def no_limit_value(self):\n283:         \"\"\"\n284:         Return the value to use for the LIMIT when we are wanting \"LIMIT\n285:         infinity\". Return None if the limit clause can be omitted in this case.\n286:         \"\"\"\n287:         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a no_limit_value() method')\n288: \n289:     def pk_default_value(self):\n290:         \"\"\"\n291:         Return the value to use during an INSERT statement to specify that\n292:         the field should use its default value.\n293:         \"\"\"\n294:         return 'DEFAULT'\n295: \n296:     def prepare_sql_script(self, sql):\n297:         \"\"\"\n298:         Take an SQL script that may contain multiple lines and return a list\n299:         of statements to feed to successive cursor.execute() calls.\n300: \n301:         Since few databases are able to process raw SQL scripts in a single\n302:         cursor.execute() call and PEP 249 doesn't talk about this use case,\n303:         the default implementation is conservative.\n304:         \"\"\"\n305:         return [\n306:             sqlparse.format(statement, strip_comments=True)\n307:             for statement in sqlparse.split(sql) if statement\n308:         ]\n309: \n310:     def process_clob(self, value):\n311:         \"\"\"\n312:         Return the value of a CLOB column, for backends that return a locator\n313:         object that requires additional processing.\n314:         \"\"\"\n315:         return value\n316: \n317:     def return_insert_columns(self, fields):\n318:         \"\"\"\n319:         For backends that support returning columns as part of an insert query,\n320:         return the SQL and params to append to the INSERT query. The returned\n321:         fragment should contain a format string to hold the appropriate column.\n322:         \"\"\"\n323:         pass\n324: \n325:     def compiler(self, compiler_name):\n326:         \"\"\"\n327:         Return the SQLCompiler class corresponding to the given name,\n328:         in the namespace corresponding to the `compiler_module` attribute\n329:         on this backend.\n330:         \"\"\"\n331:         if self._cache is None:\n332:             self._cache = import_module(self.compiler_module)\n333:         return getattr(self._cache, compiler_name)\n334: \n335:     def quote_name(self, name):\n336:         \"\"\"\n337:         Return a quoted version of the given table, index, or column name. Do\n338:         not quote the given name if it's already been quoted.\n339:         \"\"\"\n340:         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a quote_name() method')\n341: \n342:     def random_function_sql(self):\n343:         \"\"\"Return an SQL expression that returns a random value.\"\"\"\n344:         return 'RANDOM()'\n345: \n346:     def regex_lookup(self, lookup_type):\n347:         \"\"\"\n348:         Return the string to use in a query when performing regular expression\n349:         lookups (using \"regex\" or \"iregex\"). It should contain a '%s'\n350:         placeholder for the column being searched against.\n351: \n352:         If the feature is not supported (or part of it is not supported), raise\n353:         NotImplementedError.\n354:         \"\"\"\n355:         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a regex_lookup() method')\n356: \n357:     def savepoint_create_sql(self, sid):\n358:         \"\"\"\n359:         Return the SQL for starting a new savepoint. Only required if the\n360:         \"uses_savepoints\" feature is True. The \"sid\" parameter is a string\n361:         for the savepoint id.\n362:         \"\"\"\n363:         return \"SAVEPOINT %s\" % self.quote_name(sid)\n364: \n365:     def savepoint_commit_sql(self, sid):\n366:         \"\"\"\n367:         Return the SQL for committing the given savepoint.\n368:         \"\"\"\n369:         return \"RELEASE SAVEPOINT %s\" % self.quote_name(sid)\n370: \n371:     def savepoint_rollback_sql(self, sid):\n372:         \"\"\"\n373:         Return the SQL for rolling back the given savepoint.\n374:         \"\"\"\n375:         return \"ROLLBACK TO SAVEPOINT %s\" % self.quote_name(sid)\n376: \n377:     def set_time_zone_sql(self):\n378:         \"\"\"\n379:         Return the SQL that will set the connection's time zone.\n380: \n381:         Return '' if the backend doesn't support time zones.\n382:         \"\"\"\n383:         return ''\n384: \n385:     def sql_flush(self, style, tables, *, reset_sequences=False, allow_cascade=False):\n386:         \"\"\"\n387:         Return a list of SQL statements required to remove all data from\n388:         the given database tables (without actually removing the tables\n389:         themselves).\n390: \n391:         The `style` argument is a Style object as returned by either\n392:         color_style() or no_style() in django.core.management.color.\n393: \n394:         If `reset_sequences` is True, the list includes SQL statements required\n395:         to reset the sequences.\n396: \n397:         The `allow_cascade` argument determines whether truncation may cascade\n398:         to tables with foreign keys pointing the tables being truncated.\n399:         PostgreSQL requires a cascade even if these tables are empty.\n400:         \"\"\"\n401:         raise NotImplementedError('subclasses of BaseDatabaseOperations must provide a sql_flush() method')\n402: \n403:     def execute_sql_flush(self, using, sql_list):\n404:         \"\"\"Execute a list of SQL statements to flush the database.\"\"\"\n405:         with transaction.atomic(using=using, savepoint=self.connection.features.can_rollback_ddl):\n406:             with self.connection.cursor() as cursor:\n... Code Truncated ...\n\n",
  "line_level_localization": [
    {
      "filename": "/django/core/management/commands/flush.py",
      "suspect_lines": [
        63
      ]
    },
    {
      "filename": "/django/db/backends/base/operations.py",
      "suspect_lines": [
        403,
        405
      ]
    }
  ]
}