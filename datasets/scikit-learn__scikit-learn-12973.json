{
  "instance_id": "scikit-learn__scikit-learn-12973",
  "problem_statement": "LassoLarsIC: unintuitive copy_X behaviour\nHi, I would like to report what seems to be a bug in the treatment of the `copy_X` parameter of the `LassoLarsIC` class. Because it's a simple bug, it's much easier to see in the code directly than in the execution, so I am not posting steps to reproduce it.\r\n\r\nAs you can see here, LassoLarsIC accepts a copy_X parameter.\r\nhttps://github.com/scikit-learn/scikit-learn/blob/7389dbac82d362f296dc2746f10e43ffa1615660/sklearn/linear_model/least_angle.py#L1487\r\n\r\nHowever, it also takes a copy_X parameter a few lines below, in the definition of ```fit```.\r\n    ```def fit(self, X, y, copy_X=True):```\r\n\r\nNow there are two values (potentially contradicting each other) for copy_X and each one is used once. Therefore ```fit``` can have a mixed behaviour. Even worse, this can be completely invisible to the user, since copy_X has a default value of True. Let's assume that I'd like it to be False, and have set it to False in the initialization, `my_lasso = LassoLarsIC(copy_X=False)`. I then call ```my_lasso.fit(X, y)``` and my choice will be silently overwritten. \r\n\r\nIdeally I think that copy_X should be removed as an argument in ```fit```. No other estimator seems to have a duplication in class parameters and fit arguments (I've checked more than ten in the linear models module). However, this would break existing code. Therefore I propose that ```fit``` takes a default value of `None` and only overwrites the existing value if the user has explicitly passed it as an argument to ```fit```. I will submit a PR to that effect.\n",
  "localized_code": "[start of sklearn/linear_model/least_angle.py]\n1: \"\"\"\n2: Least Angle Regression algorithm. See the documentation on the\n3: Generalized Linear Model for a complete discussion.\n4: \"\"\"\n5: from __future__ import print_function\n6: \n7: # Author: Fabian Pedregosa <fabian.pedregosa@inria.fr>\n8: #         Alexandre Gramfort <alexandre.gramfort@inria.fr>\n9: #         Gael Varoquaux\n10: #\n11: # License: BSD 3 clause\n12: \n13: from math import log\n14: import sys\n15: import warnings\n16: \n17: import numpy as np\n18: from scipy import linalg, interpolate\n19: from scipy.linalg.lapack import get_lapack_funcs\n20: \n21: from .base import LinearModel\n22: from ..base import RegressorMixin\n23: from ..utils import arrayfuncs, as_float_array, check_X_y\n24: from ..model_selection import check_cv\n25: from ..exceptions import ConvergenceWarning\n26: from ..utils._joblib import Parallel, delayed\n27: \n28: solve_triangular_args = {'check_finite': False}\n29: \n30: \n31: def lars_path(X, y, Xy=None, Gram=None, max_iter=500,\n32:               alpha_min=0, method='lar', copy_X=True,\n33:               eps=np.finfo(np.float).eps,\n34:               copy_Gram=True, verbose=0, return_path=True,\n35:               return_n_iter=False, positive=False):\n36:     \"\"\"Compute Least Angle Regression or Lasso path using LARS algorithm [1]\n37: \n38:     The optimization objective for the case method='lasso' is::\n39: \n40:     (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n41: \n42:     in the case of method='lars', the objective function is only known in\n43:     the form of an implicit equation (see discussion in [1])\n44: \n45:     Read more in the :ref:`User Guide <least_angle_regression>`.\n46: \n47:     Parameters\n48:     -----------\n49:     X : array, shape: (n_samples, n_features)\n50:         Input data.\n51: \n52:     y : array, shape: (n_samples)\n53:         Input targets.\n54: \n55:     Xy : array-like, shape (n_samples,) or (n_samples, n_targets), \\\n56:             optional\n57:         Xy = np.dot(X.T, y) that can be precomputed. It is useful\n58:         only when the Gram matrix is precomputed.\n59: \n60:     Gram : None, 'auto', array, shape: (n_features, n_features), optional\n61:         Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram\n62:         matrix is precomputed from the given X, if there are more samples\n63:         than features.\n64: \n65:     max_iter : integer, optional (default=500)\n66:         Maximum number of iterations to perform, set to infinity for no limit.\n67: \n68:     alpha_min : float, optional (default=0)\n69:         Minimum correlation along the path. It corresponds to the\n70:         regularization parameter alpha parameter in the Lasso.\n71: \n72:     method : {'lar', 'lasso'}, optional (default='lar')\n73:         Specifies the returned model. Select ``'lar'`` for Least Angle\n74:         Regression, ``'lasso'`` for the Lasso.\n75: \n76:     copy_X : bool, optional (default=True)\n77:         If ``False``, ``X`` is overwritten.\n78: \n79:     eps : float, optional (default=``np.finfo(np.float).eps``)\n80:         The machine-precision regularization in the computation of the\n81:         Cholesky diagonal factors. Increase this for very ill-conditioned\n82:         systems.\n83: \n84:     copy_Gram : bool, optional (default=True)\n85:         If ``False``, ``Gram`` is overwritten.\n86: \n87:     verbose : int (default=0)\n88:         Controls output verbosity.\n89: \n90:     return_path : bool, optional (default=True)\n91:         If ``return_path==True`` returns the entire path, else returns only the\n92:         last point of the path.\n93: \n94:     return_n_iter : bool, optional (default=False)\n95:         Whether to return the number of iterations.\n96: \n97:     positive : boolean (default=False)\n98:         Restrict coefficients to be >= 0.\n99:         This option is only allowed with method 'lasso'. Note that the model\n100:         coefficients will not converge to the ordinary-least-squares solution\n101:         for small values of alpha. Only coefficients up to the smallest alpha\n102:         value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by\n103:         the stepwise Lars-Lasso algorithm are typically in congruence with the\n104:         solution of the coordinate descent lasso_path function.\n105: \n106:     Returns\n107:     --------\n108:     alphas : array, shape: [n_alphas + 1]\n109:         Maximum of covariances (in absolute value) at each iteration.\n110:         ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n111:         number of nodes in the path with ``alpha >= alpha_min``, whichever\n112:         is smaller.\n113: \n114:     active : array, shape [n_alphas]\n115:         Indices of active variables at the end of the path.\n116: \n117:     coefs : array, shape (n_features, n_alphas + 1)\n118:         Coefficients along the path\n119: \n120:     n_iter : int\n121:         Number of iterations run. Returned only if return_n_iter is set\n122:         to True.\n123: \n124:     See also\n125:     --------\n126:     lasso_path\n127:     LassoLars\n128:     Lars\n129:     LassoLarsCV\n130:     LarsCV\n131:     sklearn.decomposition.sparse_encode\n132: \n133:     References\n134:     ----------\n135:     .. [1] \"Least Angle Regression\", Effron et al.\n136:            https://statweb.stanford.edu/~tibs/ftp/lars.pdf\n137: \n138:     .. [2] `Wikipedia entry on the Least-angle regression\n139:            <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n140: \n141:     .. [3] `Wikipedia entry on the Lasso\n142:            <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n143: \n144:     \"\"\"\n145:     if method == 'lar' and positive:\n146:         warnings.warn('positive option is broken for Least'\n147:                       ' Angle Regression (LAR). Use method=\"lasso\".'\n148:                       ' This option will be removed in version 0.22.',\n149:                       DeprecationWarning)\n150: \n151:     n_features = X.shape[1]\n152:     n_samples = y.size\n153:     max_features = min(max_iter, n_features)\n154: \n155:     if return_path:\n156:         coefs = np.zeros((max_features + 1, n_features))\n157:         alphas = np.zeros(max_features + 1)\n158:     else:\n159:         coef, prev_coef = np.zeros(n_features), np.zeros(n_features)\n160:         alpha, prev_alpha = np.array([0.]), np.array([0.])  # better ideas?\n161: \n162:     n_iter, n_active = 0, 0\n163:     active, indices = list(), np.arange(n_features)\n164:     # holds the sign of covariance\n165:     sign_active = np.empty(max_features, dtype=np.int8)\n166:     drop = False\n167: \n168:     # will hold the cholesky factorization. Only lower part is\n169:     # referenced.\n170:     L = np.empty((max_features, max_features), dtype=X.dtype)\n171:     swap, nrm2 = linalg.get_blas_funcs(('swap', 'nrm2'), (X,))\n172:     solve_cholesky, = get_lapack_funcs(('potrs',), (X,))\n173: \n174:     if Gram is None or Gram is False:\n175:         Gram = None\n176:         if copy_X:\n177:             # force copy. setting the array to be fortran-ordered\n178:             # speeds up the calculation of the (partial) Gram matrix\n179:             # and allows to easily swap columns\n180:             X = X.copy('F')\n181: \n182:     elif isinstance(Gram, str) and Gram == 'auto' or Gram is True:\n183:         if Gram is True or X.shape[0] > X.shape[1]:\n184:             Gram = np.dot(X.T, X)\n185:         else:\n186:             Gram = None\n187:     elif copy_Gram:\n188:         Gram = Gram.copy()\n189: \n190:     if Xy is None:\n191:         Cov = np.dot(X.T, y)\n192:     else:\n193:         Cov = Xy.copy()\n194: \n195:     if verbose:\n196:         if verbose > 1:\n197:             print(\"Step\\t\\tAdded\\t\\tDropped\\t\\tActive set size\\t\\tC\")\n198:         else:\n199:             sys.stdout.write('.')\n200:             sys.stdout.flush()\n201: \n202:     tiny32 = np.finfo(np.float32).tiny  # to avoid division by 0 warning\n203:     equality_tolerance = np.finfo(np.float32).eps\n204: \n205:     while True:\n206:         if Cov.size:\n207:             if positive:\n208:                 C_idx = np.argmax(Cov)\n209:             else:\n210:                 C_idx = np.argmax(np.abs(Cov))\n211: \n212:             C_ = Cov[C_idx]\n213: \n214:             if positive:\n215:                 C = C_\n216:             else:\n217:                 C = np.fabs(C_)\n218:         else:\n219:             C = 0.\n220: \n221:         if return_path:\n222:             alpha = alphas[n_iter, np.newaxis]\n223:             coef = coefs[n_iter]\n224:             prev_alpha = alphas[n_iter - 1, np.newaxis]\n225:             prev_coef = coefs[n_iter - 1]\n226: \n227:         alpha[0] = C / n_samples\n228:         if alpha[0] <= alpha_min + equality_tolerance:  # early stopping\n229:             if abs(alpha[0] - alpha_min) > equality_tolerance:\n230:                 # interpolation factor 0 <= ss < 1\n231:                 if n_iter > 0:\n232:                     # In the first iteration, all alphas are zero, the formula\n233:                     # below would make ss a NaN\n234:                     ss = ((prev_alpha[0] - alpha_min) /\n235:                           (prev_alpha[0] - alpha[0]))\n236:                     coef[:] = prev_coef + ss * (coef - prev_coef)\n237:                 alpha[0] = alpha_min\n238:             if return_path:\n239:                 coefs[n_iter] = coef\n240:             break\n241: \n242:         if n_iter >= max_iter or n_active >= n_features:\n243:             break\n244: \n245:         if not drop:\n246: \n247:             ##########################################################\n248:             # Append x_j to the Cholesky factorization of (Xa * Xa') #\n249:             #                                                        #\n250:             #            ( L   0 )                                   #\n251:             #     L  ->  (       )  , where L * w = Xa' x_j          #\n252:             #            ( w   z )    and z = ||x_j||                #\n253:             #                                                        #\n254:             ##########################################################\n255: \n256:             if positive:\n257:                 sign_active[n_active] = np.ones_like(C_)\n258:             else:\n259:                 sign_active[n_active] = np.sign(C_)\n260:             m, n = n_active, C_idx + n_active\n261: \n262:             Cov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])\n263:             indices[n], indices[m] = indices[m], indices[n]\n264:             Cov_not_shortened = Cov\n265:             Cov = Cov[1:]  # remove Cov[0]\n266: \n267:             if Gram is None:\n268:                 X.T[n], X.T[m] = swap(X.T[n], X.T[m])\n269:                 c = nrm2(X.T[n_active]) ** 2\n270:                 L[n_active, :n_active] = \\\n271:                     np.dot(X.T[n_active], X.T[:n_active].T)\n272:             else:\n273:                 # swap does only work inplace if matrix is fortran\n274:                 # contiguous ...\n275:                 Gram[m], Gram[n] = swap(Gram[m], Gram[n])\n276:                 Gram[:, m], Gram[:, n] = swap(Gram[:, m], Gram[:, n])\n277:                 c = Gram[n_active, n_active]\n278:                 L[n_active, :n_active] = Gram[n_active, :n_active]\n279: \n280:             # Update the cholesky decomposition for the Gram matrix\n281:             if n_active:\n282:                 linalg.solve_triangular(L[:n_active, :n_active],\n283:                                         L[n_active, :n_active],\n284:                                         trans=0, lower=1,\n285:                                         overwrite_b=True,\n286:                                         **solve_triangular_args)\n287: \n288:             v = np.dot(L[n_active, :n_active], L[n_active, :n_active])\n289:             diag = max(np.sqrt(np.abs(c - v)), eps)\n290:             L[n_active, n_active] = diag\n291: \n292:             if diag < 1e-7:\n293:                 # The system is becoming too ill-conditioned.\n294:                 # We have degenerate vectors in our active set.\n295:                 # We'll 'drop for good' the last regressor added.\n296: \n297:                 # Note: this case is very rare. It is no longer triggered by\n298:                 # the test suite. The `equality_tolerance` margin added in 0.16\n299:                 # to get early stopping to work consistently on all versions of\n300:                 # Python including 32 bit Python under Windows seems to make it\n301:                 # very difficult to trigger the 'drop for good' strategy.\n302:                 warnings.warn('Regressors in active set degenerate. '\n303:                               'Dropping a regressor, after %i iterations, '\n304:                               'i.e. alpha=%.3e, '\n305:                               'with an active set of %i regressors, and '\n306:                               'the smallest cholesky pivot element being %.3e.'\n307:                               ' Reduce max_iter or increase eps parameters.'\n308:                               % (n_iter, alpha, n_active, diag),\n309:                               ConvergenceWarning)\n310: \n311:                 # XXX: need to figure a 'drop for good' way\n312:                 Cov = Cov_not_shortened\n313:                 Cov[0] = 0\n314:                 Cov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])\n315:                 continue\n316: \n317:             active.append(indices[n_active])\n318:             n_active += 1\n319: \n320:             if verbose > 1:\n321:                 print(\"%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\" % (n_iter, active[-1], '',\n322:                                                       n_active, C))\n323: \n324:         if method == 'lasso' and n_iter > 0 and prev_alpha[0] < alpha[0]:\n325:             # alpha is increasing. This is because the updates of Cov are\n326:             # bringing in too much numerical error that is greater than\n327:             # than the remaining correlation with the\n328:             # regressors. Time to bail out\n329:             warnings.warn('Early stopping the lars path, as the residues '\n330:                           'are small and the current value of alpha is no '\n331:                           'longer well controlled. %i iterations, alpha=%.3e, '\n332:                           'previous alpha=%.3e, with an active set of %i '\n333:                           'regressors.'\n334:                           % (n_iter, alpha, prev_alpha, n_active),\n335:                           ConvergenceWarning)\n336:             break\n337: \n338:         # least squares solution\n339:         least_squares, info = solve_cholesky(L[:n_active, :n_active],\n340:                                              sign_active[:n_active],\n341:                                              lower=True)\n342: \n343:         if least_squares.size == 1 and least_squares == 0:\n344:             # This happens because sign_active[:n_active] = 0\n345:             least_squares[...] = 1\n346:             AA = 1.\n347:         else:\n348:             # is this really needed ?\n349:             AA = 1. / np.sqrt(np.sum(least_squares * sign_active[:n_active]))\n350: \n351:             if not np.isfinite(AA):\n352:                 # L is too ill-conditioned\n353:                 i = 0\n354:                 L_ = L[:n_active, :n_active].copy()\n355:                 while not np.isfinite(AA):\n356:                     L_.flat[::n_active + 1] += (2 ** i) * eps\n357:                     least_squares, info = solve_cholesky(\n358:                         L_, sign_active[:n_active], lower=True)\n359:                     tmp = max(np.sum(least_squares * sign_active[:n_active]),\n360:                               eps)\n361:                     AA = 1. / np.sqrt(tmp)\n362:                     i += 1\n363:             least_squares *= AA\n364: \n365:         if Gram is None:\n366:             # equiangular direction of variables in the active set\n367:             eq_dir = np.dot(X.T[:n_active].T, least_squares)\n368:             # correlation between each unactive variables and\n369:             # eqiangular vector\n370:             corr_eq_dir = np.dot(X.T[n_active:], eq_dir)\n371:         else:\n372:             # if huge number of features, this takes 50% of time, I\n373:             # think could be avoided if we just update it using an\n374:             # orthogonal (QR) decomposition of X\n375:             corr_eq_dir = np.dot(Gram[:n_active, n_active:].T,\n376:                                  least_squares)\n377: \n378:         g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))\n379:         if positive:\n380:             gamma_ = min(g1, C / AA)\n381:         else:\n382:             g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))\n383:             gamma_ = min(g1, g2, C / AA)\n384: \n385:         # TODO: better names for these variables: z\n386:         drop = False\n387:         z = -coef[active] / (least_squares + tiny32)\n388:         z_pos = arrayfuncs.min_pos(z)\n389:         if z_pos < gamma_:\n390:             # some coefficients have changed sign\n391:             idx = np.where(z == z_pos)[0][::-1]\n392: \n393:             # update the sign, important for LAR\n394:             sign_active[idx] = -sign_active[idx]\n395: \n396:             if method == 'lasso':\n397:                 gamma_ = z_pos\n398:             drop = True\n399: \n400:         n_iter += 1\n401: \n402:         if return_path:\n403:             if n_iter >= coefs.shape[0]:\n404:                 del coef, alpha, prev_alpha, prev_coef\n405:                 # resize the coefs and alphas array\n406:                 add_features = 2 * max(1, (max_features - n_active))\n407:                 coefs = np.resize(coefs, (n_iter + add_features, n_features))\n408:                 coefs[-add_features:] = 0\n409:                 alphas = np.resize(alphas, n_iter + add_features)\n410:                 alphas[-add_features:] = 0\n411:             coef = coefs[n_iter]\n412:             prev_coef = coefs[n_iter - 1]\n413:         else:\n414:             # mimic the effect of incrementing n_iter on the array references\n415:             prev_coef = coef\n416:             prev_alpha[0] = alpha[0]\n417:             coef = np.zeros_like(coef)\n418: \n419:         coef[active] = prev_coef[active] + gamma_ * least_squares\n420: \n421:         # update correlations\n422:         Cov -= gamma_ * corr_eq_dir\n423: \n424:         # See if any coefficient has changed sign\n425:         if drop and method == 'lasso':\n426: \n427:             # handle the case when idx is not length of 1\n428:             [arrayfuncs.cholesky_delete(L[:n_active, :n_active], ii) for ii in\n429:                 idx]\n430: \n431:             n_active -= 1\n432:             # handle the case when idx is not length of 1\n433:             drop_idx = [active.pop(ii) for ii in idx]\n434: \n435:             if Gram is None:\n436:                 # propagate dropped variable\n437:                 for ii in idx:\n438:                     for i in range(ii, n_active):\n439:                         X.T[i], X.T[i + 1] = swap(X.T[i], X.T[i + 1])\n440:                         # yeah this is stupid\n441:                         indices[i], indices[i + 1] = indices[i + 1], indices[i]\n442: \n443:                 # TODO: this could be updated\n444:                 residual = y - np.dot(X[:, :n_active], coef[active])\n445:                 temp = np.dot(X.T[n_active], residual)\n446: \n447:                 Cov = np.r_[temp, Cov]\n448:             else:\n449:                 for ii in idx:\n450:                     for i in range(ii, n_active):\n451:                         indices[i], indices[i + 1] = indices[i + 1], indices[i]\n452:                         Gram[i], Gram[i + 1] = swap(Gram[i], Gram[i + 1])\n453:                         Gram[:, i], Gram[:, i + 1] = swap(Gram[:, i],\n454:                                                           Gram[:, i + 1])\n455: \n456:                 # Cov_n = Cov_j + x_j * X + increment(betas) TODO:\n457:                 # will this still work with multiple drops ?\n458: \n459:                 # recompute covariance. Probably could be done better\n460:                 # wrong as Xy is not swapped with the rest of variables\n461: \n462:                 # TODO: this could be updated\n463:                 residual = y - np.dot(X, coef)\n464:                 temp = np.dot(X.T[drop_idx], residual)\n465:                 Cov = np.r_[temp, Cov]\n466: \n467:             sign_active = np.delete(sign_active, idx)\n468:             sign_active = np.append(sign_active, 0.)  # just to maintain size\n469:             if verbose > 1:\n470:                 print(\"%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\" % (n_iter, '', drop_idx,\n471:                                                       n_active, abs(temp)))\n472: \n473:     if return_path:\n474:         # resize coefs in case of early stop\n475:         alphas = alphas[:n_iter + 1]\n476:         coefs = coefs[:n_iter + 1]\n477: \n478:         if return_n_iter:\n479:             return alphas, active, coefs.T, n_iter\n480:         else:\n481:             return alphas, active, coefs.T\n482:     else:\n483:         if return_n_iter:\n484:             return alpha, active, coef, n_iter\n485:         else:\n486:             return alpha, active, coef\n487: \n488: \n489: ###############################################################################\n490: # Estimator classes\n491: \n492: class Lars(LinearModel, RegressorMixin):\nCode replaced for brevity.\n707: \n708: \n709: \n710: class LassoLars(Lars):\nCode replaced for brevity.\n844: \n845: \n846: \n847: ###############################################################################\n848: # Cross-validated estimator classes\n849: \n850: def _check_copy_and_writeable(array, copy=False):\nCode replaced for brevity.\n853: \n854: \n855: \n856:                         eps=np.finfo(np.float).eps, positive=False):\nCode replaced for brevity.\n966: \n967: \n968: \n969: class LarsCV(Lars):\nCode replaced for brevity.\n1185: \n1186: \n1187: \n1188: class LassoLarsCV(LarsCV):\nCode replaced for brevity.\n1348: \n1349: \n1350: \n1351: class LassoLarsIC(LassoLars):\n1352:     \"\"\"Lasso model fit with Lars using BIC or AIC for model selection\n1353: \n1354:     The optimization objective for Lasso is::\n1355: \n1356:     (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n1357: \n1358:     AIC is the Akaike information criterion and BIC is the Bayes\n1359:     Information criterion. Such criteria are useful to select the value\n1360:     of the regularization parameter by making a trade-off between the\n1361:     goodness of fit and the complexity of the model. A good model should\n1362:     explain well the data while being simple.\n1363: \n1364:     Read more in the :ref:`User Guide <least_angle_regression>`.\n1365: \n1366:     Parameters\n1367:     ----------\n1368:     criterion : 'bic' | 'aic'\n1369:         The type of criterion to use.\n1370: \n1371:     fit_intercept : boolean\n1372:         whether to calculate the intercept for this model. If set\n1373:         to false, no intercept will be used in calculations\n1374:         (e.g. data is expected to be already centered).\n1375: \n1376:     verbose : boolean or integer, optional\n1377:         Sets the verbosity amount\n1378: \n1379:     normalize : boolean, optional, default True\n1380:         This parameter is ignored when ``fit_intercept`` is set to False.\n1381:         If True, the regressors X will be normalized before regression by\n1382:         subtracting the mean and dividing by the l2-norm.\n1383:         If you wish to standardize, please use\n1384:         :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n1385:         on an estimator with ``normalize=False``.\n1386: \n1387:     precompute : True | False | 'auto' | array-like\n1388:         Whether to use a precomputed Gram matrix to speed up\n1389:         calculations. If set to ``'auto'`` let us decide. The Gram\n1390:         matrix can also be passed as argument.\n1391: \n1392:     max_iter : integer, optional\n1393:         Maximum number of iterations to perform. Can be used for\n1394:         early stopping.\n1395: \n1396:     eps : float, optional\n1397:         The machine-precision regularization in the computation of the\n1398:         Cholesky diagonal factors. Increase this for very ill-conditioned\n1399:         systems. Unlike the ``tol`` parameter in some iterative\n1400:         optimization-based algorithms, this parameter does not control\n1401:         the tolerance of the optimization.\n1402: \n1403:     copy_X : boolean, optional, default True\n1404:         If True, X will be copied; else, it may be overwritten.\n1405: \n1406:     positive : boolean (default=False)\n1407:         Restrict coefficients to be >= 0. Be aware that you might want to\n1408:         remove fit_intercept which is set True by default.\n1409:         Under the positive restriction the model coefficients do not converge\n1410:         to the ordinary-least-squares solution for small values of alpha.\n1411:         Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n1412:         0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n1413:         algorithm are typically in congruence with the solution of the\n1414:         coordinate descent Lasso estimator.\n1415:         As a consequence using LassoLarsIC only makes sense for problems where\n1416:         a sparse solution is expected and/or reached.\n1417: \n1418: \n1419:     Attributes\n1420:     ----------\n1421:     coef_ : array, shape (n_features,)\n1422:         parameter vector (w in the formulation formula)\n1423: \n1424:     intercept_ : float\n1425:         independent term in decision function.\n1426: \n1427:     alpha_ : float\n1428:         the alpha parameter chosen by the information criterion\n1429: \n1430:     n_iter_ : int\n1431:         number of iterations run by lars_path to find the grid of\n1432:         alphas.\n1433: \n1434:     criterion_ : array, shape (n_alphas,)\n1435:         The value of the information criteria ('aic', 'bic') across all\n1436:         alphas. The alpha which has the smallest information criterion is\n1437:         chosen. This value is larger by a factor of ``n_samples`` compared to\n1438:         Eqns. 2.15 and 2.16 in (Zou et al, 2007).\n1439: \n1440: \n1441:     Examples\n1442:     --------\n1443:     >>> from sklearn import linear_model\n1444:     >>> reg = linear_model.LassoLarsIC(criterion='bic')\n1445:     >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n1446:     ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n1447:     LassoLarsIC(copy_X=True, criterion='bic', eps=..., fit_intercept=True,\n1448:           max_iter=500, normalize=True, positive=False, precompute='auto',\n1449:           verbose=False)\n1450:     >>> print(reg.coef_) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n1451:     [ 0.  -1.11...]\n1452: \n1453:     Notes\n1454:     -----\n1455:     The estimation of the number of degrees of freedom is given by:\n1456: \n1457:     \"On the degrees of freedom of the lasso\"\n1458:     Hui Zou, Trevor Hastie, and Robert Tibshirani\n1459:     Ann. Statist. Volume 35, Number 5 (2007), 2173-2192.\n1460: \n1461:     https://en.wikipedia.org/wiki/Akaike_information_criterion\n1462:     https://en.wikipedia.org/wiki/Bayesian_information_criterion\n1463: \n1464:     See also\n1465:     --------\n1466:     lars_path, LassoLars, LassoLarsCV\n1467:     \"\"\"\n1468:     def __init__(self, criterion='aic', fit_intercept=True, verbose=False,\n1469:                  normalize=True, precompute='auto', max_iter=500,\n1470:                  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n1471:         self.criterion = criterion\n1472:         self.fit_intercept = fit_intercept\n1473:         self.positive = positive\n1474:         self.max_iter = max_iter\n1475:         self.verbose = verbose\n1476:         self.normalize = normalize\n1477:         self.copy_X = copy_X\n1478:         self.precompute = precompute\n1479:         self.eps = eps\n1480:         self.fit_path = True\n1481: \n1482:     def fit(self, X, y, copy_X=True):\n1483:         \"\"\"Fit the model using X, y as training data.\n1484: \n1485:         Parameters\n1486:         ----------\n1487:         X : array-like, shape (n_samples, n_features)\n1488:             training data.\n1489: \n1490:         y : array-like, shape (n_samples,)\n1491:             target values. Will be cast to X's dtype if necessary\n1492: \n1493:         copy_X : boolean, optional, default True\n1494:             If ``True``, X will be copied; else, it may be overwritten.\n1495: \n1496:         Returns\n1497:         -------\n1498:         self : object\n1499:             returns an instance of self.\n1500:         \"\"\"\n1501:         X, y = check_X_y(X, y, y_numeric=True)\n1502: \n1503:         X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n1504:             X, y, self.fit_intercept, self.normalize, self.copy_X)\n1505:         max_iter = self.max_iter\n1506: \n1507:         Gram = self.precompute\n1508: \n1509:         alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n1510:             X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n1511:             method='lasso', verbose=self.verbose, max_iter=max_iter,\n1512:             eps=self.eps, return_n_iter=True, positive=self.positive)\n1513: \n1514:         n_samples = X.shape[0]\n1515: \n1516:         if self.criterion == 'aic':\n1517:             K = 2  # AIC\n1518:         elif self.criterion == 'bic':\n1519:             K = log(n_samples)  # BIC\n1520:         else:\n1521:             raise ValueError('criterion should be either bic or aic')\n1522: \n1523:         R = y[:, np.newaxis] - np.dot(X, coef_path_)  # residuals\n1524:         mean_squared_error = np.mean(R ** 2, axis=0)\n1525:         sigma2 = np.var(y)\n1526: \n1527:         df = np.zeros(coef_path_.shape[1], dtype=np.int)  # Degrees of freedom\n1528:         for k, coef in enumerate(coef_path_.T):\n1529:             mask = np.abs(coef) > np.finfo(coef.dtype).eps\n1530:             if not np.any(mask):\n1531:                 continue\n1532:             # get the number of degrees of freedom equal to:\n1533:             # Xc = X[:, mask]\n1534:             # Trace(Xc * inv(Xc.T, Xc) * Xc.T) ie the number of non-zero coefs\n1535:             df[k] = np.sum(mask)\n1536: \n1537:         self.alphas_ = alphas_\n1538:         eps64 = np.finfo('float64').eps\n1539:         self.criterion_ = (n_samples * mean_squared_error / (sigma2 + eps64) +\n1540:                            K * df)  # Eqns. 2.15--16 in (Zou et al, 2007)\n1541:         n_best = np.argmin(self.criterion_)\n1542: \n1543:         self.alpha_ = alphas_[n_best]\n1544:         self.coef_ = coef_path_[:, n_best]\n1545:         self._set_intercept(Xmean, ymean, Xstd)\n1546:         return self\n\n",
  "line_level_localization": [
    {
      "filename": "/sklearn/linear_model/least_angle.py",
      "suspect_lines": [
        1482,
        1493,
        1504
      ]
    }
  ]
}