{
  "instance_id": "scikit-learn__scikit-learn-15100",
  "problem_statement": "strip_accents_unicode fails to strip accents from strings that are already in NFKD form\n<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn\r\n- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn\r\nFor more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->\r\n\r\nThe `strip_accents=\"unicode\"` feature of `CountVectorizer` and related does not work as expected when it processes strings that contain accents, if those strings are already in NFKD form.\r\n\r\n#### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.feature_extraction.text import strip_accents_unicode\r\n\r\n# This string contains one code point, \"LATIN SMALL LETTER N WITH TILDE\"\r\ns1 = chr(241)\r\n\r\n# This string contains two code points, \"LATIN SMALL LETTER N\" followed by \"COMBINING TILDE\"\r\ns2 = chr(110) + chr(771)\r\n\r\n# They are visually identical, as expected\r\nprint(s1) # => ñ\r\nprint(s2) # => ñ\r\n\r\n# The tilde is removed from s1, as expected\r\nprint(strip_accents_unicode(s1)) # => n\r\n\r\n# But strip_accents_unicode returns s2 unchanged\r\nprint(strip_accents_unicode(s2) == s2) # => True\r\n```\r\n\r\n#### Expected Results\r\n\r\n`s1` and `s2` should both be normalized to the same string, `\"n\"`.\r\n\r\n#### Actual Results\r\n`s2` is not changed, because `strip_accent_unicode` does nothing if the string is already in NFKD form.\r\n\r\n#### Versions\r\n```\r\nSystem:\r\n    python: 3.7.4 (default, Jul  9 2019, 15:11:16)  [GCC 7.4.0]\r\nexecutable: /home/dgrady/.local/share/virtualenvs/profiling-data-exploration--DO1bU6C/bin/python3.7\r\n   machine: Linux-4.4.0-17763-Microsoft-x86_64-with-Ubuntu-18.04-bionic\r\n\r\nPython deps:\r\n       pip: 19.2.2\r\nsetuptools: 41.2.0\r\n   sklearn: 0.21.3\r\n     numpy: 1.17.2\r\n     scipy: 1.3.1\r\n    Cython: None\r\n    pandas: 0.25.1\r\n```\r\n\n",
  "localized_code": "[start of sklearn/feature_extraction/text.py]\n1: # -*- coding: utf-8 -*-\n2: # Authors: Olivier Grisel <olivier.grisel@ensta.org>\n3: #          Mathieu Blondel <mathieu@mblondel.org>\n4: #          Lars Buitinck\n5: #          Robert Layton <robertlayton@gmail.com>\n6: #          Jochen Wersdörfer <jochen@wersdoerfer.de>\n7: #          Roman Sinayev <roman.sinayev@gmail.com>\n8: #\n9: # License: BSD 3 clause\n10: \"\"\"\n11: The :mod:`sklearn.feature_extraction.text` submodule gathers utilities to\n12: build feature vectors from text documents.\n13: \"\"\"\n14: \n15: import array\n16: from collections import defaultdict\n17: from collections.abc import Mapping\n18: from functools import partial\n19: import numbers\n20: from operator import itemgetter\n21: import re\n22: import unicodedata\n23: import warnings\n24: \n25: import numpy as np\n26: import scipy.sparse as sp\n27: \n28: from ..base import BaseEstimator, TransformerMixin\n29: from ..preprocessing import normalize\n30: from .hashing import FeatureHasher\n31: from .stop_words import ENGLISH_STOP_WORDS\n32: from ..utils.validation import check_is_fitted, check_array, FLOAT_DTYPES\n33: from ..utils import _IS_32BIT\n34: from ..utils.fixes import _astype_copy_false\n35: from ..exceptions import ChangedBehaviorWarning, NotFittedError\n36: \n37: \n38: __all__ = ['HashingVectorizer',\n39:            'CountVectorizer',\n40:            'ENGLISH_STOP_WORDS',\n41:            'TfidfTransformer',\n42:            'TfidfVectorizer',\n43:            'strip_accents_ascii',\n44:            'strip_accents_unicode',\n45:            'strip_tags']\n46: \n47: \n48: def _preprocess(doc, accent_function=None, lower=False):\n49:     \"\"\"Chain together an optional series of text preprocessing steps to\n50:     apply to a document.\n51: \n52:     Parameters\n53:     ----------\n54:     doc: str\n55:         The string to preprocess\n56:     accent_function: callable\n57:         Function for handling accented characters. Common strategies include\n58:         normalizing and removing.\n59:     lower: bool\n60:         Whether to use str.lower to lowercase all fo the text\n61: \n62:     Returns\n63:     -------\n64:     doc: str\n65:         preprocessed string\n66:     \"\"\"\n67:     if lower:\n68:         doc = doc.lower()\n69:     if accent_function is not None:\n70:         doc = accent_function(doc)\n71:     return doc\n72: \n73: \n74:     \"\nCode replaced for brevity.\n111: \n112: \n113: \n114: def strip_accents_unicode(s):\n115:     \"\"\"Transform accentuated unicode symbols into their simple counterpart\n116: \n117:     Warning: the python-level loop and join operations make this\n118:     implementation 20 times slower than the strip_accents_ascii basic\n119:     normalization.\n120: \n121:     Parameters\n122:     ----------\n123:     s : string\n124:         The string to strip\n125: \n126:     See also\n127:     --------\n128:     strip_accents_ascii\n129:         Remove accentuated char for any unicode symbol that has a direct\n130:         ASCII equivalent.\n131:     \"\"\"\n132:     normalized = unicodedata.normalize('NFKD', s)\n133:     if normalized == s:\n134:         return s\n135:     else:\n136:         return ''.join([c for c in normalized if not unicodedata.combining(c)])\n137: \n138: \n139:     \"\nCode replaced for brevity.\n156: \n157: \n158: \n159:     \"\nCode replaced for brevity.\n170: \n171: \n172: \n173:     i\nCode replaced for brevity.\n181: \n182: \n183: \n184:     \"\nCode replaced for brevity.\n497: \n498: \n499: \n500:     \"\nCode replaced for brevity.\n779: \n780: \n781: \n782:     \"\nCode replaced for brevity.\n787: \n788: \n789: \n790:     \"\nCode replaced for brevity.\n1264: \n... Code Truncated ...\n\n",
  "line_level_localization": [
    {
      "filename": "/sklearn/feature_extraction/text.py",
      "suspect_lines": [
        132,
        133,
        135
      ]
    }
  ]
}