{
  "instance_id": "django__django-12774",
  "problem_statement": "Allow QuerySet.in_bulk() for fields with total UniqueConstraints.\nDescription\n\t\nIf a field is unique by UniqueConstraint instead of unique=True running in_bulk() on that field will fail.\nConsider:\nclass Article(models.Model):\n\tslug = models.CharField(max_length=255)\n\t\n\tclass Meta:\n\t\tconstraints = [\n\t\t\tmodels.UniqueConstraint(fields=[\"slug\"], name=\"%(app_label)s_%(class)s_slug_unq\")\n\t\t]\n>>> Article.objects.in_bulk(field_name=\"slug\")\nTraceback (most recent call last):\n File \"/usr/local/lib/python3.8/code.py\", line 90, in runcode\n\texec(code, self.locals)\n File \"<console>\", line 1, in <module>\n File \"/app/venv/lib/python3.8/site-packages/django/db/models/manager.py\", line 82, in manager_method\n\treturn getattr(self.get_queryset(), name)(*args, **kwargs)\n File \"/app/venv/lib/python3.8/site-packages/django/db/models/query.py\", line 680, in in_bulk\n\traise ValueError(\"in_bulk()'s field_name must be a unique field but %r isn't.\" % field_name)\nValueError: in_bulk()'s field_name must be a unique field but 'slug' isn't.\nIt should be pretty simple to fix this and I have a patch if accepted.\n",
  "localized_code": "[start of django/db/models/query.py]\n1: \"\"\"\n2: The main QuerySet implementation. This provides the public API for the ORM.\n3: \"\"\"\n4: \n5: import copy\n6: import operator\n7: import warnings\n8: from collections import namedtuple\n9: from functools import lru_cache\n10: from itertools import chain\n11: \n12: from django.conf import settings\n13: from django.core import exceptions\n14: from django.db import (\n15:     DJANGO_VERSION_PICKLE_KEY, IntegrityError, NotSupportedError, connections,\n16:     router, transaction,\n17: )\n18: from django.db.models import AutoField, DateField, DateTimeField, sql\n19: from django.db.models.constants import LOOKUP_SEP\n20: from django.db.models.deletion import Collector\n21: from django.db.models.expressions import Case, Expression, F, Value, When\n22: from django.db.models.functions import Cast, Trunc\n23: from django.db.models.query_utils import FilteredRelation, Q\n24: from django.db.models.sql.constants import CURSOR, GET_ITERATOR_CHUNK_SIZE\n25: from django.db.models.utils import resolve_callables\n26: from django.utils import timezone\n27: from django.utils.functional import cached_property, partition\n28: from django.utils.version import get_version\n29: \n30: # The maximum number of results to fetch in a get() query.\n31: MAX_GET_RESULTS = 21\n32: \n33: # The maximum number of items to display in a QuerySet.__repr__\n34: REPR_OUTPUT_SIZE = 20\n35: \n36: \n37: class BaseIterable:\n38:     def __init__(self, queryset, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):\n39:         self.queryset = queryset\n40:         self.chunked_fetch = chunked_fetch\n41:         self.chunk_size = chunk_size\n42: \n43: \n44: class ModelIterable(BaseIterable):\n45:     \"\"\"Iterable that yields a model instance for each row.\"\"\"\n46: \n47:     def __iter__(self):\n48:         queryset = self.queryset\n49:         db = queryset.db\n50:         compiler = queryset.query.get_compiler(using=db)\n51:         # Execute the query. This will also fill compiler.select, klass_info,\n52:         # and annotations.\n53:         results = compiler.execute_sql(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)\n54:         select, klass_info, annotation_col_map = (compiler.select, compiler.klass_info,\n55:                                                   compiler.annotation_col_map)\n56:         model_cls = klass_info['model']\n57:         select_fields = klass_info['select_fields']\n58:         model_fields_start, model_fields_end = select_fields[0], select_fields[-1] + 1\n59:         init_list = [f[0].target.attname\n60:                      for f in select[model_fields_start:model_fields_end]]\n61:         related_populators = get_related_populators(klass_info, select, db)\n62:         known_related_objects = [\n63:             (field, related_objs, operator.attrgetter(*[\n64:                 field.attname\n65:                 if from_field == 'self' else\n66:                 queryset.model._meta.get_field(from_field).attname\n67:                 for from_field in field.from_fields\n68:             ])) for field, related_objs in queryset._known_related_objects.items()\n69:         ]\n70:         for row in compiler.results_iter(results):\n71:             obj = model_cls.from_db(db, init_list, row[model_fields_start:model_fields_end])\n72:             for rel_populator in related_populators:\n73:                 rel_populator.populate(row, obj)\n74:             if annotation_col_map:\n75:                 for attr_name, col_pos in annotation_col_map.items():\n76:                     setattr(obj, attr_name, row[col_pos])\n77: \n78:             # Add the known related objects to the model.\n79:             for field, rel_objs, rel_getter in known_related_objects:\n80:                 # Avoid overwriting objects loaded by, e.g., select_related().\n81:                 if field.is_cached(obj):\n82:                     continue\n83:                 rel_obj_id = rel_getter(obj)\n84:                 try:\n85:                     rel_obj = rel_objs[rel_obj_id]\n86:                 except KeyError:\n87:                     pass  # May happen in qs1 | qs2 scenarios.\n88:                 else:\n89:                     setattr(obj, field.name, rel_obj)\n90: \n91:             yield obj\n92: \n93: \n94: class ValuesIterable(BaseIterable):\nCode replaced for brevity.\n112: \n113: \n114: \n115: class ValuesListIterable(BaseIterable):\nCode replaced for brevity.\n142: \n143: \n144: \n145: class NamedValuesListIterable(ValuesListIterable):\nCode replaced for brevity.\n168: \n169: \n170: \n171: class FlatValuesListIterable(BaseIterable):\nCode replaced for brevity.\n181: \n182: \n183: \n184: class QuerySet:\n185:     \"\"\"Represent a lazy database lookup for a set of objects.\"\"\"\n186: \n187:     def __init__(self, model=None, query=None, using=None, hints=None):\n188:         self.model = model\n189:         self._db = using\n190:         self._hints = hints or {}\n191:         self._query = query or sql.Query(self.model)\n192:         self._result_cache = None\n193:         self._sticky_filter = False\n194:         self._for_write = False\n195:         self._prefetch_related_lookups = ()\n196:         self._prefetch_done = False\n197:         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}\n198:         self._iterable_class = ModelIterable\n199:         self._fields = None\n200:         self._defer_next_filter = False\n201:         self._deferred_filter = None\n202: \n203:     @property\n204:     def query(self):\n205:         if self._deferred_filter:\n206:             negate, args, kwargs = self._deferred_filter\n207:             self._filter_or_exclude_inplace(negate, *args, **kwargs)\n208:             self._deferred_filter = None\n209:         return self._query\n210: \n211:     @query.setter\n212:     def query(self, value):\n213:         self._query = value\n214: \n215:     def as_manager(cls):\n216:         # Address the circular dependency between `Queryset` and `Manager`.\n217:         from django.db.models.manager import Manager\n218:         manager = Manager.from_queryset(cls)()\n219:         manager._built_with_as_manager = True\n220:         return manager\n221:     as_manager.queryset_only = True\n222:     as_manager = classmethod(as_manager)\n223: \n224:     ########################\n225:     # PYTHON MAGIC METHODS #\n226:     ########################\n227: \n228:     def __deepcopy__(self, memo):\n229:         \"\"\"Don't populate the QuerySet's cache.\"\"\"\n230:         obj = self.__class__()\n231:         for k, v in self.__dict__.items():\n232:             if k == '_result_cache':\n233:                 obj.__dict__[k] = None\n234:             else:\n235:                 obj.__dict__[k] = copy.deepcopy(v, memo)\n236:         return obj\n237: \n238:     def __getstate__(self):\n239:         # Force the cache to be fully populated.\n240:         self._fetch_all()\n241:         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}\n242: \n243:     def __setstate__(self, state):\n244:         msg = None\n245:         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)\n246:         if pickled_version:\n247:             current_version = get_version()\n248:             if current_version != pickled_version:\n249:                 msg = (\n250:                     \"Pickled queryset instance's Django version %s does not \"\n251:                     \"match the current version %s.\" % (pickled_version, current_version)\n252:                 )\n253:         else:\n254:             msg = \"Pickled queryset instance's Django version is not specified.\"\n255: \n256:         if msg:\n257:             warnings.warn(msg, RuntimeWarning, stacklevel=2)\n258: \n259:         self.__dict__.update(state)\n260: \n261:     def __repr__(self):\n262:         data = list(self[:REPR_OUTPUT_SIZE + 1])\n263:         if len(data) > REPR_OUTPUT_SIZE:\n264:             data[-1] = \"...(remaining elements truncated)...\"\n265:         return '<%s %r>' % (self.__class__.__name__, data)\n266: \n267:     def __len__(self):\n268:         self._fetch_all()\n269:         return len(self._result_cache)\n270: \n271:     def __iter__(self):\n272:         \"\"\"\n273:         The queryset iterator protocol uses three nested iterators in the\n274:         default case:\n275:             1. sql.compiler.execute_sql()\n276:                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)\n277:                  using cursor.fetchmany(). This part is responsible for\n278:                  doing some column masking, and returning the rows in chunks.\n279:             2. sql.compiler.results_iter()\n280:                - Returns one row at time. At this point the rows are still just\n281:                  tuples. In some cases the return values are converted to\n282:                  Python values at this location.\n283:             3. self.iterator()\n284:                - Responsible for turning the rows into model objects.\n285:         \"\"\"\n286:         self._fetch_all()\n287:         return iter(self._result_cache)\n288: \n289:     def __bool__(self):\n290:         self._fetch_all()\n291:         return bool(self._result_cache)\n292: \n293:     def __getitem__(self, k):\n294:         \"\"\"Retrieve an item or slice from the set of results.\"\"\"\n295:         if not isinstance(k, (int, slice)):\n296:             raise TypeError(\n297:                 'QuerySet indices must be integers or slices, not %s.'\n298:                 % type(k).__name__\n299:             )\n300:         assert ((not isinstance(k, slice) and (k >= 0)) or\n301:                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and\n302:                  (k.stop is None or k.stop >= 0))), \\\n303:             \"Negative indexing is not supported.\"\n304: \n305:         if self._result_cache is not None:\n306:             return self._result_cache[k]\n307: \n308:         if isinstance(k, slice):\n309:             qs = self._chain()\n310:             if k.start is not None:\n311:                 start = int(k.start)\n312:             else:\n313:                 start = None\n314:             if k.stop is not None:\n315:                 stop = int(k.stop)\n316:             else:\n317:                 stop = None\n318:             qs.query.set_limits(start, stop)\n319:             return list(qs)[::k.step] if k.step else qs\n320: \n321:         qs = self._chain()\n322:         qs.query.set_limits(k, k + 1)\n323:         qs._fetch_all()\n324:         return qs._result_cache[0]\n325: \n326:     def __class_getitem__(cls, *args, **kwargs):\n327:         return cls\n328: \n329:     def __and__(self, other):\n330:         self._merge_sanity_check(other)\n331:         if isinstance(other, EmptyQuerySet):\n332:             return other\n333:         if isinstance(self, EmptyQuerySet):\n334:             return self\n335:         combined = self._chain()\n336:         combined._merge_known_related_objects(other)\n337:         combined.query.combine(other.query, sql.AND)\n338:         return combined\n339: \n340:     def __or__(self, other):\n341:         self._merge_sanity_check(other)\n342:         if isinstance(self, EmptyQuerySet):\n343:             return other\n344:         if isinstance(other, EmptyQuerySet):\n345:             return self\n346:         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))\n347:         combined = query._chain()\n348:         combined._merge_known_related_objects(other)\n349:         if not other.query.can_filter():\n350:             other = other.model._base_manager.filter(pk__in=other.values('pk'))\n351:         combined.query.combine(other.query, sql.OR)\n352:         return combined\n353: \n354:     ####################################\n355:     # METHODS THAT DO DATABASE QUERIES #\n356:     ####################################\n357: \n358:     def _iterator(self, use_chunked_fetch, chunk_size):\n359:         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)\n360: \n361:     def iterator(self, chunk_size=2000):\n362:         \"\"\"\n363:         An iterator over the results from applying this QuerySet to the\n364:         database.\n365:         \"\"\"\n366:         if chunk_size <= 0:\n367:             raise ValueError('Chunk size must be strictly positive.')\n368:         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')\n369:         return self._iterator(use_chunked_fetch, chunk_size)\n370: \n371:     def aggregate(self, *args, **kwargs):\n372:         \"\"\"\n373:         Return a dictionary containing the calculations (aggregation)\n374:         over the current queryset.\n375: \n376:         If args is present the expression is passed as a kwarg using\n377:         the Aggregate object's default alias.\n378:         \"\"\"\n379:         if self.query.distinct_fields:\n380:             raise NotImplementedError(\"aggregate() + distinct(fields) not implemented.\")\n381:         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')\n382:         for arg in args:\n383:             # The default_alias property raises TypeError if default_alias\n384:             # can't be set automatically or AttributeError if it isn't an\n385:             # attribute.\n386:             try:\n387:                 arg.default_alias\n388:             except (AttributeError, TypeError):\n389:                 raise TypeError(\"Complex aggregates require an alias\")\n390:             kwargs[arg.default_alias] = arg\n391: \n392:         query = self.query.chain()\n393:         for (alias, aggregate_expr) in kwargs.items():\n394:             query.add_annotation(aggregate_expr, alias, is_summary=True)\n395:             if not query.annotations[alias].contains_aggregate:\n396:                 raise TypeError(\"%s is not an aggregate expression\" % alias)\n397:         return query.get_aggregation(self.db, kwargs)\n398: \n399:     def count(self):\n400:         \"\"\"\n401:         Perform a SELECT COUNT() and return the number of records as an\n402:         integer.\n403: \n404:         If the QuerySet is already fully cached, return the length of the\n405:         cached results set to avoid multiple SELECT COUNT(*) calls.\n406:         \"\"\"\n407:         if self._result_cache is not None:\n408:             return len(self._result_cache)\n409: \n410:         return self.query.get_count(using=self.db)\n411: \n412:     def get(self, *args, **kwargs):\n413:         \"\"\"\n414:         Perform the query and return a single object matching the given\n415:         keyword arguments.\n416:         \"\"\"\n417:         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)\n418:         if self.query.can_filter() and not self.query.distinct_fields:\n419:             clone = clone.order_by()\n420:         limit = None\n421:         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:\n422:             limit = MAX_GET_RESULTS\n423:             clone.query.set_limits(high=limit)\n424:         num = len(clone)\n425:         if num == 1:\n426:             return clone._result_cache[0]\n427:         if not num:\n428:             raise self.model.DoesNotExist(\n429:                 \"%s matching query does not exist.\" %\n430:                 self.model._meta.object_name\n431:             )\n432:         raise self.model.MultipleObjectsReturned(\n433:             'get() returned more than one %s -- it returned %s!' % (\n434:                 self.model._meta.object_name,\n435:                 num if not limit or num < limit else 'more than %s' % (limit - 1),\n436:             )\n437:         )\n438: \n439:     def create(self, **kwargs):\n440:         \"\"\"\n441:         Create a new object with the given kwargs, saving it to the database\n442:         and returning the created object.\n443:         \"\"\"\n444:         obj = self.model(**kwargs)\n445:         self._for_write = True\n446:         obj.save(force_insert=True, using=self.db)\n447:         return obj\n448: \n449:     def _populate_pk_values(self, objs):\n450:         for obj in objs:\n451:             if obj.pk is None:\n452:                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)\n453: \n454:     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):\n455:         \"\"\"\n456:         Insert each of the instances into the database. Do *not* call\n457:         save() on each of the instances, do not send any pre/post_save\n458:         signals, and do not set the primary key attribute if it is an\n459:         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).\n460:         Multi-table models are not supported.\n461:         \"\"\"\n462:         # When you bulk insert you don't get the primary keys back (if it's an\n463:         # autoincrement, except if can_return_rows_from_bulk_insert=True), so\n464:         # you can't insert into the child tables which references this. There\n465:         # are two workarounds:\n466:         # 1) This could be implemented if you didn't have an autoincrement pk\n467:         # 2) You could do it by doing O(n) normal inserts into the parent\n468:         #    tables to get the primary keys back and then doing a single bulk\n469:         #    insert into the childmost table.\n470:         # We currently set the primary keys on the objects when using\n471:         # PostgreSQL via the RETURNING ID clause. It should be possible for\n472:         # Oracle as well, but the semantics for extracting the primary keys is\n473:         # trickier so it's not done yet.\n474:         assert batch_size is None or batch_size > 0\n475:         # Check that the parents share the same concrete model with the our\n476:         # model to detect the inheritance pattern ConcreteGrandParent ->\n477:         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy\n478:         # would not identify that case as involving multiple tables.\n479:         for parent in self.model._meta.get_parent_list():\n480:             if parent._meta.concrete_model is not self.model._meta.concrete_model:\n481:                 raise ValueError(\"Can't bulk create a multi-table inherited model\")\n482:         if not objs:\n483:             return objs\n484:         self._for_write = True\n485:         connection = connections[self.db]\n486:         opts = self.model._meta\n487:         fields = opts.concrete_fields\n488:         objs = list(objs)\n489:         self._populate_pk_values(objs)\n490:         with transaction.atomic(using=self.db, savepoint=False):\n491:             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n492:             if objs_with_pk:\n493:                 returned_columns = self._batched_insert(\n494:                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,\n495:                 )\n496:                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):\n497:                     for result, field in zip(results, opts.db_returning_fields):\n498:                         if field != opts.pk:\n499:                             setattr(obj_with_pk, field.attname, result)\n500:                 for obj_with_pk in objs_with_pk:\n501:                     obj_with_pk._state.adding = False\n502:                     obj_with_pk._state.db = self.db\n503:             if objs_without_pk:\n504:                 fields = [f for f in fields if not isinstance(f, AutoField)]\n505:                 returned_columns = self._batched_insert(\n506:                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,\n507:                 )\n508:                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:\n509:                     assert len(returned_columns) == len(objs_without_pk)\n510:                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):\n511:                     for result, field in zip(results, opts.db_returning_fields):\n512:                         setattr(obj_without_pk, field.attname, result)\n513:                     obj_without_pk._state.adding = False\n514:                     obj_without_pk._state.db = self.db\n515: \n516:         return objs\n517: \n518:     def bulk_update(self, objs, fields, batch_size=None):\n519:         \"\"\"\n520:         Update the given fields in each of the given objects in the database.\n521:         \"\"\"\n522:         if batch_size is not None and batch_size < 0:\n523:             raise ValueError('Batch size must be a positive integer.')\n524:         if not fields:\n525:             raise ValueError('Field names must be given to bulk_update().')\n526:         objs = tuple(objs)\n527:         if any(obj.pk is None for obj in objs):\n528:             raise ValueError('All bulk_update() objects must have a primary key set.')\n529:         fields = [self.model._meta.get_field(name) for name in fields]\n530:         if any(not f.concrete or f.many_to_many for f in fields):\n531:             raise ValueError('bulk_update() can only be used with concrete fields.')\n532:         if any(f.primary_key for f in fields):\n533:             raise ValueError('bulk_update() cannot be used with primary key fields.')\n534:         if not objs:\n535:             return\n536:         # PK is used twice in the resulting update query, once in the filter\n537:         # and once in the WHEN. Each field will also have one CAST.\n538:         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)\n539:         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n540:         requires_casting = connections[self.db].features.requires_casted_case_in_updates\n541:         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))\n542:         updates = []\n543:         for batch_objs in batches:\n544:             update_kwargs = {}\n545:             for field in fields:\n546:                 when_statements = []\n547:                 for obj in batch_objs:\n548:                     attr = getattr(obj, field.attname)\n549:                     if not isinstance(attr, Expression):\n550:                         attr = Value(attr, output_field=field)\n551:                     when_statements.append(When(pk=obj.pk, then=attr))\n552:                 case_statement = Case(*when_statements, output_field=field)\n553:                 if requires_casting:\n554:                     case_statement = Cast(case_statement, output_field=field)\n555:                 update_kwargs[field.attname] = case_statement\n556:             updates.append(([obj.pk for obj in batch_objs], update_kwargs))\n557:         with transaction.atomic(using=self.db, savepoint=False):\n558:             for pks, update_kwargs in updates:\n559:                 self.filter(pk__in=pks).update(**update_kwargs)\n560:     bulk_update.alters_data = True\n561: \n562:     def get_or_create(self, defaults=None, **kwargs):\n563:         \"\"\"\n564:         Look up an object with the given kwargs, creating one if necessary.\n565:         Return a tuple of (object, created), where created is a boolean\n566:         specifying whether an object was created.\n567:         \"\"\"\n568:         # The get() needs to be targeted at the write database in order\n569:         # to avoid potential transaction consistency problems.\n570:         self._for_write = True\n571:         try:\n572:             return self.get(**kwargs), False\n573:         except self.model.DoesNotExist:\n574:             params = self._extract_model_params(defaults, **kwargs)\n575:             return self._create_object_from_params(kwargs, params)\n576: \n577:     def update_or_create(self, defaults=None, **kwargs):\n578:         \"\"\"\n579:         Look up an object with the given kwargs, updating one with defaults\n580:         if it exists, otherwise create a new one.\n581:         Return a tuple (object, created), where created is a boolean\n582:         specifying whether an object was created.\n583:         \"\"\"\n584:         defaults = defaults or {}\n585:         self._for_write = True\n586:         with transaction.atomic(using=self.db):\n587:             try:\n588:                 obj = self.select_for_update().get(**kwargs)\n589:             except self.model.DoesNotExist:\n590:                 params = self._extract_model_params(defaults, **kwargs)\n591:                 # Lock the row so that a concurrent update is blocked until\n592:                 # after update_or_create() has performed its save.\n593:                 obj, created = self._create_object_from_params(kwargs, params, lock=True)\n594:                 if created:\n595:                     return obj, created\n596:             for k, v in resolve_callables(defaults):\n597:                 setattr(obj, k, v)\n598:             obj.save(using=self.db)\n599:         return obj, False\n600: \n601:     def _create_object_from_params(self, lookup, params, lock=False):\n602:         \"\"\"\n603:         Try to create an object using passed params. Used by get_or_create()\n604:         and update_or_create().\n605:         \"\"\"\n606:         try:\n607:             with transaction.atomic(using=self.db):\n608:                 params = dict(resolve_callables(params))\n609:                 obj = self.create(**params)\n610:             return obj, True\n611:         except IntegrityError:\n612:             try:\n613:                 qs = self.select_for_update() if lock else self\n614:                 return qs.get(**lookup), False\n615:             except self.model.DoesNotExist:\n616:                 pass\n617:             raise\n618: \n619:     def _extract_model_params(self, defaults, **kwargs):\n620:         \"\"\"\n621:         Prepare `params` for creating a model instance based on the given\n622:         kwargs; for use by get_or_create() and update_or_create().\n623:         \"\"\"\n624:         defaults = defaults or {}\n625:         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}\n626:         params.update(defaults)\n627:         property_names = self.model._meta._property_names\n628:         invalid_params = []\n629:         for param in params:\n630:             try:\n631:                 self.model._meta.get_field(param)\n632:             except exceptions.FieldDoesNotExist:\n633:                 # It's okay to use a model's property if it has a setter.\n634:                 if not (param in property_names and getattr(self.model, param).fset):\n635:                     invalid_params.append(param)\n636:         if invalid_params:\n637:             raise exceptions.FieldError(\n638:                 \"Invalid field name(s) for model %s: '%s'.\" % (\n639:                     self.model._meta.object_name,\n640:                     \"', '\".join(sorted(invalid_params)),\n641:                 ))\n642:         return params\n643: \n644:     def _earliest(self, *fields):\n645:         \"\"\"\n646:         Return the earliest object according to fields (if given) or by the\n647:         model's Meta.get_latest_by.\n648:         \"\"\"\n649:         if fields:\n650:             order_by = fields\n651:         else:\n652:             order_by = getattr(self.model._meta, 'get_latest_by')\n653:             if order_by and not isinstance(order_by, (tuple, list)):\n654:                 order_by = (order_by,)\n655:         if order_by is None:\n656:             raise ValueError(\n657:                 \"earliest() and latest() require either fields as positional \"\n658:                 \"arguments or 'get_latest_by' in the model's Meta.\"\n659:             )\n660: \n661:         assert not self.query.is_sliced, \\\n662:             \"Cannot change a query once a slice has been taken.\"\n663:         obj = self._chain()\n664:         obj.query.set_limits(high=1)\n665:         obj.query.clear_ordering(force_empty=True)\n666:         obj.query.add_ordering(*order_by)\n667:         return obj.get()\n668: \n669:     def earliest(self, *fields):\n670:         return self._earliest(*fields)\n671: \n672:     def latest(self, *fields):\n673:         return self.reverse()._earliest(*fields)\n674: \n675:     def first(self):\n676:         \"\"\"Return the first object of a query or None if no match is found.\"\"\"\n677:         for obj in (self if self.ordered else self.order_by('pk'))[:1]:\n678:             return obj\n679: \n680:     def last(self):\n681:         \"\"\"Return the last object of a query or None if no match is found.\"\"\"\n682:         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:\n683:             return obj\n684: \n685:     def in_bulk(self, id_list=None, *, field_name='pk'):\n686:         \"\"\"\n687:         Return a dictionary mapping each of the given IDs to the object with\n688:         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.\n689:         \"\"\"\n690:         assert not self.query.is_sliced, \\\n691:             \"Cannot use 'limit' or 'offset' with in_bulk\"\n692:         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:\n693:             raise ValueError(\"in_bulk()'s field_name must be a unique field but %r isn't.\" % field_name)\n694:         if id_list is not None:\n695:             if not id_list:\n696:                 return {}\n697:             filter_key = '{}__in'.format(field_name)\n698:             batch_size = connections[self.db].features.max_query_params\n699:             id_list = tuple(id_list)\n700:             # If the database has a limit on the number of query parameters\n701:             # (e.g. SQLite), retrieve objects in batches if necessary.\n702:             if batch_size and batch_size < len(id_list):\n703:                 qs = ()\n704:                 for offset in range(0, len(id_list), batch_size):\n705:                     batch = id_list[offset:offset + batch_size]\n706:                     qs += tuple(self.filter(**{filter_key: batch}).order_by())\n707:             else:\n708:                 qs = self.filter(**{filter_key: id_list}).order_by()\n709:         else:\n710:             qs = self._chain()\n711:         return {getattr(obj, field_name): obj for obj in qs}\n712: \n713:     def delete(self):\n714:         \"\"\"Delete the records in the current QuerySet.\"\"\"\n715:         self._not_support_combined_queries('delete')\n716:         assert not self.query.is_sliced, \\\n717:             \"Cannot use 'limit' or 'offset' with delete.\"\n718: \n719:         if self._fields is not None:\n720:             raise TypeError(\"Cannot call delete() after .values() or .values_list()\")\n721: \n722:         del_query = self._chain()\n723: \n724:         # The delete is actually 2 queries - one to find related objects,\n725:         # and one to delete. Make sure that the discovery of related\n726:         # objects is performed on the same database as the deletion.\n727:         del_query._for_write = True\n728: \n729:         # Disable non-supported fields.\n730:         del_query.query.select_for_update = False\n731:         del_query.query.select_related = False\n732:         del_query.query.clear_ordering(force_empty=True)\n733: \n734:         collector = Collector(using=del_query.db)\n735:         collector.collect(del_query)\n736:         deleted, _rows_count = collector.delete()\n737: \n738:         # Clear the result cache, in case this QuerySet gets reused.\n739:         self._result_cache = None\n740:         return deleted, _rows_count\n741: \n742:     delete.alters_data = True\n743:     delete.queryset_only = True\n744: \n745:     def _raw_delete(self, using):\n746:         \"\"\"\n747:         Delete objects found from the given queryset in single direct SQL\n748:         query. No signals are sent and there is no protection for cascades.\n749:         \"\"\"\n750:         query = self.query.clone()\n751:         query.__class__ = sql.DeleteQuery\n752:         cursor = query.get_compiler(using).execute_sql(CURSOR)\n753:         if cursor:\n754:             with cursor:\n755:                 return cursor.rowcount\n756:         return 0\n757:     _raw_delete.alters_data = True\n758: \n759:     def update(self, **kwargs):\n760:         \"\"\"\n761:         Update all elements in the current QuerySet, setting all the given\n762:         fields to the appropriate values.\n763:         \"\"\"\n... Code Truncated ...\n\n",
  "line_level_localization": [
    {
      "filename": "/django/db/models/query.py",
      "suspect_lines": [
        692
      ]
    }
  ]
}