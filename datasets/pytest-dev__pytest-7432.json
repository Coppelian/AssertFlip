{
  "instance_id": "pytest-dev__pytest-7432",
  "problem_statement": "skipping: --runxfail breaks pytest.mark.skip location reporting\npytest versions: 5.4.x, current master\r\n\r\nWhen `@pytest.mark.skip`/`skipif` marks are used to skip a test, for example\r\n\r\n```py\r\nimport pytest\r\n@pytest.mark.skip\r\ndef test_skip_location() -> None:\r\n    assert 0\r\n```\r\n\r\nthe expected skip location reported should point to the item itself, and this is indeed what happens when running with `pytest -rs`:\r\n\r\n```\r\nSKIPPED [1] test_it.py:3: unconditional skip\r\n```\r\n\r\nHowever, adding `pytest -rs --runxfail` breaks this:\r\n\r\n```\r\nSKIPPED [1] src/_pytest/skipping.py:238: unconditional skip\r\n```\r\n\r\nThe `--runxfail` is only about xfail and should not affect this at all.\r\n\r\n---\r\n\r\nHint: the bug is in `src/_pytest/skipping.py`, the `pytest_runtest_makereport` hook.\n",
  "localized_code": "[start of src/_pytest/skipping.py]\n1: \"\"\" support for skip/xfail functions and markers. \"\"\"\n2: import os\n3: import platform\n4: import sys\n5: import traceback\n6: from typing import Generator\n7: from typing import Optional\n8: from typing import Tuple\n9: \n10: import attr\n11: \n12: import _pytest._code\n13: from _pytest.compat import TYPE_CHECKING\n14: from _pytest.config import Config\n15: from _pytest.config import hookimpl\n16: from _pytest.config.argparsing import Parser\n17: from _pytest.mark.structures import Mark\n18: from _pytest.nodes import Item\n19: from _pytest.outcomes import fail\n20: from _pytest.outcomes import skip\n21: from _pytest.outcomes import xfail\n22: from _pytest.reports import BaseReport\n23: from _pytest.runner import CallInfo\n24: from _pytest.store import StoreKey\n25: \n26: if TYPE_CHECKING:\n27:     from typing import Type\n28: \n29: \n30: def pytest_addoption(parser: Parser) -> None:\n31:     group = parser.getgroup(\"general\")\n32:     group.addoption(\n33:         \"--runxfail\",\n34:         action=\"store_true\",\n35:         dest=\"runxfail\",\n36:         default=False,\n37:         help=\"report the results of xfail tests as if they were not marked\",\n38:     )\n39: \n40:     parser.addini(\n41:         \"xfail_strict\",\n42:         \"default for the strict parameter of xfail \"\n43:         \"markers when not given explicitly (default: False)\",\n44:         default=False,\n45:         type=\"bool\",\n46:     )\n47: \n48: \n49: def pytest_configure(config: Config) -> None:\n50:     if config.option.runxfail:\n51:         # yay a hack\n52:         import pytest\n53: \n54:         old = pytest.xfail\n55:         config._cleanup.append(lambda: setattr(pytest, \"xfail\", old))\n56: \n57:         def nop(*args, **kwargs):\n58:             pass\n59: \n60:         nop.Exception = xfail.Exception  # type: ignore[attr-defined] # noqa: F821\n61:         setattr(pytest, \"xfail\", nop)\n62: \n63:     config.addinivalue_line(\n64:         \"markers\",\n65:         \"skip(reason=None): skip the given test function with an optional reason. \"\n66:         'Example: skip(reason=\"no way of currently testing this\") skips the '\n67:         \"test.\",\n68:     )\n69:     config.addinivalue_line(\n70:         \"markers\",\n71:         \"skipif(condition, ..., *, reason=...): \"\n72:         \"skip the given test function if any of the conditions evaluate to True. \"\n73:         \"Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. \"\n74:         \"See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif\",\n75:     )\n76:     config.addinivalue_line(\n77:         \"markers\",\n78:         \"xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): \"\n79:         \"mark the test function as an expected failure if any of the conditions \"\n80:         \"evaluate to True. Optionally specify a reason for better reporting \"\n81:         \"and run=False if you don't even want to execute the test function. \"\n82:         \"If only specific exception(s) are expected, you can list them in \"\n83:         \"raises, and if the test fails in other ways, it will be reported as \"\n84:         \"a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail\",\n85:     )\n86: \n87: \n88: def evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:\nCode replaced for brevity.\n149: \n150: \n151: \n152: @attr.s(slots=True, frozen=True)\n153: class Skip:\nCode replaced for brevity.\n156: \n157: \n158: \n159: def evaluate_skip_marks(item: Item) -> Optional[Skip]:\nCode replaced for brevity.\n187: \n188: \n189: \n190: @attr.s(slots=True, frozen=True)\n191: class Xfail:\nCode replaced for brevity.\n197: \n198: \n199: \n200: def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\nCode replaced for brevity.\n222: \n223: \n224: \n225: # Whether skipped due to skip or skipif marks.\n226: skipped_by_mark_key = StoreKey[bool]()\n227: # Saves the xfail mark evaluation. Can be refreshed during call if None.\n228: xfailed_key = StoreKey[Optional[Xfail]]()\n229: unexpectedsuccess_key = StoreKey[str]()\n230: \n231: \n232: @hookimpl(tryfirst=True)\n233: def pytest_runtest_setup(item: Item) -> None:\nCode replaced for brevity.\n244: \n245: \n246: \n247: @hookimpl(hookwrapper=True)\n248: def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\nCode replaced for brevity.\n257: \n258: \n259: \n260: @hookimpl(hookwrapper=True)\n261: def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n262:     outcome = yield\n263:     rep = outcome.get_result()\n264:     xfailed = item._store.get(xfailed_key, None)\n265:     # unittest special case, see setting of unexpectedsuccess_key\n266:     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n267:         reason = item._store[unexpectedsuccess_key]\n268:         if reason:\n269:             rep.longrepr = \"Unexpected success: {}\".format(reason)\n270:         else:\n271:             rep.longrepr = \"Unexpected success\"\n272:         rep.outcome = \"failed\"\n273:     elif item.config.option.runxfail:\n274:         pass  # don't interfere\n275:     elif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):\n276:         assert call.excinfo.value.msg is not None\n277:         rep.wasxfail = \"reason: \" + call.excinfo.value.msg\n278:         rep.outcome = \"skipped\"\n279:     elif not rep.skipped and xfailed:\n280:         if call.excinfo:\n281:             raises = xfailed.raises\n282:             if raises is not None and not isinstance(call.excinfo.value, raises):\n283:                 rep.outcome = \"failed\"\n284:             else:\n285:                 rep.outcome = \"skipped\"\n286:                 rep.wasxfail = xfailed.reason\n287:         elif call.when == \"call\":\n288:             if xfailed.strict:\n289:                 rep.outcome = \"failed\"\n290:                 rep.longrepr = \"[XPASS(strict)] \" + xfailed.reason\n291:             else:\n292:                 rep.outcome = \"passed\"\n293:                 rep.wasxfail = xfailed.reason\n294:     elif (\n295:         item._store.get(skipped_by_mark_key, True)\n296:         and rep.skipped\n297:         and type(rep.longrepr) is tuple\n298:     ):\n299:         # skipped by mark.skipif; change the location of the failure\n300:         # to point to the item definition, otherwise it will display\n301:         # the location of where the skip exception was raised within pytest\n302:         _, _, reason = rep.longrepr\n303:         filename, line = item.reportinfo()[:2]\n304:         assert line is not None\n305:         rep.longrepr = str(filename), line + 1, reason\n306: \n307: \n308: def pytest_report_teststatus(report: BaseReport) -> Optional[Tuple[str, str, str]]:\nCode replaced for brevity.\n314: \n\n",
  "line_level_localization": [
    {
      "filename": "/src/_pytest/skipping.py",
      "suspect_lines": [
        294
      ]
    }
  ]
}