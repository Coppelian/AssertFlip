{
  "instance_id": "django__django-15368",
  "problem_statement": "bulk_update() does not work with plain F('...') expressions.\nDescription\n\t\nRepro:\nassign plain F(...) to some model instance field\nsave with bulk_update\nExample:\nCode highlighting:\n>>> from exampleapp.models import SelfRef\n>>> o = SelfRef.objects.all().first()\n>>> o.c8 = F('name')\t# model has char fields 'c8' and 'name'\n>>> SelfRef.objects.bulk_update([o], ['c8'])\n1\n>>> o.refresh_from_db()\n>>> o.c8\n'F(name)'\n>>> from django.db import connection\n>>> connection.queries[-2]\n{'sql': 'UPDATE \"exampleapp_selfref\" SET \"c8\" = CASE WHEN (\"exampleapp_selfref\".\"id\" = 1290012) THEN \\'F(name)\\' ELSE NULL END WHERE \"exampleapp_selfref\".\"id\" IN (1290012)', 'time': '0.001'}\nThe created SQL contains the string repr of F(), instead of resolving to the column name. Looking at the source code, the culprit seems to be a too narrow type check in â€‹https://github.com/django/django/blob/2eed554c3fd75dae1beade79f357ffd18d3c4fdf/django/db/models/query.py#L673.\nIt works, if the type check gets replaced by one of these:\nCode highlighting:\n# either do duck type testing\nif not hasattr(attr, 'resolve_expression'):\n\t...\n# or test for F explicitly:\nif not isinstance(attr, (Expression, F)):\n\t...\n",
  "localized_code": "[start of django/db/models/query.py]\n1: \"\"\"\n2: The main QuerySet implementation. This provides the public API for the ORM.\n3: \"\"\"\n4: \n5: import copy\n6: import operator\n7: import warnings\n8: from itertools import chain, islice\n9: \n10: import django\n11: from django.conf import settings\n12: from django.core import exceptions\n13: from django.db import (\n14:     DJANGO_VERSION_PICKLE_KEY, IntegrityError, NotSupportedError, connections,\n15:     router, transaction,\n16: )\n17: from django.db.models import AutoField, DateField, DateTimeField, sql\n18: from django.db.models.constants import LOOKUP_SEP, OnConflict\n19: from django.db.models.deletion import Collector\n20: from django.db.models.expressions import Case, Expression, F, Ref, Value, When\n21: from django.db.models.functions import Cast, Trunc\n22: from django.db.models.query_utils import FilteredRelation, Q\n23: from django.db.models.sql.constants import CURSOR, GET_ITERATOR_CHUNK_SIZE\n24: from django.db.models.utils import create_namedtuple_class, resolve_callables\n25: from django.utils import timezone\n26: from django.utils.deprecation import RemovedInDjango50Warning\n27: from django.utils.functional import cached_property, partition\n28: \n29: # The maximum number of results to fetch in a get() query.\n30: MAX_GET_RESULTS = 21\n31: \n32: # The maximum number of items to display in a QuerySet.__repr__\n33: REPR_OUTPUT_SIZE = 20\n34: \n35: \n36: class BaseIterable:\n37:     def __init__(self, queryset, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):\n38:         self.queryset = queryset\n39:         self.chunked_fetch = chunked_fetch\n40:         self.chunk_size = chunk_size\n41: \n42: \n43: class ModelIterable(BaseIterable):\n44:     \"\"\"Iterable that yields a model instance for each row.\"\"\"\n45: \n46:     def __iter__(self):\n47:         queryset = self.queryset\n48:         db = queryset.db\n49:         compiler = queryset.query.get_compiler(using=db)\n50:         # Execute the query. This will also fill compiler.select, klass_info,\n51:         # and annotations.\n52:         results = compiler.execute_sql(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)\n53:         select, klass_info, annotation_col_map = (compiler.select, compiler.klass_info,\n54:                                                   compiler.annotation_col_map)\n55:         model_cls = klass_info['model']\n56:         select_fields = klass_info['select_fields']\n57:         model_fields_start, model_fields_end = select_fields[0], select_fields[-1] + 1\n58:         init_list = [f[0].target.attname\n59:                      for f in select[model_fields_start:model_fields_end]]\n60:         related_populators = get_related_populators(klass_info, select, db)\n61:         known_related_objects = [\n62:             (field, related_objs, operator.attrgetter(*[\n63:                 field.attname\n64:                 if from_field == 'self' else\n65:                 queryset.model._meta.get_field(from_field).attname\n66:                 for from_field in field.from_fields\n67:             ])) for field, related_objs in queryset._known_related_objects.items()\n68:         ]\n69:         for row in compiler.results_iter(results):\n70:             obj = model_cls.from_db(db, init_list, row[model_fields_start:model_fields_end])\n71:             for rel_populator in related_populators:\n72:                 rel_populator.populate(row, obj)\n73:             if annotation_col_map:\n74:                 for attr_name, col_pos in annotation_col_map.items():\n75:                     setattr(obj, attr_name, row[col_pos])\n76: \n77:             # Add the known related objects to the model.\n78:             for field, rel_objs, rel_getter in known_related_objects:\n79:                 # Avoid overwriting objects loaded by, e.g., select_related().\n80:                 if field.is_cached(obj):\n81:                     continue\n82:                 rel_obj_id = rel_getter(obj)\n83:                 try:\n84:                     rel_obj = rel_objs[rel_obj_id]\n85:                 except KeyError:\n86:                     pass  # May happen in qs1 | qs2 scenarios.\n87:                 else:\n88:                     setattr(obj, field.name, rel_obj)\n89: \n90:             yield obj\n91: \n92: \n93: class ValuesIterable(BaseIterable):\nCode replaced for brevity.\n111: \n112: \n113: \n114: class ValuesListIterable(BaseIterable):\nCode replaced for brevity.\n141: \n142: \n143: \n144: class NamedValuesListIterable(ValuesListIterable):\nCode replaced for brevity.\n160: \n161: \n162: \n163: class FlatValuesListIterable(BaseIterable):\nCode replaced for brevity.\n173: \n174: \n175: \n176: class QuerySet:\n177:     \"\"\"Represent a lazy database lookup for a set of objects.\"\"\"\n178: \n179:     def __init__(self, model=None, query=None, using=None, hints=None):\n180:         self.model = model\n181:         self._db = using\n182:         self._hints = hints or {}\n183:         self._query = query or sql.Query(self.model)\n184:         self._result_cache = None\n185:         self._sticky_filter = False\n186:         self._for_write = False\n187:         self._prefetch_related_lookups = ()\n188:         self._prefetch_done = False\n189:         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}\n190:         self._iterable_class = ModelIterable\n191:         self._fields = None\n192:         self._defer_next_filter = False\n193:         self._deferred_filter = None\n194: \n195:     @property\n196:     def query(self):\n197:         if self._deferred_filter:\n198:             negate, args, kwargs = self._deferred_filter\n199:             self._filter_or_exclude_inplace(negate, args, kwargs)\n200:             self._deferred_filter = None\n201:         return self._query\n202: \n203:     @query.setter\n204:     def query(self, value):\n205:         if value.values_select:\n206:             self._iterable_class = ValuesIterable\n207:         self._query = value\n208: \n209:     def as_manager(cls):\n210:         # Address the circular dependency between `Queryset` and `Manager`.\n211:         from django.db.models.manager import Manager\n212:         manager = Manager.from_queryset(cls)()\n213:         manager._built_with_as_manager = True\n214:         return manager\n215:     as_manager.queryset_only = True\n216:     as_manager = classmethod(as_manager)\n217: \n218:     ########################\n219:     # PYTHON MAGIC METHODS #\n220:     ########################\n221: \n222:     def __deepcopy__(self, memo):\n223:         \"\"\"Don't populate the QuerySet's cache.\"\"\"\n224:         obj = self.__class__()\n225:         for k, v in self.__dict__.items():\n226:             if k == '_result_cache':\n227:                 obj.__dict__[k] = None\n228:             else:\n229:                 obj.__dict__[k] = copy.deepcopy(v, memo)\n230:         return obj\n231: \n232:     def __getstate__(self):\n233:         # Force the cache to be fully populated.\n234:         self._fetch_all()\n235:         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}\n236: \n237:     def __setstate__(self, state):\n238:         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)\n239:         if pickled_version:\n240:             if pickled_version != django.__version__:\n241:                 warnings.warn(\n242:                     \"Pickled queryset instance's Django version %s does not \"\n243:                     \"match the current version %s.\"\n244:                     % (pickled_version, django.__version__),\n245:                     RuntimeWarning,\n246:                     stacklevel=2,\n247:                 )\n248:         else:\n249:             warnings.warn(\n250:                 \"Pickled queryset instance's Django version is not specified.\",\n251:                 RuntimeWarning,\n252:                 stacklevel=2,\n253:             )\n254:         self.__dict__.update(state)\n255: \n256:     def __repr__(self):\n257:         data = list(self[:REPR_OUTPUT_SIZE + 1])\n258:         if len(data) > REPR_OUTPUT_SIZE:\n259:             data[-1] = \"...(remaining elements truncated)...\"\n260:         return '<%s %r>' % (self.__class__.__name__, data)\n261: \n262:     def __len__(self):\n263:         self._fetch_all()\n264:         return len(self._result_cache)\n265: \n266:     def __iter__(self):\n267:         \"\"\"\n268:         The queryset iterator protocol uses three nested iterators in the\n269:         default case:\n270:             1. sql.compiler.execute_sql()\n271:                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)\n272:                  using cursor.fetchmany(). This part is responsible for\n273:                  doing some column masking, and returning the rows in chunks.\n274:             2. sql.compiler.results_iter()\n275:                - Returns one row at time. At this point the rows are still just\n276:                  tuples. In some cases the return values are converted to\n277:                  Python values at this location.\n278:             3. self.iterator()\n279:                - Responsible for turning the rows into model objects.\n280:         \"\"\"\n281:         self._fetch_all()\n282:         return iter(self._result_cache)\n283: \n284:     def __bool__(self):\n285:         self._fetch_all()\n286:         return bool(self._result_cache)\n287: \n288:     def __getitem__(self, k):\n289:         \"\"\"Retrieve an item or slice from the set of results.\"\"\"\n290:         if not isinstance(k, (int, slice)):\n291:             raise TypeError(\n292:                 'QuerySet indices must be integers or slices, not %s.'\n293:                 % type(k).__name__\n294:             )\n295:         if (\n296:             (isinstance(k, int) and k < 0) or\n297:             (isinstance(k, slice) and (\n298:                 (k.start is not None and k.start < 0) or\n299:                 (k.stop is not None and k.stop < 0)\n300:             ))\n301:         ):\n302:             raise ValueError('Negative indexing is not supported.')\n303: \n304:         if self._result_cache is not None:\n305:             return self._result_cache[k]\n306: \n307:         if isinstance(k, slice):\n308:             qs = self._chain()\n309:             if k.start is not None:\n310:                 start = int(k.start)\n311:             else:\n312:                 start = None\n313:             if k.stop is not None:\n314:                 stop = int(k.stop)\n315:             else:\n316:                 stop = None\n317:             qs.query.set_limits(start, stop)\n318:             return list(qs)[::k.step] if k.step else qs\n319: \n320:         qs = self._chain()\n321:         qs.query.set_limits(k, k + 1)\n322:         qs._fetch_all()\n323:         return qs._result_cache[0]\n324: \n325:     def __class_getitem__(cls, *args, **kwargs):\n326:         return cls\n327: \n328:     def __and__(self, other):\n329:         self._check_operator_queryset(other, '&')\n330:         self._merge_sanity_check(other)\n331:         if isinstance(other, EmptyQuerySet):\n332:             return other\n333:         if isinstance(self, EmptyQuerySet):\n334:             return self\n335:         combined = self._chain()\n336:         combined._merge_known_related_objects(other)\n337:         combined.query.combine(other.query, sql.AND)\n338:         return combined\n339: \n340:     def __or__(self, other):\n341:         self._check_operator_queryset(other, '|')\n342:         self._merge_sanity_check(other)\n343:         if isinstance(self, EmptyQuerySet):\n344:             return other\n345:         if isinstance(other, EmptyQuerySet):\n346:             return self\n347:         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))\n348:         combined = query._chain()\n349:         combined._merge_known_related_objects(other)\n350:         if not other.query.can_filter():\n351:             other = other.model._base_manager.filter(pk__in=other.values('pk'))\n352:         combined.query.combine(other.query, sql.OR)\n353:         return combined\n354: \n355:     ####################################\n356:     # METHODS THAT DO DATABASE QUERIES #\n357:     ####################################\n358: \n359:     def _iterator(self, use_chunked_fetch, chunk_size):\n360:         iterable = self._iterable_class(\n361:             self,\n362:             chunked_fetch=use_chunked_fetch,\n363:             chunk_size=chunk_size or 2000,\n364:         )\n365:         if not self._prefetch_related_lookups or chunk_size is None:\n366:             yield from iterable\n367:             return\n368: \n369:         iterator = iter(iterable)\n370:         while results := list(islice(iterator, chunk_size)):\n371:             prefetch_related_objects(results, *self._prefetch_related_lookups)\n372:             yield from results\n373: \n374:     def iterator(self, chunk_size=None):\n375:         \"\"\"\n376:         An iterator over the results from applying this QuerySet to the\n377:         database. chunk_size must be provided for QuerySets that prefetch\n378:         related objects. Otherwise, a default chunk_size of 2000 is supplied.\n379:         \"\"\"\n380:         if chunk_size is None:\n381:             if self._prefetch_related_lookups:\n382:                 # When the deprecation ends, replace with:\n383:                 # raise ValueError(\n384:                 #     'chunk_size must be provided when using '\n385:                 #     'QuerySet.iterator() after prefetch_related().'\n386:                 # )\n387:                 warnings.warn(\n388:                     'Using QuerySet.iterator() after prefetch_related() '\n389:                     'without specifying chunk_size is deprecated.',\n390:                     category=RemovedInDjango50Warning,\n391:                     stacklevel=2,\n392:                 )\n393:         elif chunk_size <= 0:\n394:             raise ValueError('Chunk size must be strictly positive.')\n395:         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')\n396:         return self._iterator(use_chunked_fetch, chunk_size)\n397: \n398:     def aggregate(self, *args, **kwargs):\n399:         \"\"\"\n400:         Return a dictionary containing the calculations (aggregation)\n401:         over the current queryset.\n402: \n403:         If args is present the expression is passed as a kwarg using\n404:         the Aggregate object's default alias.\n405:         \"\"\"\n406:         if self.query.distinct_fields:\n407:             raise NotImplementedError(\"aggregate() + distinct(fields) not implemented.\")\n408:         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')\n409:         for arg in args:\n410:             # The default_alias property raises TypeError if default_alias\n411:             # can't be set automatically or AttributeError if it isn't an\n412:             # attribute.\n413:             try:\n414:                 arg.default_alias\n415:             except (AttributeError, TypeError):\n416:                 raise TypeError(\"Complex aggregates require an alias\")\n417:             kwargs[arg.default_alias] = arg\n418: \n419:         query = self.query.chain()\n420:         for (alias, aggregate_expr) in kwargs.items():\n421:             query.add_annotation(aggregate_expr, alias, is_summary=True)\n422:             annotation = query.annotations[alias]\n423:             if not annotation.contains_aggregate:\n424:                 raise TypeError(\"%s is not an aggregate expression\" % alias)\n425:             for expr in annotation.get_source_expressions():\n426:                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:\n427:                     name = expr.refs\n428:                     raise exceptions.FieldError(\n429:                         \"Cannot compute %s('%s'): '%s' is an aggregate\"\n430:                         % (annotation.name, name, name)\n431:                     )\n432:         return query.get_aggregation(self.db, kwargs)\n433: \n434:     def count(self):\n435:         \"\"\"\n436:         Perform a SELECT COUNT() and return the number of records as an\n437:         integer.\n438: \n439:         If the QuerySet is already fully cached, return the length of the\n440:         cached results set to avoid multiple SELECT COUNT(*) calls.\n441:         \"\"\"\n442:         if self._result_cache is not None:\n443:             return len(self._result_cache)\n444: \n445:         return self.query.get_count(using=self.db)\n446: \n447:     def get(self, *args, **kwargs):\n448:         \"\"\"\n449:         Perform the query and return a single object matching the given\n450:         keyword arguments.\n451:         \"\"\"\n452:         if self.query.combinator and (args or kwargs):\n453:             raise NotSupportedError(\n454:                 'Calling QuerySet.get(...) with filters after %s() is not '\n455:                 'supported.' % self.query.combinator\n456:             )\n457:         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)\n458:         if self.query.can_filter() and not self.query.distinct_fields:\n459:             clone = clone.order_by()\n460:         limit = None\n461:         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:\n462:             limit = MAX_GET_RESULTS\n463:             clone.query.set_limits(high=limit)\n464:         num = len(clone)\n465:         if num == 1:\n466:             return clone._result_cache[0]\n467:         if not num:\n468:             raise self.model.DoesNotExist(\n469:                 \"%s matching query does not exist.\" %\n470:                 self.model._meta.object_name\n471:             )\n472:         raise self.model.MultipleObjectsReturned(\n473:             'get() returned more than one %s -- it returned %s!' % (\n474:                 self.model._meta.object_name,\n475:                 num if not limit or num < limit else 'more than %s' % (limit - 1),\n476:             )\n477:         )\n478: \n479:     def create(self, **kwargs):\n480:         \"\"\"\n481:         Create a new object with the given kwargs, saving it to the database\n482:         and returning the created object.\n483:         \"\"\"\n484:         obj = self.model(**kwargs)\n485:         self._for_write = True\n486:         obj.save(force_insert=True, using=self.db)\n487:         return obj\n488: \n489:     def _prepare_for_bulk_create(self, objs):\n490:         for obj in objs:\n491:             if obj.pk is None:\n492:                 # Populate new PK values.\n493:                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)\n494:             obj._prepare_related_fields_for_save(operation_name='bulk_create')\n495: \n496:     def _check_bulk_create_options(self, ignore_conflicts, update_conflicts, update_fields, unique_fields):\n497:         if ignore_conflicts and update_conflicts:\n498:             raise ValueError(\n499:                 'ignore_conflicts and update_conflicts are mutually exclusive.'\n500:             )\n501:         db_features = connections[self.db].features\n502:         if ignore_conflicts:\n503:             if not db_features.supports_ignore_conflicts:\n504:                 raise NotSupportedError(\n505:                     'This database backend does not support ignoring conflicts.'\n506:                 )\n507:             return OnConflict.IGNORE\n508:         elif update_conflicts:\n509:             if not db_features.supports_update_conflicts:\n510:                 raise NotSupportedError(\n511:                     'This database backend does not support updating conflicts.'\n512:                 )\n513:             if not update_fields:\n514:                 raise ValueError(\n515:                     'Fields that will be updated when a row insertion fails '\n516:                     'on conflicts must be provided.'\n517:                 )\n518:             if unique_fields and not db_features.supports_update_conflicts_with_target:\n519:                 raise NotSupportedError(\n520:                     'This database backend does not support updating '\n521:                     'conflicts with specifying unique fields that can trigger '\n522:                     'the upsert.'\n523:                 )\n524:             if not unique_fields and db_features.supports_update_conflicts_with_target:\n525:                 raise ValueError(\n526:                     'Unique fields that can trigger the upsert must be '\n527:                     'provided.'\n528:                 )\n529:             # Updating primary keys and non-concrete fields is forbidden.\n530:             update_fields = [self.model._meta.get_field(name) for name in update_fields]\n531:             if any(not f.concrete or f.many_to_many for f in update_fields):\n532:                 raise ValueError(\n533:                     'bulk_create() can only be used with concrete fields in '\n534:                     'update_fields.'\n535:                 )\n536:             if any(f.primary_key for f in update_fields):\n537:                 raise ValueError(\n538:                     'bulk_create() cannot be used with primary keys in '\n539:                     'update_fields.'\n540:                 )\n541:             if unique_fields:\n542:                 # Primary key is allowed in unique_fields.\n543:                 unique_fields = [\n544:                     self.model._meta.get_field(name)\n545:                     for name in unique_fields if name != 'pk'\n546:                 ]\n547:                 if any(not f.concrete or f.many_to_many for f in unique_fields):\n548:                     raise ValueError(\n549:                         'bulk_create() can only be used with concrete fields '\n550:                         'in unique_fields.'\n551:                     )\n552:             return OnConflict.UPDATE\n553:         return None\n554: \n555:     def bulk_create(\n556:         self, objs, batch_size=None, ignore_conflicts=False,\n557:         update_conflicts=False, update_fields=None, unique_fields=None,\n558:     ):\n559:         \"\"\"\n560:         Insert each of the instances into the database. Do *not* call\n561:         save() on each of the instances, do not send any pre/post_save\n562:         signals, and do not set the primary key attribute if it is an\n563:         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).\n564:         Multi-table models are not supported.\n565:         \"\"\"\n566:         # When you bulk insert you don't get the primary keys back (if it's an\n567:         # autoincrement, except if can_return_rows_from_bulk_insert=True), so\n568:         # you can't insert into the child tables which references this. There\n569:         # are two workarounds:\n570:         # 1) This could be implemented if you didn't have an autoincrement pk\n571:         # 2) You could do it by doing O(n) normal inserts into the parent\n572:         #    tables to get the primary keys back and then doing a single bulk\n573:         #    insert into the childmost table.\n574:         # We currently set the primary keys on the objects when using\n575:         # PostgreSQL via the RETURNING ID clause. It should be possible for\n576:         # Oracle as well, but the semantics for extracting the primary keys is\n577:         # trickier so it's not done yet.\n578:         if batch_size is not None and batch_size <= 0:\n579:             raise ValueError('Batch size must be a positive integer.')\n580:         # Check that the parents share the same concrete model with the our\n581:         # model to detect the inheritance pattern ConcreteGrandParent ->\n582:         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy\n583:         # would not identify that case as involving multiple tables.\n584:         for parent in self.model._meta.get_parent_list():\n585:             if parent._meta.concrete_model is not self.model._meta.concrete_model:\n586:                 raise ValueError(\"Can't bulk create a multi-table inherited model\")\n587:         if not objs:\n588:             return objs\n589:         on_conflict = self._check_bulk_create_options(\n590:             ignore_conflicts,\n591:             update_conflicts,\n592:             update_fields,\n593:             unique_fields,\n594:         )\n595:         self._for_write = True\n596:         opts = self.model._meta\n597:         fields = opts.concrete_fields\n598:         objs = list(objs)\n599:         self._prepare_for_bulk_create(objs)\n600:         with transaction.atomic(using=self.db, savepoint=False):\n601:             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n602:             if objs_with_pk:\n603:                 returned_columns = self._batched_insert(\n604:                     objs_with_pk,\n605:                     fields,\n606:                     batch_size,\n607:                     on_conflict=on_conflict,\n608:                     update_fields=update_fields,\n609:                     unique_fields=unique_fields,\n610:                 )\n611:                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):\n612:                     for result, field in zip(results, opts.db_returning_fields):\n613:                         if field != opts.pk:\n614:                             setattr(obj_with_pk, field.attname, result)\n615:                 for obj_with_pk in objs_with_pk:\n616:                     obj_with_pk._state.adding = False\n617:                     obj_with_pk._state.db = self.db\n618:             if objs_without_pk:\n619:                 fields = [f for f in fields if not isinstance(f, AutoField)]\n620:                 returned_columns = self._batched_insert(\n621:                     objs_without_pk,\n622:                     fields,\n623:                     batch_size,\n624:                     on_conflict=on_conflict,\n625:                     update_fields=update_fields,\n626:                     unique_fields=unique_fields,\n627:                 )\n628:                 connection = connections[self.db]\n629:                 if connection.features.can_return_rows_from_bulk_insert and on_conflict is None:\n630:                     assert len(returned_columns) == len(objs_without_pk)\n631:                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):\n632:                     for result, field in zip(results, opts.db_returning_fields):\n633:                         setattr(obj_without_pk, field.attname, result)\n634:                     obj_without_pk._state.adding = False\n635:                     obj_without_pk._state.db = self.db\n636: \n637:         return objs\n638: \n639:     def bulk_update(self, objs, fields, batch_size=None):\n640:         \"\"\"\n641:         Update the given fields in each of the given objects in the database.\n642:         \"\"\"\n643:         if batch_size is not None and batch_size < 0:\n644:             raise ValueError('Batch size must be a positive integer.')\n645:         if not fields:\n646:             raise ValueError('Field names must be given to bulk_update().')\n647:         objs = tuple(objs)\n648:         if any(obj.pk is None for obj in objs):\n649:             raise ValueError('All bulk_update() objects must have a primary key set.')\n650:         fields = [self.model._meta.get_field(name) for name in fields]\n651:         if any(not f.concrete or f.many_to_many for f in fields):\n652:             raise ValueError('bulk_update() can only be used with concrete fields.')\n653:         if any(f.primary_key for f in fields):\n654:             raise ValueError('bulk_update() cannot be used with primary key fields.')\n655:         if not objs:\n656:             return 0\n657:         for obj in objs:\n658:             obj._prepare_related_fields_for_save(operation_name='bulk_update', fields=fields)\n659:         # PK is used twice in the resulting update query, once in the filter\n660:         # and once in the WHEN. Each field will also have one CAST.\n661:         connection = connections[self.db]\n662:         max_batch_size = connection.ops.bulk_batch_size(['pk', 'pk'] + fields, objs)\n663:         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n664:         requires_casting = connection.features.requires_casted_case_in_updates\n665:         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))\n666:         updates = []\n667:         for batch_objs in batches:\n668:             update_kwargs = {}\n669:             for field in fields:\n670:                 when_statements = []\n671:                 for obj in batch_objs:\n672:                     attr = getattr(obj, field.attname)\n673:                     if not isinstance(attr, Expression):\n674:                         attr = Value(attr, output_field=field)\n675:                     when_statements.append(When(pk=obj.pk, then=attr))\n676:                 case_statement = Case(*when_statements, output_field=field)\n677:                 if requires_casting:\n678:                     case_statement = Cast(case_statement, output_field=field)\n679:                 update_kwargs[field.attname] = case_statement\n680:             updates.append(([obj.pk for obj in batch_objs], update_kwargs))\n681:         rows_updated = 0\n682:         with transaction.atomic(using=self.db, savepoint=False):\n683:             for pks, update_kwargs in updates:\n684:                 rows_updated += self.filter(pk__in=pks).update(**update_kwargs)\n685:         return rows_updated\n686:     bulk_update.alters_data = True\n687: \n688:     def get_or_create(self, defaults=None, **kwargs):\n689:         \"\"\"\n690:         Look up an object with the given kwargs, creating one if necessary.\n691:         Return a tuple of (object, created), where created is a boolean\n692:         specifying whether an object was created.\n693:         \"\"\"\n694:         # The get() needs to be targeted at the write database in order\n695:         # to avoid potential transaction consistency problems.\n696:         self._for_write = True\n697:         try:\n698:             return self.get(**kwargs), False\n699:         except self.model.DoesNotExist:\n700:             params = self._extract_model_params(defaults, **kwargs)\n701:             # Try to create an object using passed params.\n702:             try:\n703:                 with transaction.atomic(using=self.db):\n704:                     params = dict(resolve_callables(params))\n705:                     return self.create(**params), True\n706:             except IntegrityError:\n707:                 try:\n708:                     return self.get(**kwargs), False\n709:                 except self.model.DoesNotExist:\n710:                     pass\n711:                 raise\n712: \n713:     def update_or_create(self, defaults=None, **kwargs):\n714:         \"\"\"\n715:         Look up an object with the given kwargs, updating one with defaults\n716:         if it exists, otherwise create a new one.\n717:         Return a tuple (object, created), where created is a boolean\n718:         specifying whether an object was created.\n719:         \"\"\"\n720:         defaults = defaults or {}\n721:         self._for_write = True\n722:         with transaction.atomic(using=self.db):\n723:             # Lock the row so that a concurrent update is blocked until\n724:             # update_or_create() has performed its save.\n725:             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)\n726:             if created:\n727:                 return obj, created\n728:             for k, v in resolve_callables(defaults):\n729:                 setattr(obj, k, v)\n730:             obj.save(using=self.db)\n731:         return obj, False\n732: \n733:     def _extract_model_params(self, defaults, **kwargs):\n734:         \"\"\"\n735:         Prepare `params` for creating a model instance based on the given\n736:         kwargs; for use by get_or_create().\n737:         \"\"\"\n... Code Truncated ...\n\n",
  "line_level_localization": [
    {
      "filename": "/django/db/models/query.py",
      "suspect_lines": [
        20,
        673
      ]
    }
  ]
}