{
  "instance_id": "scikit-learn__scikit-learn-13142",
  "problem_statement": "GaussianMixture predict and fit_predict disagree when n_init>1\n#### Description\r\nWhen `n_init` is specified in GaussianMixture, the results of fit_predict(X) and predict(X) are often different.  The `test_gaussian_mixture_fit_predict` unit test doesn't catch this because it does not set `n_init`.\r\n\r\n#### Steps/Code to Reproduce\r\n```\r\npython\r\nfrom sklearn.mixture import GaussianMixture\r\nfrom sklearn.utils.testing import assert_array_equal\r\nimport numpy\r\nX = numpy.random.randn(1000,5)\r\nprint 'no n_init'\r\ngm = GaussianMixture(n_components=5)\r\nc1 = gm.fit_predict(X)\r\nc2 = gm.predict(X)\r\nassert_array_equal(c1,c2)\r\nprint 'n_init=5'\r\ngm = GaussianMixture(n_components=5, n_init=5)\r\nc1 = gm.fit_predict(X)\r\nc2 = gm.predict(X)\r\nassert_array_equal(c1,c2)\r\n```\r\n\r\n#### Expected Results\r\n```\r\nno n_init\r\nn_init=5\r\n```\r\nNo exceptions.\r\n\r\n#### Actual Results\r\n```\r\nno n_init\r\nn_init=5\r\nTraceback (most recent call last):\r\n  File \"test_gm.py\", line 17, in <module>\r\n    assert_array_equal(c1,c2)\r\n  File \"/home/scott/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 872, in assert_array_equal\r\n    verbose=verbose, header='Arrays are not equal')\r\n  File \"/home/scott/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 796, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n\r\n(mismatch 88.6%)\r\n x: array([4, 0, 1, 1, 1, 3, 3, 4, 4, 2, 0, 0, 1, 2, 0, 2, 0, 1, 3, 1, 1, 3,\r\n       2, 1, 0, 2, 1, 0, 2, 0, 3, 1, 2, 3, 3, 1, 0, 2, 2, 0, 3, 0, 2, 0,\r\n       4, 2, 3, 0, 4, 2, 4, 1, 0, 2, 2, 1, 3, 2, 1, 4, 0, 2, 2, 1, 1, 2,...\r\n y: array([4, 1, 0, 2, 2, 1, 1, 4, 4, 0, 4, 1, 0, 3, 1, 0, 2, 2, 1, 2, 0, 0,\r\n       1, 0, 4, 1, 0, 4, 0, 1, 1, 2, 3, 1, 4, 0, 1, 4, 4, 4, 0, 1, 0, 2,\r\n       4, 1, 1, 2, 4, 3, 4, 0, 2, 3, 2, 3, 0, 0, 2, 3, 3, 3, 3, 0, 3, 2,...\r\n```\r\n\r\n#### Versions\r\n```\r\nSystem:\r\n    python: 2.7.15rc1 (default, Nov 12 2018, 14:31:15)  [GCC 7.3.0]\r\n   machine: Linux-4.15.0-43-generic-x86_64-with-Ubuntu-18.04-bionic\r\nexecutable: /usr/bin/python\r\n\r\nBLAS:\r\n    macros: HAVE_CBLAS=None, NO_ATLAS_INFO=-1\r\ncblas_libs: cblas\r\n  lib_dirs: /usr/lib/x86_64-linux-gnu\r\n\r\nPython deps:\r\n    Cython: 0.28.5\r\n     scipy: 1.2.0\r\nsetuptools: 39.0.1\r\n       pip: 19.0.1\r\n     numpy: 1.16.0\r\n    pandas: 0.23.1\r\n   sklearn: 0.20.2\r\n```\n",
  "localized_code": "[start of sklearn/mixture/base.py]\n1: \"\"\"Base class for mixture models.\"\"\"\n2: \n3: # Author: Wei Xue <xuewei4d@gmail.com>\n4: # Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>\n5: # License: BSD 3 clause\n6: \n7: import warnings\n8: from abc import ABCMeta, abstractmethod\n9: from time import time\n10: \n11: import numpy as np\n12: \n13: from .. import cluster\n14: from ..base import BaseEstimator\n15: from ..base import DensityMixin\n16: from ..exceptions import ConvergenceWarning\n17: from ..utils import check_array, check_random_state\n18: from ..utils.fixes import logsumexp\n19: \n20: \n21: def _check_shape(param, param_shape, name):\n22:     \"\"\"Validate the shape of the input parameter 'param'.\n23: \n24:     Parameters\n25:     ----------\n26:     param : array\n27: \n28:     param_shape : tuple\n29: \n30:     name : string\n31:     \"\"\"\n32:     param = np.array(param)\n33:     if param.shape != param_shape:\n34:         raise ValueError(\"The parameter '%s' should have the shape of %s, \"\n35:                          \"but got %s\" % (name, param_shape, param.shape))\n36: \n37: \n38: def _check_X(X, n_components=None, n_features=None, ensure_min_samples=1):\n39:     \"\"\"Check the input data X.\n40: \n41:     Parameters\n42:     ----------\n43:     X : array-like, shape (n_samples, n_features)\n44: \n45:     n_components : int\n46: \n47:     Returns\n48:     -------\n49:     X : array, shape (n_samples, n_features)\n50:     \"\"\"\n51:     X = check_array(X, dtype=[np.float64, np.float32],\n52:                     ensure_min_samples=ensure_min_samples)\n53:     if n_components is not None and X.shape[0] < n_components:\n54:         raise ValueError('Expected n_samples >= n_components '\n55:                          'but got n_components = %d, n_samples = %d'\n56:                          % (n_components, X.shape[0]))\n57:     if n_features is not None and X.shape[1] != n_features:\n58:         raise ValueError(\"Expected the input data X have %d features, \"\n59:                          \"but got %d features\"\n60:                          % (n_features, X.shape[1]))\n61:     return X\n62: \n63: \n64: class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n65:     \"\"\"Base class for mixture models.\n66: \n67:     This abstract class specifies an interface for all mixture classes and\n68:     provides basic common methods for mixture models.\n69:     \"\"\"\n70: \n71:     def __init__(self, n_components, tol, reg_covar,\n72:                  max_iter, n_init, init_params, random_state, warm_start,\n73:                  verbose, verbose_interval):\n74:         self.n_components = n_components\n75:         self.tol = tol\n76:         self.reg_covar = reg_covar\n77:         self.max_iter = max_iter\n78:         self.n_init = n_init\n79:         self.init_params = init_params\n80:         self.random_state = random_state\n81:         self.warm_start = warm_start\n82:         self.verbose = verbose\n83:         self.verbose_interval = verbose_interval\n84: \n85:     def _check_initial_parameters(self, X):\n86:         \"\"\"Check values of the basic parameters.\n87: \n88:         Parameters\n89:         ----------\n90:         X : array-like, shape (n_samples, n_features)\n91:         \"\"\"\n92:         if self.n_components < 1:\n93:             raise ValueError(\"Invalid value for 'n_components': %d \"\n94:                              \"Estimation requires at least one component\"\n95:                              % self.n_components)\n96: \n97:         if self.tol < 0.:\n98:             raise ValueError(\"Invalid value for 'tol': %.5f \"\n99:                              \"Tolerance used by the EM must be non-negative\"\n100:                              % self.tol)\n101: \n102:         if self.n_init < 1:\n103:             raise ValueError(\"Invalid value for 'n_init': %d \"\n104:                              \"Estimation requires at least one run\"\n105:                              % self.n_init)\n106: \n107:         if self.max_iter < 1:\n108:             raise ValueError(\"Invalid value for 'max_iter': %d \"\n109:                              \"Estimation requires at least one iteration\"\n110:                              % self.max_iter)\n111: \n112:         if self.reg_covar < 0.:\n113:             raise ValueError(\"Invalid value for 'reg_covar': %.5f \"\n114:                              \"regularization on covariance must be \"\n115:                              \"non-negative\"\n116:                              % self.reg_covar)\n117: \n118:         # Check all the parameters values of the derived class\n119:         self._check_parameters(X)\n120: \n121:     @abstractmethod\n122:     def _check_parameters(self, X):\n123:         \"\"\"Check initial parameters of the derived class.\n124: \n125:         Parameters\n126:         ----------\n127:         X : array-like, shape  (n_samples, n_features)\n128:         \"\"\"\n129:         pass\n130: \n131:     def _initialize_parameters(self, X, random_state):\n132:         \"\"\"Initialize the model parameters.\n133: \n134:         Parameters\n135:         ----------\n136:         X : array-like, shape  (n_samples, n_features)\n137: \n138:         random_state : RandomState\n139:             A random number generator instance.\n140:         \"\"\"\n141:         n_samples, _ = X.shape\n142: \n143:         if self.init_params == 'kmeans':\n144:             resp = np.zeros((n_samples, self.n_components))\n145:             label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n146:                                    random_state=random_state).fit(X).labels_\n147:             resp[np.arange(n_samples), label] = 1\n148:         elif self.init_params == 'random':\n149:             resp = random_state.rand(n_samples, self.n_components)\n150:             resp /= resp.sum(axis=1)[:, np.newaxis]\n151:         else:\n152:             raise ValueError(\"Unimplemented initialization method '%s'\"\n153:                              % self.init_params)\n154: \n155:         self._initialize(X, resp)\n156: \n157:     @abstractmethod\n158:     def _initialize(self, X, resp):\n159:         \"\"\"Initialize the model parameters of the derived class.\n160: \n161:         Parameters\n162:         ----------\n163:         X : array-like, shape  (n_samples, n_features)\n164: \n165:         resp : array-like, shape (n_samples, n_components)\n166:         \"\"\"\n167:         pass\n168: \n169:     def fit(self, X, y=None):\n170:         \"\"\"Estimate model parameters with the EM algorithm.\n171: \n172:         The method fits the model ``n_init`` times and sets the parameters with\n173:         which the model has the largest likelihood or lower bound. Within each\n174:         trial, the method iterates between E-step and M-step for ``max_iter``\n175:         times until the change of likelihood or lower bound is less than\n176:         ``tol``, otherwise, a ``ConvergenceWarning`` is raised.\n177:         If ``warm_start`` is ``True``, then ``n_init`` is ignored and a single\n178:         initialization is performed upon the first call. Upon consecutive\n179:         calls, training starts where it left off.\n180: \n181:         Parameters\n182:         ----------\n183:         X : array-like, shape (n_samples, n_features)\n184:             List of n_features-dimensional data points. Each row\n185:             corresponds to a single data point.\n186: \n187:         Returns\n188:         -------\n189:         self\n190:         \"\"\"\n191:         self.fit_predict(X, y)\n192:         return self\n193: \n194:     def fit_predict(self, X, y=None):\n195:         \"\"\"Estimate model parameters using X and predict the labels for X.\n196: \n197:         The method fits the model n_init times and sets the parameters with\n198:         which the model has the largest likelihood or lower bound. Within each\n199:         trial, the method iterates between E-step and M-step for `max_iter`\n200:         times until the change of likelihood or lower bound is less than\n201:         `tol`, otherwise, a `ConvergenceWarning` is raised. After fitting, it\n202:         predicts the most probable label for the input data points.\n203: \n204:         .. versionadded:: 0.20\n205: \n206:         Parameters\n207:         ----------\n208:         X : array-like, shape (n_samples, n_features)\n209:             List of n_features-dimensional data points. Each row\n210:             corresponds to a single data point.\n211: \n212:         Returns\n213:         -------\n214:         labels : array, shape (n_samples,)\n215:             Component labels.\n216:         \"\"\"\n217:         X = _check_X(X, self.n_components, ensure_min_samples=2)\n218:         self._check_initial_parameters(X)\n219: \n220:         # if we enable warm_start, we will have a unique initialisation\n221:         do_init = not(self.warm_start and hasattr(self, 'converged_'))\n222:         n_init = self.n_init if do_init else 1\n223: \n224:         max_lower_bound = -np.infty\n225:         self.converged_ = False\n226: \n227:         random_state = check_random_state(self.random_state)\n228: \n229:         n_samples, _ = X.shape\n230:         for init in range(n_init):\n231:             self._print_verbose_msg_init_beg(init)\n232: \n233:             if do_init:\n234:                 self._initialize_parameters(X, random_state)\n235: \n236:             lower_bound = (-np.infty if do_init else self.lower_bound_)\n237: \n238:             for n_iter in range(1, self.max_iter + 1):\n239:                 prev_lower_bound = lower_bound\n240: \n241:                 log_prob_norm, log_resp = self._e_step(X)\n242:                 self._m_step(X, log_resp)\n243:                 lower_bound = self._compute_lower_bound(\n244:                     log_resp, log_prob_norm)\n245: \n246:                 change = lower_bound - prev_lower_bound\n247:                 self._print_verbose_msg_iter_end(n_iter, change)\n248: \n249:                 if abs(change) < self.tol:\n250:                     self.converged_ = True\n251:                     break\n252: \n253:             self._print_verbose_msg_init_end(lower_bound)\n254: \n255:             if lower_bound > max_lower_bound:\n256:                 max_lower_bound = lower_bound\n257:                 best_params = self._get_parameters()\n258:                 best_n_iter = n_iter\n259: \n260:         # Always do a final e-step to guarantee that the labels returned by\n261:         # fit_predict(X) are always consistent with fit(X).predict(X)\n262:         # for any value of max_iter and tol (and any random_state).\n263:         _, log_resp = self._e_step(X)\n264: \n265:         if not self.converged_:\n... Code Truncated ...\n\n",
  "line_level_localization": [
    {
      "filename": "/sklearn/mixture/base.py",
      "suspect_lines": [
        260,
        261,
        262,
        263,
        264
      ]
    }
  ]
}