{
  "instance_id": "scikit-learn__scikit-learn-12682",
  "problem_statement": "`SparseCoder` doesn't expose `max_iter` for `Lasso`\n`SparseCoder` uses `Lasso` if the algorithm is set to `lasso_cd`. It sets some of the `Lasso`'s parameters, but not `max_iter`, and that by default is 1000. This results in a warning in `examples/decomposition/plot_sparse_coding.py` complaining that the estimator has not converged.\r\n\r\nI guess there should be a way for the user to specify other parameters of the estimator used in `SparseCoder` other than the ones provided in the `SparseCoder.__init__` right now.\n",
  "localized_code": "[start of examples/decomposition/plot_sparse_coding.py]\n1: \"\"\"\n2: ===========================================\n3: Sparse coding with a precomputed dictionary\n4: ===========================================\n5: \n6: Transform a signal as a sparse combination of Ricker wavelets. This example\n7: visually compares different sparse coding methods using the\n8: :class:`sklearn.decomposition.SparseCoder` estimator. The Ricker (also known\n9: as Mexican hat or the second derivative of a Gaussian) is not a particularly\n10: good kernel to represent piecewise constant signals like this one. It can\n11: therefore be seen how much adding different widths of atoms matters and it\n12: therefore motivates learning the dictionary to best fit your type of signals.\n13: \n14: The richer dictionary on the right is not larger in size, heavier subsampling\n15: is performed in order to stay on the same order of magnitude.\n16: \"\"\"\n17: print(__doc__)\n18: \n19: from distutils.version import LooseVersion\n20: \n21: import numpy as np\n22: import matplotlib.pyplot as plt\n23: \n24: from sklearn.decomposition import SparseCoder\n25: \n26: \n27: def ricker_function(resolution, center, width):\n28:     \"\"\"Discrete sub-sampled Ricker (Mexican hat) wavelet\"\"\"\n29:     x = np.linspace(0, resolution - 1, resolution)\n30:     x = ((2 / ((np.sqrt(3 * width) * np.pi ** 1 / 4)))\n31:          * (1 - ((x - center) ** 2 / width ** 2))\n32:          * np.exp((-(x - center) ** 2) / (2 * width ** 2)))\n33:     return x\n34: \n35: \n36: def ricker_matrix(width, resolution, n_components):\n37:     \"\"\"Dictionary of Ricker (Mexican hat) wavelets\"\"\"\n38:     centers = np.linspace(0, resolution - 1, n_components)\n39:     D = np.empty((n_components, resolution))\n40:     for i, center in enumerate(centers):\n41:         D[i] = ricker_function(resolution, center, width)\n42:     D /= np.sqrt(np.sum(D ** 2, axis=1))[:, np.newaxis]\n43:     return D\n44: \n45: \n46: resolution = 1024\n47: subsampling = 3  # subsampling factor\n48: width = 100\n49: n_components = resolution // subsampling\n50: \n51: # Compute a wavelet dictionary\n... Code Truncated ...\n\n[start of sklearn/decomposition/dict_learning.py]\n1: \"\"\" Dictionary learning\n2: \"\"\"\n3: # Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\n4: # License: BSD 3 clause\n5: \n6: import time\n7: import sys\n8: import itertools\n9: \n10: from math import ceil\n11: \n12: import numpy as np\n13: from scipy import linalg\n14: from joblib import Parallel, delayed, effective_n_jobs\n15: \n16: from ..base import BaseEstimator, TransformerMixin\n17: from ..utils import (check_array, check_random_state, gen_even_slices,\n18:                      gen_batches)\n19: from ..utils.extmath import randomized_svd, row_norms\n20: from ..utils.validation import check_is_fitted\n21: from ..linear_model import Lasso, orthogonal_mp_gram, LassoLars, Lars\n22: \n23: \n24: def _check_positive_coding(method, positive):\n25:     if positive and method in [\"omp\", \"lars\"]:\n26:         raise ValueError(\n27:                 \"Positive constraint not supported for '{}' \"\n28:                 \"coding method.\".format(method)\n29:             )\n30: \n31: \n32: def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',\n33:                    regularization=None, copy_cov=True,\n34:                    init=None, max_iter=1000, check_input=True, verbose=0,\n35:                    positive=False):\n36:     \"\"\"Generic sparse coding\n37: \n38:     Each column of the result is the solution to a Lasso problem.\n39: \n40:     Parameters\n41:     ----------\n42:     X : array of shape (n_samples, n_features)\n43:         Data matrix.\n44: \n45:     dictionary : array of shape (n_components, n_features)\n46:         The dictionary matrix against which to solve the sparse coding of\n47:         the data. Some of the algorithms assume normalized rows.\n48: \n49:     gram : None | array, shape=(n_components, n_components)\n50:         Precomputed Gram matrix, dictionary * dictionary'\n51:         gram can be None if method is 'threshold'.\n52: \n53:     cov : array, shape=(n_components, n_samples)\n54:         Precomputed covariance, dictionary * X'\n55: \n56:     algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}\n57:         lars: uses the least angle regression method (linear_model.lars_path)\n58:         lasso_lars: uses Lars to compute the Lasso solution\n59:         lasso_cd: uses the coordinate descent method to compute the\n60:         Lasso solution (linear_model.Lasso). lasso_lars will be faster if\n61:         the estimated components are sparse.\n62:         omp: uses orthogonal matching pursuit to estimate the sparse solution\n63:         threshold: squashes to zero all coefficients less than regularization\n64:         from the projection dictionary * data'\n65: \n66:     regularization : int | float\n67:         The regularization parameter. It corresponds to alpha when\n68:         algorithm is 'lasso_lars', 'lasso_cd' or 'threshold'.\n69:         Otherwise it corresponds to n_nonzero_coefs.\n70: \n71:     init : array of shape (n_samples, n_components)\n72:         Initialization value of the sparse code. Only used if\n73:         `algorithm='lasso_cd'`.\n74: \n75:     max_iter : int, 1000 by default\n76:         Maximum number of iterations to perform if `algorithm='lasso_cd'`.\n77: \n78:     copy_cov : boolean, optional\n79:         Whether to copy the precomputed covariance matrix; if False, it may be\n80:         overwritten.\n81: \n82:     check_input : boolean, optional\n83:         If False, the input arrays X and dictionary will not be checked.\n84: \n85:     verbose : int\n86:         Controls the verbosity; the higher, the more messages. Defaults to 0.\n87: \n88:     positive: boolean\n89:         Whether to enforce a positivity constraint on the sparse code.\n90: \n91:         .. versionadded:: 0.20\n92: \n93:     Returns\n94:     -------\n95:     code : array of shape (n_components, n_features)\n96:         The sparse codes\n97: \n98:     See also\n99:     --------\n100:     sklearn.linear_model.lars_path\n101:     sklearn.linear_model.orthogonal_mp\n102:     sklearn.linear_model.Lasso\n103:     SparseCoder\n104:     \"\"\"\n105:     if X.ndim == 1:\n106:         X = X[:, np.newaxis]\n107:     n_samples, n_features = X.shape\n108:     n_components = dictionary.shape[0]\n109:     if dictionary.shape[1] != X.shape[1]:\n110:         raise ValueError(\"Dictionary and X have different numbers of features:\"\n111:                          \"dictionary.shape: {} X.shape{}\".format(\n112:                              dictionary.shape, X.shape))\n113:     if cov is None and algorithm != 'lasso_cd':\n114:         # overwriting cov is safe\n115:         copy_cov = False\n116:         cov = np.dot(dictionary, X.T)\n117: \n118:     _check_positive_coding(algorithm, positive)\n119: \n120:     if algorithm == 'lasso_lars':\n121:         alpha = float(regularization) / n_features  # account for scaling\n122:         try:\n123:             err_mgt = np.seterr(all='ignore')\n124: \n125:             # Not passing in verbose=max(0, verbose-1) because Lars.fit already\n126:             # corrects the verbosity level.\n127:             lasso_lars = LassoLars(alpha=alpha, fit_intercept=False,\n128:                                    verbose=verbose, normalize=False,\n129:                                    precompute=gram, fit_path=False,\n130:                                    positive=positive)\n131:             lasso_lars.fit(dictionary.T, X.T, Xy=cov)\n132:             new_code = lasso_lars.coef_\n133:         finally:\n134:             np.seterr(**err_mgt)\n135: \n136:     elif algorithm == 'lasso_cd':\n137:         alpha = float(regularization) / n_features  # account for scaling\n138: \n139:         # TODO: Make verbosity argument for Lasso?\n140:         # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\n141:         # argument that we could pass in from Lasso.\n142:         clf = Lasso(alpha=alpha, fit_intercept=False, normalize=False,\n143:                     precompute=gram, max_iter=max_iter, warm_start=True,\n144:                     positive=positive)\n145: \n146:         if init is not None:\n147:             clf.coef_ = init\n148: \n149:         clf.fit(dictionary.T, X.T, check_input=check_input)\n150:         new_code = clf.coef_\n151: \n152:     elif algorithm == 'lars':\n153:         try:\n154:             err_mgt = np.seterr(all='ignore')\n155: \n156:             # Not passing in verbose=max(0, verbose-1) because Lars.fit already\n157:             # corrects the verbosity level.\n158:             lars = Lars(fit_intercept=False, verbose=verbose, normalize=False,\n159:                         precompute=gram, n_nonzero_coefs=int(regularization),\n160:                         fit_path=False)\n161:             lars.fit(dictionary.T, X.T, Xy=cov)\n162:             new_code = lars.coef_\n163:         finally:\n164:             np.seterr(**err_mgt)\n165: \n166:     elif algorithm == 'threshold':\n167:         new_code = ((np.sign(cov) *\n168:                     np.maximum(np.abs(cov) - regularization, 0)).T)\n169:         if positive:\n170:             np.clip(new_code, 0, None, out=new_code)\n171: \n172:     elif algorithm == 'omp':\n173:         new_code = orthogonal_mp_gram(\n174:             Gram=gram, Xy=cov, n_nonzero_coefs=int(regularization),\n175:             tol=None, norms_squared=row_norms(X, squared=True),\n176:             copy_Xy=copy_cov).T\n177:     else:\n178:         raise ValueError('Sparse coding method must be \"lasso_lars\" '\n179:                          '\"lasso_cd\", \"lasso\", \"threshold\" or \"omp\", got %s.'\n180:                          % algorithm)\n181:     if new_code.ndim != 2:\n182:         return new_code.reshape(n_samples, n_components)\n183:     return new_code\n184: \n185: \n186: # XXX : could be moved to the linear_model module\n187: def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',\n188:                   n_nonzero_coefs=None, alpha=None, copy_cov=True, init=None,\n189:                   max_iter=1000, n_jobs=None, check_input=True, verbose=0,\n190:                   positive=False):\n191:     \"\"\"Sparse coding\n192: \n193:     Each row of the result is the solution to a sparse coding problem.\n194:     The goal is to find a sparse array `code` such that::\n195: \n196:         X ~= code * dictionary\n197: \n198:     Read more in the :ref:`User Guide <SparseCoder>`.\n199: \n200:     Parameters\n201:     ----------\n202:     X : array of shape (n_samples, n_features)\n203:         Data matrix\n204: \n205:     dictionary : array of shape (n_components, n_features)\n206:         The dictionary matrix against which to solve the sparse coding of\n207:         the data. Some of the algorithms assume normalized rows for meaningful\n208:         output.\n209: \n210:     gram : array, shape=(n_components, n_components)\n211:         Precomputed Gram matrix, dictionary * dictionary'\n212: \n213:     cov : array, shape=(n_components, n_samples)\n214:         Precomputed covariance, dictionary' * X\n215: \n216:     algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}\n217:         lars: uses the least angle regression method (linear_model.lars_path)\n218:         lasso_lars: uses Lars to compute the Lasso solution\n219:         lasso_cd: uses the coordinate descent method to compute the\n220:         Lasso solution (linear_model.Lasso). lasso_lars will be faster if\n221:         the estimated components are sparse.\n222:         omp: uses orthogonal matching pursuit to estimate the sparse solution\n223:         threshold: squashes to zero all coefficients less than alpha from\n224:         the projection dictionary * X'\n225: \n226:     n_nonzero_coefs : int, 0.1 * n_features by default\n227:         Number of nonzero coefficients to target in each column of the\n228:         solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n229:         and is overridden by `alpha` in the `omp` case.\n230: \n231:     alpha : float, 1. by default\n232:         If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n233:         penalty applied to the L1 norm.\n234:         If `algorithm='threshold'`, `alpha` is the absolute value of the\n235:         threshold below which coefficients will be squashed to zero.\n236:         If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n237:         the reconstruction error targeted. In this case, it overrides\n238:         `n_nonzero_coefs`.\n239: \n240:     copy_cov : boolean, optional\n241:         Whether to copy the precomputed covariance matrix; if False, it may be\n242:         overwritten.\n243: \n244:     init : array of shape (n_samples, n_components)\n245:         Initialization value of the sparse codes. Only used if\n246:         `algorithm='lasso_cd'`.\n247: \n248:     max_iter : int, 1000 by default\n249:         Maximum number of iterations to perform if `algorithm='lasso_cd'`.\n250: \n251:     n_jobs : int or None, optional (default=None)\n252:         Number of parallel jobs to run.\n253:         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n254:         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n255:         for more details.\n256: \n257:     check_input : boolean, optional\n258:         If False, the input arrays X and dictionary will not be checked.\n259: \n260:     verbose : int, optional\n261:         Controls the verbosity; the higher, the more messages. Defaults to 0.\n262: \n263:     positive : boolean, optional\n264:         Whether to enforce positivity when finding the encoding.\n265: \n266:         .. versionadded:: 0.20\n267: \n268:     Returns\n269:     -------\n270:     code : array of shape (n_samples, n_components)\n271:         The sparse codes\n272: \n273:     See also\n274:     --------\n275:     sklearn.linear_model.lars_path\n276:     sklearn.linear_model.orthogonal_mp\n277:     sklearn.linear_model.Lasso\n278:     SparseCoder\n279:     \"\"\"\n280:     if check_input:\n281:         if algorithm == 'lasso_cd':\n282:             dictionary = check_array(dictionary, order='C', dtype='float64')\n283:             X = check_array(X, order='C', dtype='float64')\n284:         else:\n285:             dictionary = check_array(dictionary)\n286:             X = check_array(X)\n287: \n288:     n_samples, n_features = X.shape\n289:     n_components = dictionary.shape[0]\n290: \n291:     if gram is None and algorithm != 'threshold':\n292:         gram = np.dot(dictionary, dictionary.T)\n293: \n294:     if cov is None and algorithm != 'lasso_cd':\n295:         copy_cov = False\n296:         cov = np.dot(dictionary, X.T)\n297: \n298:     if algorithm in ('lars', 'omp'):\n299:         regularization = n_nonzero_coefs\n300:         if regularization is None:\n301:             regularization = min(max(n_features / 10, 1), n_components)\n302:     else:\n303:         regularization = alpha\n304:         if regularization is None:\n305:             regularization = 1.\n306: \n307:     if effective_n_jobs(n_jobs) == 1 or algorithm == 'threshold':\n308:         code = _sparse_encode(X,\n309:                               dictionary, gram, cov=cov,\n310:                               algorithm=algorithm,\n311:                               regularization=regularization, copy_cov=copy_cov,\n312:                               init=init,\n313:                               max_iter=max_iter,\n314:                               check_input=False,\n315:                               verbose=verbose,\n316:                               positive=positive)\n317:         return code\n318: \n319:     # Enter parallel code block\n320:     code = np.empty((n_samples, n_components))\n321:     slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\n322: \n323:     code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\n324:         delayed(_sparse_encode)(\n325:             X[this_slice], dictionary, gram,\n326:             cov[:, this_slice] if cov is not None else None,\n327:             algorithm,\n328:             regularization=regularization, copy_cov=copy_cov,\n329:             init=init[this_slice] if init is not None else None,\n330:             max_iter=max_iter,\n331:             check_input=False,\n332:             positive=positive)\n333:         for this_slice in slices)\n334:     for this_slice, this_view in zip(slices, code_views):\n335:         code[this_slice] = this_view\n336:     return code\n337: \n338: \n339:                  random_state=None, positive=False):\nCode replaced for brevity.\n419: \n420: \n421: \n422: def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,\n423:                   method='lars', n_jobs=None, dict_init=None, code_init=None,\n424:                   callback=None, verbose=False, random_state=None,\n425:                   return_n_iter=False, positive_dict=False,\n426:                   positive_code=False):\n427:     \"\"\"Solves a dictionary learning matrix factorization problem.\n428: \n429:     Finds the best dictionary and the corresponding sparse code for\n430:     approximating the data matrix X by solving::\n431: \n432:         (U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n433:                      (U,V)\n434:                     with || V_k ||_2 = 1 for all  0 <= k < n_components\n435: \n436:     where V is the dictionary and U is the sparse code.\n437: \n438:     Read more in the :ref:`User Guide <DictionaryLearning>`.\n439: \n440:     Parameters\n441:     ----------\n442:     X : array of shape (n_samples, n_features)\n443:         Data matrix.\n444: \n445:     n_components : int,\n446:         Number of dictionary atoms to extract.\n447: \n448:     alpha : int,\n449:         Sparsity controlling parameter.\n450: \n451:     max_iter : int,\n452:         Maximum number of iterations to perform.\n453: \n454:     tol : float,\n455:         Tolerance for the stopping condition.\n456: \n457:     method : {'lars', 'cd'}\n458:         lars: uses the least angle regression method to solve the lasso problem\n459:         (linear_model.lars_path)\n460:         cd: uses the coordinate descent method to compute the\n461:         Lasso solution (linear_model.Lasso). Lars will be faster if\n462:         the estimated components are sparse.\n463: \n464:     n_jobs : int or None, optional (default=None)\n465:         Number of parallel jobs to run.\n466:         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n467:         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n468:         for more details.\n469: \n470:     dict_init : array of shape (n_components, n_features),\n471:         Initial value for the dictionary for warm restart scenarios.\n472: \n473:     code_init : array of shape (n_samples, n_components),\n474:         Initial value for the sparse code for warm restart scenarios.\n475: \n476:     callback : callable or None, optional (default: None)\n477:         Callable that gets invoked every five iterations\n478: \n479:     verbose : bool, optional (default: False)\n480:         To control the verbosity of the procedure.\n481: \n482:     random_state : int, RandomState instance or None, optional (default=None)\n483:         If int, random_state is the seed used by the random number generator;\n484:         If RandomState instance, random_state is the random number generator;\n485:         If None, the random number generator is the RandomState instance used\n486:         by `np.random`.\n487: \n488:     return_n_iter : bool\n489:         Whether or not to return the number of iterations.\n490: \n491:     positive_dict : bool\n492:         Whether to enforce positivity when finding the dictionary.\n493: \n494:         .. versionadded:: 0.20\n495: \n496:     positive_code : bool\n497:         Whether to enforce positivity when finding the code.\n498: \n499:         .. versionadded:: 0.20\n500: \n501:     Returns\n502:     -------\n503:     code : array of shape (n_samples, n_components)\n504:         The sparse code factor in the matrix factorization.\n505: \n506:     dictionary : array of shape (n_components, n_features),\n507:         The dictionary factor in the matrix factorization.\n508: \n509:     errors : array\n510:         Vector of errors at each iteration.\n511: \n512:     n_iter : int\n513:         Number of iterations run. Returned only if `return_n_iter` is\n514:         set to True.\n515: \n516:     See also\n517:     --------\n518:     dict_learning_online\n519:     DictionaryLearning\n520:     MiniBatchDictionaryLearning\n521:     SparsePCA\n522:     MiniBatchSparsePCA\n523:     \"\"\"\n524:     if method not in ('lars', 'cd'):\n525:         raise ValueError('Coding method %r not supported as a fit algorithm.'\n526:                          % method)\n527: \n528:     _check_positive_coding(method, positive_code)\n529: \n530:     method = 'lasso_' + method\n531: \n532:     t0 = time.time()\n533:     # Avoid integer division problems\n534:     alpha = float(alpha)\n535:     random_state = check_random_state(random_state)\n536: \n537:     # Init the code and the dictionary with SVD of Y\n538:     if code_init is not None and dict_init is not None:\n539:         code = np.array(code_init, order='F')\n540:         # Don't copy V, it will happen below\n541:         dictionary = dict_init\n542:     else:\n543:         code, S, dictionary = linalg.svd(X, full_matrices=False)\n544:         dictionary = S[:, np.newaxis] * dictionary\n545:     r = len(dictionary)\n546:     if n_components <= r:  # True even if n_components=None\n547:         code = code[:, :n_components]\n548:         dictionary = dictionary[:n_components, :]\n549:     else:\n550:         code = np.c_[code, np.zeros((len(code), n_components - r))]\n551:         dictionary = np.r_[dictionary,\n552:                            np.zeros((n_components - r, dictionary.shape[1]))]\n553: \n554:     # Fortran-order dict, as we are going to access its row vectors\n555:     dictionary = np.array(dictionary, order='F')\n556: \n557:     residuals = 0\n558: \n559:     errors = []\n560:     current_cost = np.nan\n561: \n562:     if verbose == 1:\n563:         print('[dict_learning]', end=' ')\n564: \n565:     # If max_iter is 0, number of iterations returned should be zero\n566:     ii = -1\n567: \n568:     for ii in range(max_iter):\n569:         dt = (time.time() - t0)\n570:         if verbose == 1:\n571:             sys.stdout.write(\".\")\n572:             sys.stdout.flush()\n573:         elif verbose:\n574:             print(\"Iteration % 3i \"\n575:                   \"(elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\n576:                   % (ii, dt, dt / 60, current_cost))\n577: \n578:         # Update code\n579:         code = sparse_encode(X, dictionary, algorithm=method, alpha=alpha,\n580:                              init=code, n_jobs=n_jobs, positive=positive_code)\n581:         # Update dictionary\n582:         dictionary, residuals = _update_dict(dictionary.T, X.T, code.T,\n583:                                              verbose=verbose, return_r2=True,\n584:                                              random_state=random_state,\n585:                                              positive=positive_dict)\n586:         dictionary = dictionary.T\n587: \n588:         # Cost function\n589:         current_cost = 0.5 * residuals + alpha * np.sum(np.abs(code))\n590:         errors.append(current_cost)\n591: \n592:         if ii > 0:\n593:             dE = errors[-2] - errors[-1]\n594:             # assert(dE >= -tol * errors[-1])\n595:             if dE < tol * errors[-1]:\n596:                 if verbose == 1:\n597:                     # A line return\n598:                     print(\"\")\n599:                 elif verbose:\n600:                     print(\"--- Convergence reached after %d iterations\" % ii)\n601:                 break\n602:         if ii % 5 == 0 and callback is not None:\n603:             callback(locals())\n604: \n605:     if return_n_iter:\n606:         return code, dictionary, errors, ii + 1\n607:     else:\n608:         return code, dictionary, errors\n609: \n610: \n611: def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,\n612:                          return_code=True, dict_init=None, callback=None,\n613:                          batch_size=3, verbose=False, shuffle=True,\n614:                          n_jobs=None, method='lars', iter_offset=0,\n615:                          random_state=None, return_inner_stats=False,\n616:                          inner_stats=None, return_n_iter=False,\n617:                          positive_dict=False, positive_code=False):\n618:     \"\"\"Solves a dictionary learning matrix factorization problem online.\n619: \n620:     Finds the best dictionary and the corresponding sparse code for\n621:     approximating the data matrix X by solving::\n622: \n623:         (U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n624:                      (U,V)\n625:                      with || V_k ||_2 = 1 for all  0 <= k < n_components\n626: \n627:     where V is the dictionary and U is the sparse code. This is\n628:     accomplished by repeatedly iterating over mini-batches by slicing\n629:     the input data.\n630: \n631:     Read more in the :ref:`User Guide <DictionaryLearning>`.\n632: \n633:     Parameters\n634:     ----------\n635:     X : array of shape (n_samples, n_features)\n636:         Data matrix.\n637: \n638:     n_components : int,\n639:         Number of dictionary atoms to extract.\n640: \n641:     alpha : float,\n642:         Sparsity controlling parameter.\n643: \n644:     n_iter : int,\n645:         Number of iterations to perform.\n646: \n647:     return_code : boolean,\n648:         Whether to also return the code U or just the dictionary V.\n649: \n650:     dict_init : array of shape (n_components, n_features),\n651:         Initial value for the dictionary for warm restart scenarios.\n652: \n653:     callback : callable or None, optional (default: None)\n654:         callable that gets invoked every five iterations\n655: \n656:     batch_size : int,\n657:         The number of samples to take in each batch.\n658: \n659:     verbose : bool, optional (default: False)\n660:         To control the verbosity of the procedure.\n661: \n662:     shuffle : boolean,\n663:         Whether to shuffle the data before splitting it in batches.\n664: \n665:     n_jobs : int or None, optional (default=None)\n666:         Number of parallel jobs to run.\n667:         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n668:         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n669:         for more details.\n670: \n671:     method : {'lars', 'cd'}\n672:         lars: uses the least angle regression method to solve the lasso problem\n673:         (linear_model.lars_path)\n674:         cd: uses the coordinate descent method to compute the\n675:         Lasso solution (linear_model.Lasso). Lars will be faster if\n676:         the estimated components are sparse.\n677: \n678:     iter_offset : int, default 0\n679:         Number of previous iterations completed on the dictionary used for\n680:         initialization.\n681: \n682:     random_state : int, RandomState instance or None, optional (default=None)\n683:         If int, random_state is the seed used by the random number generator;\n684:         If RandomState instance, random_state is the random number generator;\n685:         If None, the random number generator is the RandomState instance used\n686:         by `np.random`.\n687: \n688:     return_inner_stats : boolean, optional\n689:         Return the inner statistics A (dictionary covariance) and B\n690:         (data approximation). Useful to restart the algorithm in an\n691:         online setting. If return_inner_stats is True, return_code is\n692:         ignored\n693: \n694:     inner_stats : tuple of (A, B) ndarrays\n695:         Inner sufficient statistics that are kept by the algorithm.\n696:         Passing them at initialization is useful in online settings, to\n697:         avoid loosing the history of the evolution.\n698:         A (n_components, n_components) is the dictionary covariance matrix.\n699:         B (n_features, n_components) is the data approximation matrix\n700: \n701:     return_n_iter : bool\n702:         Whether or not to return the number of iterations.\n703: \n704:     positive_dict : bool\n705:         Whether to enforce positivity when finding the dictionary.\n706: \n707:         .. versionadded:: 0.20\n708: \n709:     positive_code : bool\n710:         Whether to enforce positivity when finding the code.\n711: \n712:         .. versionadded:: 0.20\n713: \n714:     Returns\n715:     -------\n716:     code : array of shape (n_samples, n_components),\n717:         the sparse code (only returned if `return_code=True`)\n718: \n719:     dictionary : array of shape (n_components, n_features),\n720:         the solutions to the dictionary learning problem\n721: \n722:     n_iter : int\n723:         Number of iterations run. Returned only if `return_n_iter` is\n724:         set to `True`.\n725: \n726:     See also\n727:     --------\n728:     dict_learning\n729:     DictionaryLearning\n730:     MiniBatchDictionaryLearning\n731:     SparsePCA\n732:     MiniBatchSparsePCA\n733: \n734:     \"\"\"\n735:     if n_components is None:\n736:         n_components = X.shape[1]\n737: \n738:     if method not in ('lars', 'cd'):\n739:         raise ValueError('Coding method not supported as a fit algorithm.')\n740: \n741:     _check_positive_coding(method, positive_code)\n742: \n743:     method = 'lasso_' + method\n744: \n745:     t0 = time.time()\n746:     n_samples, n_features = X.shape\n747:     # Avoid integer division problems\n748:     alpha = float(alpha)\n749:     random_state = check_random_state(random_state)\n750: \n751:     # Init V with SVD of X\n752:     if dict_init is not None:\n753:         dictionary = dict_init\n754:     else:\n755:         _, S, dictionary = randomized_svd(X, n_components,\n756:                                           random_state=random_state)\n757:         dictionary = S[:, np.newaxis] * dictionary\n758:     r = len(dictionary)\n759:     if n_components <= r:\n760:         dictionary = dictionary[:n_components, :]\n761:     else:\n762:         dictionary = np.r_[dictionary,\n763:                            np.zeros((n_components - r, dictionary.shape[1]))]\n764: \n765:     if verbose == 1:\n766:         print('[dict_learning]', end=' ')\n767: \n768:     if shuffle:\n769:         X_train = X.copy()\n770:         random_state.shuffle(X_train)\n771:     else:\n772:         X_train = X\n773: \n774:     dictionary = check_array(dictionary.T, order='F', dtype=np.float64,\n775:                              copy=False)\n776:     dictionary = np.require(dictionary, requirements='W')\n777: \n778:     X_train = check_array(X_train, order='C', dtype=np.float64, copy=False)\n779: \n780:     batches = gen_batches(n_samples, batch_size)\n781:     batches = itertools.cycle(batches)\n782: \n783:     # The covariance of the dictionary\n784:     if inner_stats is None:\n785:         A = np.zeros((n_components, n_components))\n786:         # The data approximation\n787:         B = np.zeros((n_features, n_components))\n788:     else:\n789:         A = inner_stats[0].copy()\n790:         B = inner_stats[1].copy()\n791: \n792:     # If n_iter is zero, we need to return zero.\n793:     ii = iter_offset - 1\n794: \n795:     for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\n796:         this_X = X_train[batch]\n797:         dt = (time.time() - t0)\n798:         if verbose == 1:\n799:             sys.stdout.write(\".\")\n800:             sys.stdout.flush()\n801:         elif verbose:\n802:             if verbose > 10 or ii % ceil(100. / verbose) == 0:\n803:                 print(\"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\"\n804:                       % (ii, dt, dt / 60))\n805: \n806:         this_code = sparse_encode(this_X, dictionary.T, algorithm=method,\n807:                                   alpha=alpha, n_jobs=n_jobs,\n808:                                   check_input=False,\n809:                                   positive=positive_code).T\n810: \n811:         # Update the auxiliary variables\n812:         if ii < batch_size - 1:\n813:             theta = float((ii + 1) * batch_size)\n814:         else:\n815:             theta = float(batch_size ** 2 + ii + 1 - batch_size)\n816:         beta = (theta + 1 - batch_size) / (theta + 1)\n817: \n818:         A *= beta\n819:         A += np.dot(this_code, this_code.T)\n820:         B *= beta\n821:         B += np.dot(this_X.T, this_code.T)\n822: \n823:         # Update dictionary\n824:         dictionary = _update_dict(dictionary, B, A, verbose=verbose,\n825:                                   random_state=random_state,\n826:                                   positive=positive_dict)\n827:         # XXX: Can the residuals be of any use?\n828: \n829:         # Maybe we need a stopping criteria based on the amount of\n830:         # modification in the dictionary\n831:         if callback is not None:\n832:             callback(locals())\n833: \n834:     if return_inner_stats:\n835:         if return_n_iter:\n836:             return dictionary.T, (A, B), ii - iter_offset + 1\n837:         else:\n838:             return dictionary.T, (A, B)\n839:     if return_code:\n840:         if verbose > 1:\n841:             print('Learning code...', end=' ')\n842:         elif verbose == 1:\n843:             print('|', end=' ')\n844:         code = sparse_encode(X, dictionary.T, algorithm=method, alpha=alpha,\n845:                              n_jobs=n_jobs, check_input=False,\n846:                              positive=positive_code)\n847:         if verbose > 1:\n848:             dt = (time.time() - t0)\n849:             print('done (total time: % 3is, % 4.1fmn)' % (dt, dt / 60))\n850:         if return_n_iter:\n851:             return code, dictionary.T, ii - iter_offset + 1\n852:         else:\n853:             return code, dictionary.T\n854: \n855:     if return_n_iter:\n856:         return dictionary.T, ii - iter_offset + 1\n857:     else:\n858:         return dictionary.T\n859: \n860: \n861: class SparseCodingMixin(TransformerMixin):\n862:     \"\"\"Sparse coding mixin\"\"\"\n863: \n864:     def _set_sparse_coding_params(self, n_components,\n865:                                   transform_algorithm='omp',\n866:                                   transform_n_nonzero_coefs=None,\n867:                                   transform_alpha=None, split_sign=False,\n868:                                   n_jobs=None, positive_code=False):\n869:         self.n_components = n_components\n870:         self.transform_algorithm = transform_algorithm\n871:         self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\n872:         self.transform_alpha = transform_alpha\n873:         self.split_sign = split_sign\n874:         self.n_jobs = n_jobs\n875:         self.positive_code = positive_code\n876: \n877:     def transform(self, X):\n878:         \"\"\"Encode the data as a sparse combination of the dictionary atoms.\n879: \n880:         Coding method is determined by the object parameter\n881:         `transform_algorithm`.\n882: \n883:         Parameters\n884:         ----------\n885:         X : array of shape (n_samples, n_features)\n886:             Test data to be transformed, must have the same number of\n887:             features as the data used to train the model.\n888: \n889:         Returns\n890:         -------\n891:         X_new : array, shape (n_samples, n_components)\n892:             Transformed data\n893: \n894:         \"\"\"\n895:         check_is_fitted(self, 'components_')\n896: \n897:         X = check_array(X)\n898: \n899:         code = sparse_encode(\n900:             X, self.components_, algorithm=self.transform_algorithm,\n901:             n_nonzero_coefs=self.transform_n_nonzero_coefs,\n902:             alpha=self.transform_alpha, n_jobs=self.n_jobs,\n903:             positive=self.positive_code)\n904: \n905:         if self.split_sign:\n906:             # feature vector is split into a positive and negative side\n907:             n_samples, n_features = code.shape\n908:             split_code = np.empty((n_samples, 2 * n_features))\n909:             split_code[:, :n_features] = np.maximum(code, 0)\n910:             split_code[:, n_features:] = -np.minimum(code, 0)\n911:             code = split_code\n912: \n913:         return code\n914: \n915: \n916: class SparseCoder(BaseEstimator, SparseCodingMixin):\n917:     \"\"\"Sparse coding\n918: \n919:     Finds a sparse representation of data against a fixed, precomputed\n920:     dictionary.\n921: \n922:     Each row of the result is the solution to a sparse coding problem.\n923:     The goal is to find a sparse array `code` such that::\n924: \n925:         X ~= code * dictionary\n926: \n927:     Read more in the :ref:`User Guide <SparseCoder>`.\n928: \n929:     Parameters\n930:     ----------\n931:     dictionary : array, [n_components, n_features]\n932:         The dictionary atoms used for sparse coding. Lines are assumed to be\n933:         normalized to unit norm.\n934: \n935:     transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\n936:     'threshold'}\n937:         Algorithm used to transform the data:\n938:         lars: uses the least angle regression method (linear_model.lars_path)\n939:         lasso_lars: uses Lars to compute the Lasso solution\n940:         lasso_cd: uses the coordinate descent method to compute the\n941:         Lasso solution (linear_model.Lasso). lasso_lars will be faster if\n942:         the estimated components are sparse.\n943:         omp: uses orthogonal matching pursuit to estimate the sparse solution\n944:         threshold: squashes to zero all coefficients less than alpha from\n945:         the projection ``dictionary * X'``\n946: \n947:     transform_n_nonzero_coefs : int, ``0.1 * n_features`` by default\n948:         Number of nonzero coefficients to target in each column of the\n949:         solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n950:         and is overridden by `alpha` in the `omp` case.\n951: \n952:     transform_alpha : float, 1. by default\n953:         If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n954:         penalty applied to the L1 norm.\n955:         If `algorithm='threshold'`, `alpha` is the absolute value of the\n956:         threshold below which coefficients will be squashed to zero.\n957:         If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n958:         the reconstruction error targeted. In this case, it overrides\n959:         `n_nonzero_coefs`.\n960: \n961:     split_sign : bool, False by default\n962:         Whether to split the sparse feature vector into the concatenation of\n963:         its negative part and its positive part. This can improve the\n964:         performance of downstream classifiers.\n965: \n966:     n_jobs : int or None, optional (default=None)\n967:         Number of parallel jobs to run.\n968:         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n969:         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n970:         for more details.\n971: \n972:     positive_code : bool\n973:         Whether to enforce positivity when finding the code.\n974: \n975:         .. versionadded:: 0.20\n976: \n977:     Attributes\n978:     ----------\n979:     components_ : array, [n_components, n_features]\n980:         The unchanged dictionary atoms\n981: \n982:     See also\n983:     --------\n984:     DictionaryLearning\n985:     MiniBatchDictionaryLearning\n986:     SparsePCA\n987:     MiniBatchSparsePCA\n988:     sparse_encode\n989:     \"\"\"\n990:     _required_parameters = [\"dictionary\"]\n991: \n992:     def __init__(self, dictionary, transform_algorithm='omp',\n993:                  transform_n_nonzero_coefs=None, transform_alpha=None,\n994:                  split_sign=False, n_jobs=None, positive_code=False):\n995:         self._set_sparse_coding_params(dictionary.shape[0],\n996:                                        transform_algorithm,\n997:                                        transform_n_nonzero_coefs,\n998:                                        transform_alpha, split_sign, n_jobs,\n999:                                        positive_code)\n1000:         self.components_ = dictionary\n1001: \n1002:     def fit(self, X, y=None):\n1003:         \"\"\"Do nothing and return the estimator unchanged\n1004: \n1005:         This method is just there to implement the usual API and hence\n1006:         work in pipelines.\n1007: \n1008:         Parameters\n1009:         ----------\n1010:         X : Ignored\n1011: \n1012:         y : Ignored\n1013: \n1014:         Returns\n1015:         -------\n1016:         self : object\n1017:             Returns the object itself\n1018:         \"\"\"\n1019:         return self\n1020: \n1021: \n1022: class DictionaryLearning(BaseEstimator, SparseCodingMixin):\n1023:     \"\"\"Dictionary learning\n1024: \n1025:     Finds a dictionary (a set of atoms) that can best be used to represent data\n1026:     using a sparse code.\n1027: \n1028:     Solves the optimization problem::\n1029: \n1030:         (U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1\n1031:                     (U,V)\n1032:                     with || V_k ||_2 = 1 for all  0 <= k < n_components\n1033: \n1034:     Read more in the :ref:`User Guide <DictionaryLearning>`.\n1035: \n1036:     Parameters\n1037:     ----------\n1038:     n_components : int,\n1039:         number of dictionary elements to extract\n1040: \n1041:     alpha : float,\n1042:         sparsity controlling parameter\n1043: \n1044:     max_iter : int,\n1045:         maximum number of iterations to perform\n1046: \n1047:     tol : float,\n1048:         tolerance for numerical error\n1049: \n1050:     fit_algorithm : {'lars', 'cd'}\n1051:         lars: uses the least angle regression method to solve the lasso problem\n1052:         (linear_model.lars_path)\n1053:         cd: uses the coordinate descent method to compute the\n1054:         Lasso solution (linear_model.Lasso). Lars will be faster if\n1055:         the estimated components are sparse.\n1056: \n1057:         .. versionadded:: 0.17\n1058:            *cd* coordinate descent method to improve speed.\n1059: \n1060:     transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\n1061:     'threshold'}\n1062:         Algorithm used to transform the data\n1063:         lars: uses the least angle regression method (linear_model.lars_path)\n1064:         lasso_lars: uses Lars to compute the Lasso solution\n1065:         lasso_cd: uses the coordinate descent method to compute the\n1066:         Lasso solution (linear_model.Lasso). lasso_lars will be faster if\n1067:         the estimated components are sparse.\n1068:         omp: uses orthogonal matching pursuit to estimate the sparse solution\n1069:         threshold: squashes to zero all coefficients less than alpha from\n1070:         the projection ``dictionary * X'``\n1071: \n1072:         .. versionadded:: 0.17\n1073:            *lasso_cd* coordinate descent method to improve speed.\n1074: \n1075:     transform_n_nonzero_coefs : int, ``0.1 * n_features`` by default\n1076:         Number of nonzero coefficients to target in each column of the\n1077:         solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n1078:         and is overridden by `alpha` in the `omp` case.\n1079: \n1080:     transform_alpha : float, 1. by default\n1081:         If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n1082:         penalty applied to the L1 norm.\n1083:         If `algorithm='threshold'`, `alpha` is the absolute value of the\n1084:         threshold below which coefficients will be squashed to zero.\n1085:         If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n1086:         the reconstruction error targeted. In this case, it overrides\n1087:         `n_nonzero_coefs`.\n1088: \n1089:     n_jobs : int or None, optional (default=None)\n1090:         Number of parallel jobs to run.\n1091:         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n1092:         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n1093:         for more details.\n1094: \n1095:     code_init : array of shape (n_samples, n_components),\n1096:         initial value for the code, for warm restart\n1097: \n1098:     dict_init : array of shape (n_components, n_features),\n1099:         initial values for the dictionary, for warm restart\n1100: \n1101:     verbose : bool, optional (default: False)\n1102:         To control the verbosity of the procedure.\n1103: \n1104:     split_sign : bool, False by default\n1105:         Whether to split the sparse feature vector into the concatenation of\n1106:         its negative part and its positive part. This can improve the\n1107:         performance of downstream classifiers.\n1108: \n1109:     random_state : int, RandomState instance or None, optional (default=None)\n1110:         If int, random_state is the seed used by the random number generator;\n1111:         If RandomState instance, random_state is the random number generator;\n1112:         If None, the random number generator is the RandomState instance used\n1113:         by `np.random`.\n1114: \n1115:     positive_code : bool\n1116:         Whether to enforce positivity when finding the code.\n1117: \n1118:         .. versionadded:: 0.20\n1119: \n1120:     positive_dict : bool\n1121:         Whether to enforce positivity when finding the dictionary\n1122: \n1123:         .. versionadded:: 0.20\n1124: \n1125:     Attributes\n1126:     ----------\n1127:     components_ : array, [n_components, n_features]\n1128:         dictionary atoms extracted from the data\n1129: \n1130:     error_ : array\n1131:         vector of errors at each iteration\n1132: \n1133:     n_iter_ : int\n1134:         Number of iterations run.\n1135: \n1136:     Notes\n1137:     -----\n1138:     **References:**\n1139: \n1140:     J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\n1141:     for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\n1142: \n1143:     See also\n1144:     --------\n1145:     SparseCoder\n1146:     MiniBatchDictionaryLearning\n1147:     SparsePCA\n1148:     MiniBatchSparsePCA\n1149:     \"\"\"\n1150:     def __init__(self, n_components=None, alpha=1, max_iter=1000, tol=1e-8,\n1151:                  fit_algorithm='lars', transform_algorithm='omp',\n1152:                  transform_n_nonzero_coefs=None, transform_alpha=None,\n1153:                  n_jobs=None, code_init=None, dict_init=None, verbose=False,\n1154:                  split_sign=False, random_state=None,\n1155:                  positive_code=False, positive_dict=False):\n1156: \n1157:         self._set_sparse_coding_params(n_components, transform_algorithm,\n1158:                                        transform_n_nonzero_coefs,\n1159:                                        transform_alpha, split_sign, n_jobs,\n1160:                                        positive_code)\n1161:         self.alpha = alpha\n1162:         self.max_iter = max_iter\n1163:         self.tol = tol\n1164:         self.fit_algorithm = fit_algorithm\n1165:         self.code_init = code_init\n1166:         self.dict_init = dict_init\n1167:         self.verbose = verbose\n1168:         self.random_state = random_state\n1169:         self.positive_dict = positive_dict\n1170: \n1171:     def fit(self, X, y=None):\n1172:         \"\"\"Fit the model from data in X.\n1173: \n1174:         Parameters\n1175:         ----------\n1176:         X : array-like, shape (n_samples, n_features)\n1177:             Training vector, where n_samples in the number of samples\n1178:             and n_features is the number of features.\n1179: \n1180:         y : Ignored\n1181: \n1182:         Returns\n1183:         -------\n1184:         self : object\n1185:             Returns the object itself\n1186:         \"\"\"\n1187:         random_state = check_random_state(self.random_state)\n1188:         X = check_array(X)\n1189:         if self.n_components is None:\n1190:             n_components = X.shape[1]\n1191:         else:\n1192:             n_components = self.n_components\n1193: \n1194:         V, U, E, self.n_iter_ = dict_learning(\n1195:             X, n_components, self.alpha,\n1196:             tol=self.tol, max_iter=self.max_iter,\n1197:             method=self.fit_algorithm,\n1198:             n_jobs=self.n_jobs,\n1199:             code_init=self.code_init,\n1200:             dict_init=self.dict_init,\n1201:             verbose=self.verbose,\n1202:             random_state=random_state,\n1203:             return_n_iter=True,\n1204:             positive_dict=self.positive_dict,\n1205:             positive_code=self.positive_code)\n1206:         self.components_ = U\n1207:         self.error_ = E\n1208:         return self\n1209: \n1210: \n1211: class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):\n1212:     \"\"\"Mini-batch dictionary learning\n1213: \n1214:     Finds a dictionary (a set of atoms) that can best be used to represent data\n1215:     using a sparse code.\n1216: \n1217:     Solves the optimization problem::\n1218: \n1219:        (U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1\n1220:                     (U,V)\n1221:                     with || V_k ||_2 = 1 for all  0 <= k < n_components\n1222: \n1223:     Read more in the :ref:`User Guide <DictionaryLearning>`.\n1224: \n1225:     Parameters\n1226:     ----------\n1227:     n_components : int,\n1228:         number of dictionary elements to extract\n1229: \n1230:     alpha : float,\n1231:         sparsity controlling parameter\n1232: \n1233:     n_iter : int,\n1234:         total number of iterations to perform\n1235: \n1236:     fit_algorithm : {'lars', 'cd'}\n1237:         lars: uses the least angle regression method to solve the lasso problem\n1238:         (linear_model.lars_path)\n1239:         cd: uses the coordinate descent method to compute the\n1240:         Lasso solution (linear_model.Lasso). Lars will be faster if\n1241:         the estimated components are sparse.\n1242: \n1243:     n_jobs : int or None, optional (default=None)\n1244:         Number of parallel jobs to run.\n1245:         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n1246:         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n1247:         for more details.\n1248: \n1249:     batch_size : int,\n1250:         number of samples in each mini-batch\n1251: \n1252:     shuffle : bool,\n1253:         whether to shuffle the samples before forming batches\n1254: \n1255:     dict_init : array of shape (n_components, n_features),\n1256:         initial value of the dictionary for warm restart scenarios\n1257: \n1258:     transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\n1259:     'threshold'}\n1260:         Algorithm used to transform the data.\n1261:         lars: uses the least angle regression method (linear_model.lars_path)\n1262:         lasso_lars: uses Lars to compute the Lasso solution\n1263:         lasso_cd: uses the coordinate descent method to compute the\n1264:         Lasso solution (linear_model.Lasso). lasso_lars will be faster if\n1265:         the estimated components are sparse.\n1266:         omp: uses orthogonal matching pursuit to estimate the sparse solution\n1267:         threshold: squashes to zero all coefficients less than alpha from\n1268:         the projection dictionary * X'\n1269: \n1270:     transform_n_nonzero_coefs : int, ``0.1 * n_features`` by default\n1271:         Number of nonzero coefficients to target in each column of the\n1272:         solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n1273:         and is overridden by `alpha` in the `omp` case.\n1274: \n1275:     transform_alpha : float, 1. by default\n1276:         If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n1277:         penalty applied to the L1 norm.\n1278:         If `algorithm='threshold'`, `alpha` is the absolute value of the\n1279:         threshold below which coefficients will be squashed to zero.\n1280:         If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n1281:         the reconstruction error targeted. In this case, it overrides\n1282:         `n_nonzero_coefs`.\n1283: \n1284:     verbose : bool, optional (default: False)\n1285:         To control the verbosity of the procedure.\n1286: \n1287:     split_sign : bool, False by default\n1288:         Whether to split the sparse feature vector into the concatenation of\n1289:         its negative part and its positive part. This can improve the\n1290:         performance of downstream classifiers.\n1291: \n1292:     random_state : int, RandomState instance or None, optional (default=None)\n1293:         If int, random_state is the seed used by the random number generator;\n1294:         If RandomState instance, random_state is the random number generator;\n1295:         If None, the random number generator is the RandomState instance used\n1296:         by `np.random`.\n1297: \n1298:     positive_code : bool\n1299:         Whether to enforce positivity when finding the code.\n1300: \n1301:         .. versionadded:: 0.20\n1302: \n1303:     positive_dict : bool\n1304:         Whether to enforce positivity when finding the dictionary.\n1305: \n1306:         .. versionadded:: 0.20\n1307: \n1308:     Attributes\n1309:     ----------\n1310:     components_ : array, [n_components, n_features]\n1311:         components extracted from the data\n1312: \n1313:     inner_stats_ : tuple of (A, B) ndarrays\n1314:         Internal sufficient statistics that are kept by the algorithm.\n1315:         Keeping them is useful in online settings, to avoid loosing the\n1316:         history of the evolution, but they shouldn't have any use for the\n1317:         end user.\n1318:         A (n_components, n_components) is the dictionary covariance matrix.\n1319:         B (n_features, n_components) is the data approximation matrix\n1320: \n1321:     n_iter_ : int\n1322:         Number of iterations run.\n1323: \n1324:     Notes\n1325:     -----\n1326:     **References:**\n1327: \n1328:     J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\n1329:     for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\n1330: \n1331:     See also\n1332:     --------\n1333:     SparseCoder\n1334:     DictionaryLearning\n1335:     SparsePCA\n1336:     MiniBatchSparsePCA\n1337: \n1338:     \"\"\"\n1339:     def __init__(self, n_components=None, alpha=1, n_iter=1000,\n1340:                  fit_algorithm='lars', n_jobs=None, batch_size=3,\n1341:                  shuffle=True, dict_init=None, transform_algorithm='omp',\n1342:                  transform_n_nonzero_coefs=None, transform_alpha=None,\n1343:                  verbose=False, split_sign=False, random_state=None,\n1344:                  positive_code=False, positive_dict=False):\n1345: \n1346:         self._set_sparse_coding_params(n_components, transform_algorithm,\n1347:                                        transform_n_nonzero_coefs,\n1348:                                        transform_alpha, split_sign, n_jobs,\n1349:                                        positive_code)\n1350:         self.alpha = alpha\n1351:         self.n_iter = n_iter\n1352:         self.fit_algorithm = fit_algorithm\n1353:         self.dict_init = dict_init\n1354:         self.verbose = verbose\n1355:         self.shuffle = shuffle\n1356:         self.batch_size = batch_size\n1357:         self.split_sign = split_sign\n1358:         self.random_state = random_state\n1359:         self.positive_dict = positive_dict\n1360: \n1361:     def fit(self, X, y=None):\n1362:         \"\"\"Fit the model from data in X.\n1363: \n1364:         Parameters\n1365:         ----------\n1366:         X : array-like, shape (n_samples, n_features)\n1367:             Training vector, where n_samples in the number of samples\n1368:             and n_features is the number of features.\n1369: \n1370:         y : Ignored\n1371: \n1372:         Returns\n1373:         -------\n1374:         self : object\n1375:             Returns the instance itself.\n1376:         \"\"\"\n1377:         random_state = check_random_state(self.random_state)\n1378:         X = check_array(X)\n1379: \n1380:         U, (A, B), self.n_iter_ = dict_learning_online(\n1381:             X, self.n_components, self.alpha,\n1382:             n_iter=self.n_iter, return_code=False,\n1383:             method=self.fit_algorithm,\n1384:             n_jobs=self.n_jobs, dict_init=self.dict_init,\n1385:             batch_size=self.batch_size, shuffle=self.shuffle,\n1386:             verbose=self.verbose, random_state=random_state,\n1387:             return_inner_stats=True,\n1388:             return_n_iter=True,\n1389:             positive_dict=self.positive_dict,\n1390:             positive_code=self.positive_code)\n1391:         self.components_ = U\n1392:         # Keep track of the state of the algorithm to be able to do\n1393:         # some online fitting (partial_fit)\n1394:         self.inner_stats_ = (A, B)\n1395:         self.iter_offset_ = self.n_iter\n1396:         return self\n1397: \n1398:     def partial_fit(self, X, y=None, iter_offset=None):\n1399:         \"\"\"Updates the model using the data in X as a mini-batch.\n1400: \n1401:         Parameters\n1402:         ----------\n1403:         X : array-like, shape (n_samples, n_features)\n1404:             Training vector, where n_samples in the number of samples\n1405:             and n_features is the number of features.\n1406: \n1407:         y : Ignored\n1408: \n1409:         iter_offset : integer, optional\n1410:             The number of iteration on data batches that has been\n1411:             performed before this call to partial_fit. This is optional:\n1412:             if no number is passed, the memory of the object is\n1413:             used.\n1414: \n1415:         Returns\n1416:         -------\n1417:         self : object\n1418:             Returns the instance itself.\n1419:         \"\"\"\n1420:         if not hasattr(self, 'random_state_'):\n1421:             self.random_state_ = check_random_state(self.random_state)\n1422:         X = check_array(X)\n1423:         if hasattr(self, 'components_'):\n1424:             dict_init = self.components_\n1425:         else:\n1426:             dict_init = self.dict_init\n1427:         inner_stats = getattr(self, 'inner_stats_', None)\n1428:         if iter_offset is None:\n... Code Truncated ...\n\n",
  "line_level_localization": [
    {
      "filename": "/examples/decomposition/plot_sparse_coding.py",
      "suspect_lines": [
        30,
        31,
        32
      ]
    },
    {
      "filename": "/sklearn/decomposition/dict_learning.py",
      "suspect_lines": [
        76,
        130,
        249,
        426,
        580,
        617,
        645,
        809,
        846,
        868,
        902,
        903,
        994,
        999,
        1154,
        1155,
        1160,
        1340,
        1341,
        1344,
        1349
      ]
    }
  ]
}