{
  "instance_id": "django__django-14559",
  "problem_statement": "Include number of rows matched in bulk_update() return value\nDescription\n\t\nCurrently, bulk_update() returns None, unlike update(), which returns ​the number of rows matched.\nIt looks like it would be easy to add the same functionality to bulk_update() since bulk_update() simply calls update() repeatedly:\n​https://github.com/django/django/blob/2b4b6c8af0aae8785bc1347cf1be2e8e70fd5ff3/django/db/models/query.py#L568\nI.e. the return values could simply be added and returned.\n",
  "localized_code": "[start of django/db/models/query.py]\n1: \"\"\"\n2: The main QuerySet implementation. This provides the public API for the ORM.\n3: \"\"\"\n4: \n5: import copy\n6: import operator\n7: import warnings\n8: from itertools import chain\n9: \n10: import django\n11: from django.conf import settings\n12: from django.core import exceptions\n13: from django.db import (\n14:     DJANGO_VERSION_PICKLE_KEY, IntegrityError, NotSupportedError, connections,\n15:     router, transaction,\n16: )\n17: from django.db.models import AutoField, DateField, DateTimeField, sql\n18: from django.db.models.constants import LOOKUP_SEP\n19: from django.db.models.deletion import Collector\n20: from django.db.models.expressions import Case, Expression, F, Ref, Value, When\n21: from django.db.models.functions import Cast, Trunc\n22: from django.db.models.query_utils import FilteredRelation, Q\n23: from django.db.models.sql.constants import CURSOR, GET_ITERATOR_CHUNK_SIZE\n24: from django.db.models.utils import create_namedtuple_class, resolve_callables\n25: from django.utils import timezone\n26: from django.utils.functional import cached_property, partition\n27: \n28: # The maximum number of results to fetch in a get() query.\n29: MAX_GET_RESULTS = 21\n30: \n31: # The maximum number of items to display in a QuerySet.__repr__\n32: REPR_OUTPUT_SIZE = 20\n33: \n34: \n35: class BaseIterable:\n36:     def __init__(self, queryset, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):\n37:         self.queryset = queryset\n38:         self.chunked_fetch = chunked_fetch\n39:         self.chunk_size = chunk_size\n40: \n41: \n42: class ModelIterable(BaseIterable):\n43:     \"\"\"Iterable that yields a model instance for each row.\"\"\"\n44: \n45:     def __iter__(self):\n46:         queryset = self.queryset\n47:         db = queryset.db\n48:         compiler = queryset.query.get_compiler(using=db)\n49:         # Execute the query. This will also fill compiler.select, klass_info,\n50:         # and annotations.\n51:         results = compiler.execute_sql(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)\n52:         select, klass_info, annotation_col_map = (compiler.select, compiler.klass_info,\n53:                                                   compiler.annotation_col_map)\n54:         model_cls = klass_info['model']\n55:         select_fields = klass_info['select_fields']\n56:         model_fields_start, model_fields_end = select_fields[0], select_fields[-1] + 1\n57:         init_list = [f[0].target.attname\n58:                      for f in select[model_fields_start:model_fields_end]]\n59:         related_populators = get_related_populators(klass_info, select, db)\n60:         known_related_objects = [\n61:             (field, related_objs, operator.attrgetter(*[\n62:                 field.attname\n63:                 if from_field == 'self' else\n64:                 queryset.model._meta.get_field(from_field).attname\n65:                 for from_field in field.from_fields\n66:             ])) for field, related_objs in queryset._known_related_objects.items()\n67:         ]\n68:         for row in compiler.results_iter(results):\n69:             obj = model_cls.from_db(db, init_list, row[model_fields_start:model_fields_end])\n70:             for rel_populator in related_populators:\n71:                 rel_populator.populate(row, obj)\n72:             if annotation_col_map:\n73:                 for attr_name, col_pos in annotation_col_map.items():\n74:                     setattr(obj, attr_name, row[col_pos])\n75: \n76:             # Add the known related objects to the model.\n77:             for field, rel_objs, rel_getter in known_related_objects:\n78:                 # Avoid overwriting objects loaded by, e.g., select_related().\n79:                 if field.is_cached(obj):\n80:                     continue\n81:                 rel_obj_id = rel_getter(obj)\n82:                 try:\n83:                     rel_obj = rel_objs[rel_obj_id]\n84:                 except KeyError:\n85:                     pass  # May happen in qs1 | qs2 scenarios.\n86:                 else:\n87:                     setattr(obj, field.name, rel_obj)\n88: \n89:             yield obj\n90: \n91: \n92: class ValuesIterable(BaseIterable):\nCode replaced for brevity.\n110: \n111: \n112: \n113: class ValuesListIterable(BaseIterable):\nCode replaced for brevity.\n140: \n141: \n142: \n143: class NamedValuesListIterable(ValuesListIterable):\nCode replaced for brevity.\n159: \n160: \n161: \n162: class FlatValuesListIterable(BaseIterable):\nCode replaced for brevity.\n172: \n173: \n174: \n175: class QuerySet:\n176:     \"\"\"Represent a lazy database lookup for a set of objects.\"\"\"\n177: \n178:     def __init__(self, model=None, query=None, using=None, hints=None):\n179:         self.model = model\n180:         self._db = using\n181:         self._hints = hints or {}\n182:         self._query = query or sql.Query(self.model)\n183:         self._result_cache = None\n184:         self._sticky_filter = False\n185:         self._for_write = False\n186:         self._prefetch_related_lookups = ()\n187:         self._prefetch_done = False\n188:         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}\n189:         self._iterable_class = ModelIterable\n190:         self._fields = None\n191:         self._defer_next_filter = False\n192:         self._deferred_filter = None\n193: \n194:     @property\n195:     def query(self):\n196:         if self._deferred_filter:\n197:             negate, args, kwargs = self._deferred_filter\n198:             self._filter_or_exclude_inplace(negate, args, kwargs)\n199:             self._deferred_filter = None\n200:         return self._query\n201: \n202:     @query.setter\n203:     def query(self, value):\n204:         if value.values_select:\n205:             self._iterable_class = ValuesIterable\n206:         self._query = value\n207: \n208:     def as_manager(cls):\n209:         # Address the circular dependency between `Queryset` and `Manager`.\n210:         from django.db.models.manager import Manager\n211:         manager = Manager.from_queryset(cls)()\n212:         manager._built_with_as_manager = True\n213:         return manager\n214:     as_manager.queryset_only = True\n215:     as_manager = classmethod(as_manager)\n216: \n217:     ########################\n218:     # PYTHON MAGIC METHODS #\n219:     ########################\n220: \n221:     def __deepcopy__(self, memo):\n222:         \"\"\"Don't populate the QuerySet's cache.\"\"\"\n223:         obj = self.__class__()\n224:         for k, v in self.__dict__.items():\n225:             if k == '_result_cache':\n226:                 obj.__dict__[k] = None\n227:             else:\n228:                 obj.__dict__[k] = copy.deepcopy(v, memo)\n229:         return obj\n230: \n231:     def __getstate__(self):\n232:         # Force the cache to be fully populated.\n233:         self._fetch_all()\n234:         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}\n235: \n236:     def __setstate__(self, state):\n237:         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)\n238:         if pickled_version:\n239:             if pickled_version != django.__version__:\n240:                 warnings.warn(\n241:                     \"Pickled queryset instance's Django version %s does not \"\n242:                     \"match the current version %s.\"\n243:                     % (pickled_version, django.__version__),\n244:                     RuntimeWarning,\n245:                     stacklevel=2,\n246:                 )\n247:         else:\n248:             warnings.warn(\n249:                 \"Pickled queryset instance's Django version is not specified.\",\n250:                 RuntimeWarning,\n251:                 stacklevel=2,\n252:             )\n253:         self.__dict__.update(state)\n254: \n255:     def __repr__(self):\n256:         data = list(self[:REPR_OUTPUT_SIZE + 1])\n257:         if len(data) > REPR_OUTPUT_SIZE:\n258:             data[-1] = \"...(remaining elements truncated)...\"\n259:         return '<%s %r>' % (self.__class__.__name__, data)\n260: \n261:     def __len__(self):\n262:         self._fetch_all()\n263:         return len(self._result_cache)\n264: \n265:     def __iter__(self):\n266:         \"\"\"\n267:         The queryset iterator protocol uses three nested iterators in the\n268:         default case:\n269:             1. sql.compiler.execute_sql()\n270:                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)\n271:                  using cursor.fetchmany(). This part is responsible for\n272:                  doing some column masking, and returning the rows in chunks.\n273:             2. sql.compiler.results_iter()\n274:                - Returns one row at time. At this point the rows are still just\n275:                  tuples. In some cases the return values are converted to\n276:                  Python values at this location.\n277:             3. self.iterator()\n278:                - Responsible for turning the rows into model objects.\n279:         \"\"\"\n280:         self._fetch_all()\n281:         return iter(self._result_cache)\n282: \n283:     def __bool__(self):\n284:         self._fetch_all()\n285:         return bool(self._result_cache)\n286: \n287:     def __getitem__(self, k):\n288:         \"\"\"Retrieve an item or slice from the set of results.\"\"\"\n289:         if not isinstance(k, (int, slice)):\n290:             raise TypeError(\n291:                 'QuerySet indices must be integers or slices, not %s.'\n292:                 % type(k).__name__\n293:             )\n294:         assert ((not isinstance(k, slice) and (k >= 0)) or\n295:                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and\n296:                  (k.stop is None or k.stop >= 0))), \\\n297:             \"Negative indexing is not supported.\"\n298: \n299:         if self._result_cache is not None:\n300:             return self._result_cache[k]\n301: \n302:         if isinstance(k, slice):\n303:             qs = self._chain()\n304:             if k.start is not None:\n305:                 start = int(k.start)\n306:             else:\n307:                 start = None\n308:             if k.stop is not None:\n309:                 stop = int(k.stop)\n310:             else:\n311:                 stop = None\n312:             qs.query.set_limits(start, stop)\n313:             return list(qs)[::k.step] if k.step else qs\n314: \n315:         qs = self._chain()\n316:         qs.query.set_limits(k, k + 1)\n317:         qs._fetch_all()\n318:         return qs._result_cache[0]\n319: \n320:     def __class_getitem__(cls, *args, **kwargs):\n321:         return cls\n322: \n323:     def __and__(self, other):\n324:         self._merge_sanity_check(other)\n325:         if isinstance(other, EmptyQuerySet):\n326:             return other\n327:         if isinstance(self, EmptyQuerySet):\n328:             return self\n329:         combined = self._chain()\n330:         combined._merge_known_related_objects(other)\n331:         combined.query.combine(other.query, sql.AND)\n332:         return combined\n333: \n334:     def __or__(self, other):\n335:         self._merge_sanity_check(other)\n336:         if isinstance(self, EmptyQuerySet):\n337:             return other\n338:         if isinstance(other, EmptyQuerySet):\n339:             return self\n340:         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))\n341:         combined = query._chain()\n342:         combined._merge_known_related_objects(other)\n343:         if not other.query.can_filter():\n344:             other = other.model._base_manager.filter(pk__in=other.values('pk'))\n345:         combined.query.combine(other.query, sql.OR)\n346:         return combined\n347: \n348:     ####################################\n349:     # METHODS THAT DO DATABASE QUERIES #\n350:     ####################################\n351: \n352:     def _iterator(self, use_chunked_fetch, chunk_size):\n353:         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)\n354: \n355:     def iterator(self, chunk_size=2000):\n356:         \"\"\"\n357:         An iterator over the results from applying this QuerySet to the\n358:         database.\n359:         \"\"\"\n360:         if chunk_size <= 0:\n361:             raise ValueError('Chunk size must be strictly positive.')\n362:         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')\n363:         return self._iterator(use_chunked_fetch, chunk_size)\n364: \n365:     def aggregate(self, *args, **kwargs):\n366:         \"\"\"\n367:         Return a dictionary containing the calculations (aggregation)\n368:         over the current queryset.\n369: \n370:         If args is present the expression is passed as a kwarg using\n371:         the Aggregate object's default alias.\n372:         \"\"\"\n373:         if self.query.distinct_fields:\n374:             raise NotImplementedError(\"aggregate() + distinct(fields) not implemented.\")\n375:         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')\n376:         for arg in args:\n377:             # The default_alias property raises TypeError if default_alias\n378:             # can't be set automatically or AttributeError if it isn't an\n379:             # attribute.\n380:             try:\n381:                 arg.default_alias\n382:             except (AttributeError, TypeError):\n383:                 raise TypeError(\"Complex aggregates require an alias\")\n384:             kwargs[arg.default_alias] = arg\n385: \n386:         query = self.query.chain()\n387:         for (alias, aggregate_expr) in kwargs.items():\n388:             query.add_annotation(aggregate_expr, alias, is_summary=True)\n389:             annotation = query.annotations[alias]\n390:             if not annotation.contains_aggregate:\n391:                 raise TypeError(\"%s is not an aggregate expression\" % alias)\n392:             for expr in annotation.get_source_expressions():\n393:                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:\n394:                     name = expr.refs\n395:                     raise exceptions.FieldError(\n396:                         \"Cannot compute %s('%s'): '%s' is an aggregate\"\n397:                         % (annotation.name, name, name)\n398:                     )\n399:         return query.get_aggregation(self.db, kwargs)\n400: \n401:     def count(self):\n402:         \"\"\"\n403:         Perform a SELECT COUNT() and return the number of records as an\n404:         integer.\n405: \n406:         If the QuerySet is already fully cached, return the length of the\n407:         cached results set to avoid multiple SELECT COUNT(*) calls.\n408:         \"\"\"\n409:         if self._result_cache is not None:\n410:             return len(self._result_cache)\n411: \n412:         return self.query.get_count(using=self.db)\n413: \n414:     def get(self, *args, **kwargs):\n415:         \"\"\"\n416:         Perform the query and return a single object matching the given\n417:         keyword arguments.\n418:         \"\"\"\n419:         if self.query.combinator and (args or kwargs):\n420:             raise NotSupportedError(\n421:                 'Calling QuerySet.get(...) with filters after %s() is not '\n422:                 'supported.' % self.query.combinator\n423:             )\n424:         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)\n425:         if self.query.can_filter() and not self.query.distinct_fields:\n426:             clone = clone.order_by()\n427:         limit = None\n428:         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:\n429:             limit = MAX_GET_RESULTS\n430:             clone.query.set_limits(high=limit)\n431:         num = len(clone)\n432:         if num == 1:\n433:             return clone._result_cache[0]\n434:         if not num:\n435:             raise self.model.DoesNotExist(\n436:                 \"%s matching query does not exist.\" %\n437:                 self.model._meta.object_name\n438:             )\n439:         raise self.model.MultipleObjectsReturned(\n440:             'get() returned more than one %s -- it returned %s!' % (\n441:                 self.model._meta.object_name,\n442:                 num if not limit or num < limit else 'more than %s' % (limit - 1),\n443:             )\n444:         )\n445: \n446:     def create(self, **kwargs):\n447:         \"\"\"\n448:         Create a new object with the given kwargs, saving it to the database\n449:         and returning the created object.\n450:         \"\"\"\n451:         obj = self.model(**kwargs)\n452:         self._for_write = True\n453:         obj.save(force_insert=True, using=self.db)\n454:         return obj\n455: \n456:     def _prepare_for_bulk_create(self, objs):\n457:         for obj in objs:\n458:             if obj.pk is None:\n459:                 # Populate new PK values.\n460:                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)\n461:             obj._prepare_related_fields_for_save(operation_name='bulk_create')\n462: \n463:     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):\n464:         \"\"\"\n465:         Insert each of the instances into the database. Do *not* call\n466:         save() on each of the instances, do not send any pre/post_save\n467:         signals, and do not set the primary key attribute if it is an\n468:         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).\n469:         Multi-table models are not supported.\n470:         \"\"\"\n471:         # When you bulk insert you don't get the primary keys back (if it's an\n472:         # autoincrement, except if can_return_rows_from_bulk_insert=True), so\n473:         # you can't insert into the child tables which references this. There\n474:         # are two workarounds:\n475:         # 1) This could be implemented if you didn't have an autoincrement pk\n476:         # 2) You could do it by doing O(n) normal inserts into the parent\n477:         #    tables to get the primary keys back and then doing a single bulk\n478:         #    insert into the childmost table.\n479:         # We currently set the primary keys on the objects when using\n480:         # PostgreSQL via the RETURNING ID clause. It should be possible for\n481:         # Oracle as well, but the semantics for extracting the primary keys is\n482:         # trickier so it's not done yet.\n483:         assert batch_size is None or batch_size > 0\n484:         # Check that the parents share the same concrete model with the our\n485:         # model to detect the inheritance pattern ConcreteGrandParent ->\n486:         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy\n487:         # would not identify that case as involving multiple tables.\n488:         for parent in self.model._meta.get_parent_list():\n489:             if parent._meta.concrete_model is not self.model._meta.concrete_model:\n490:                 raise ValueError(\"Can't bulk create a multi-table inherited model\")\n491:         if not objs:\n492:             return objs\n493:         self._for_write = True\n494:         connection = connections[self.db]\n495:         opts = self.model._meta\n496:         fields = opts.concrete_fields\n497:         objs = list(objs)\n498:         self._prepare_for_bulk_create(objs)\n499:         with transaction.atomic(using=self.db, savepoint=False):\n500:             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n501:             if objs_with_pk:\n502:                 returned_columns = self._batched_insert(\n503:                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,\n504:                 )\n505:                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):\n506:                     for result, field in zip(results, opts.db_returning_fields):\n507:                         if field != opts.pk:\n508:                             setattr(obj_with_pk, field.attname, result)\n509:                 for obj_with_pk in objs_with_pk:\n510:                     obj_with_pk._state.adding = False\n511:                     obj_with_pk._state.db = self.db\n512:             if objs_without_pk:\n513:                 fields = [f for f in fields if not isinstance(f, AutoField)]\n514:                 returned_columns = self._batched_insert(\n515:                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,\n516:                 )\n517:                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:\n518:                     assert len(returned_columns) == len(objs_without_pk)\n519:                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):\n520:                     for result, field in zip(results, opts.db_returning_fields):\n521:                         setattr(obj_without_pk, field.attname, result)\n522:                     obj_without_pk._state.adding = False\n523:                     obj_without_pk._state.db = self.db\n524: \n525:         return objs\n526: \n527:     def bulk_update(self, objs, fields, batch_size=None):\n528:         \"\"\"\n529:         Update the given fields in each of the given objects in the database.\n530:         \"\"\"\n531:         if batch_size is not None and batch_size < 0:\n532:             raise ValueError('Batch size must be a positive integer.')\n533:         if not fields:\n534:             raise ValueError('Field names must be given to bulk_update().')\n535:         objs = tuple(objs)\n536:         if any(obj.pk is None for obj in objs):\n537:             raise ValueError('All bulk_update() objects must have a primary key set.')\n538:         fields = [self.model._meta.get_field(name) for name in fields]\n539:         if any(not f.concrete or f.many_to_many for f in fields):\n540:             raise ValueError('bulk_update() can only be used with concrete fields.')\n541:         if any(f.primary_key for f in fields):\n542:             raise ValueError('bulk_update() cannot be used with primary key fields.')\n543:         if not objs:\n544:             return\n545:         # PK is used twice in the resulting update query, once in the filter\n546:         # and once in the WHEN. Each field will also have one CAST.\n547:         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)\n548:         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n549:         requires_casting = connections[self.db].features.requires_casted_case_in_updates\n550:         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))\n551:         updates = []\n552:         for batch_objs in batches:\n553:             update_kwargs = {}\n554:             for field in fields:\n555:                 when_statements = []\n556:                 for obj in batch_objs:\n557:                     attr = getattr(obj, field.attname)\n558:                     if not isinstance(attr, Expression):\n559:                         attr = Value(attr, output_field=field)\n560:                     when_statements.append(When(pk=obj.pk, then=attr))\n561:                 case_statement = Case(*when_statements, output_field=field)\n562:                 if requires_casting:\n563:                     case_statement = Cast(case_statement, output_field=field)\n564:                 update_kwargs[field.attname] = case_statement\n565:             updates.append(([obj.pk for obj in batch_objs], update_kwargs))\n566:         with transaction.atomic(using=self.db, savepoint=False):\n567:             for pks, update_kwargs in updates:\n568:                 self.filter(pk__in=pks).update(**update_kwargs)\n569:     bulk_update.alters_data = True\n570: \n571:     def get_or_create(self, defaults=None, **kwargs):\n572:         \"\"\"\n573:         Look up an object with the given kwargs, creating one if necessary.\n574:         Return a tuple of (object, created), where created is a boolean\n575:         specifying whether an object was created.\n576:         \"\"\"\n577:         # The get() needs to be targeted at the write database in order\n578:         # to avoid potential transaction consistency problems.\n579:         self._for_write = True\n580:         try:\n581:             return self.get(**kwargs), False\n582:         except self.model.DoesNotExist:\n583:             params = self._extract_model_params(defaults, **kwargs)\n584:             # Try to create an object using passed params.\n585:             try:\n586:                 with transaction.atomic(using=self.db):\n587:                     params = dict(resolve_callables(params))\n588:                     return self.create(**params), True\n589:             except IntegrityError:\n590:                 try:\n591:                     return self.get(**kwargs), False\n592:                 except self.model.DoesNotExist:\n593:                     pass\n594:                 raise\n595: \n596:     def update_or_create(self, defaults=None, **kwargs):\n597:         \"\"\"\n598:         Look up an object with the given kwargs, updating one with defaults\n599:         if it exists, otherwise create a new one.\n600:         Return a tuple (object, created), where created is a boolean\n601:         specifying whether an object was created.\n602:         \"\"\"\n603:         defaults = defaults or {}\n604:         self._for_write = True\n605:         with transaction.atomic(using=self.db):\n606:             # Lock the row so that a concurrent update is blocked until\n607:             # update_or_create() has performed its save.\n608:             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)\n609:             if created:\n610:                 return obj, created\n611:             for k, v in resolve_callables(defaults):\n612:                 setattr(obj, k, v)\n613:             obj.save(using=self.db)\n614:         return obj, False\n615: \n616:     def _extract_model_params(self, defaults, **kwargs):\n617:         \"\"\"\n618:         Prepare `params` for creating a model instance based on the given\n619:         kwargs; for use by get_or_create().\n620:         \"\"\"\n621:         defaults = defaults or {}\n622:         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}\n623:         params.update(defaults)\n624:         property_names = self.model._meta._property_names\n625:         invalid_params = []\n626:         for param in params:\n627:             try:\n628:                 self.model._meta.get_field(param)\n629:             except exceptions.FieldDoesNotExist:\n630:                 # It's okay to use a model's property if it has a setter.\n631:                 if not (param in property_names and getattr(self.model, param).fset):\n632:                     invalid_params.append(param)\n... Code Truncated ...\n\n",
  "line_level_localization": [
    {
      "filename": "/django/db/models/query.py",
      "suspect_lines": [
        544,
        568
      ]
    }
  ]
}