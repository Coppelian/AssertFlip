{
  "instance_id": "django__django-11734",
  "problem_statement": "OuterRef in exclude() or ~Q() uses wrong model.\nDescription\n\t\nThe following test (added to tests/queries/test_qs_combinators) fails when trying to exclude results using OuterRef()\ndef test_exists_exclude(self):\n\t# filter()\n\tqs = Number.objects.annotate(\n\t\tfoo=Exists(\n\t\t\tItem.objects.filter(tags__category_id=OuterRef('pk'))\n\t\t)\n\t).filter(foo=True)\n\tprint(qs) # works\n\t# exclude()\n\tqs = Number.objects.annotate(\n\t\tfoo =Exists(\n\t\t\tItem.objects.exclude(tags__category_id=OuterRef('pk'))\n\t\t)\n\t).filter(foo=True)\n\tprint(qs) # crashes\n\t# filter(~Q())\n\tqs = Number.objects.annotate(\n\t\tfoo =Exists(\n\t\t\tItem.objects.filter(~Q(tags__category_id=OuterRef('pk')))\n\t\t)\n\t).filter(foo=True)\n\tprint(qs) # crashes\nIt results in the following error\nValueError: This queryset contains a reference to an outer query and may only be used in a subquery\n",
  "localized_code": "[start of django/db/models/fields/__init__.py]\n1: import collections.abc\n2: import copy\n3: import datetime\n4: import decimal\n5: import operator\n6: import uuid\n7: import warnings\n8: from base64 import b64decode, b64encode\n9: from functools import partialmethod, total_ordering\n10: \n11: from django import forms\n12: from django.apps import apps\n13: from django.conf import settings\n14: from django.core import checks, exceptions, validators\n15: # When the _meta object was formalized, this exception was moved to\n16: # django.core.exceptions. It is retained here for backwards compatibility\n17: # purposes.\n18: from django.core.exceptions import FieldDoesNotExist  # NOQA\n19: from django.db import connection, connections, router\n20: from django.db.models.constants import LOOKUP_SEP\n21: from django.db.models.query_utils import DeferredAttribute, RegisterLookupMixin\n22: from django.utils import timezone\n23: from django.utils.datastructures import DictWrapper\n24: from django.utils.dateparse import (\n25:     parse_date, parse_datetime, parse_duration, parse_time,\n26: )\n27: from django.utils.duration import duration_microseconds, duration_string\n28: from django.utils.functional import Promise, cached_property\n29: from django.utils.ipv6 import clean_ipv6_address\n30: from django.utils.itercompat import is_iterable\n31: from django.utils.text import capfirst\n32: from django.utils.translation import gettext_lazy as _\n33: \n34: __all__ = [\n35:     'AutoField', 'BLANK_CHOICE_DASH', 'BigAutoField', 'BigIntegerField',\n36:     'BinaryField', 'BooleanField', 'CharField', 'CommaSeparatedIntegerField',\n37:     'DateField', 'DateTimeField', 'DecimalField', 'DurationField',\n38:     'EmailField', 'Empty', 'Field', 'FieldDoesNotExist', 'FilePathField',\n39:     'FloatField', 'GenericIPAddressField', 'IPAddressField', 'IntegerField',\n40:     'NOT_PROVIDED', 'NullBooleanField', 'PositiveIntegerField',\n41:     'PositiveSmallIntegerField', 'SlugField', 'SmallAutoField',\n42:     'SmallIntegerField', 'TextField', 'TimeField', 'URLField', 'UUIDField',\n43: ]\n44: \n45: \n46: class Empty:\n47:     pass\n48: \n49: \n50: class NOT_PROVIDED:\n51:     pass\n52: \n53: \n54: # The values to use for \"blank\" in SelectFields. Will be appended to the start\n55: # of most \"choices\" lists.\n56: BLANK_CHOICE_DASH = [(\"\", \"---------\")]\n57: \n58: \n59: def _load_field(app_label, model_name, field_name):\nCode replaced for brevity.\n60: \n61: \n62: \n63: # A guide to Field parameters:\n64: #\n65: #   * name:      The name of the field specified in the model.\n66: #   * attname:   The attribute to use on the model object. This is the same as\n67: #                \"name\", except in the case of ForeignKeys, where \"_id\" is\n68: #                appended.\n69: #   * db_column: The db_column specified in the model (or None).\n70: #   * column:    The database column for this field. This is the same as\n71: #                \"attname\", except if db_column is specified.\n72: #\n73: # Code that introspects values, or does other dynamic things, should use\n74: # attname. For example, this gets the primary key value of object \"obj\":\n75: #\n76: #     getattr(obj, opts.pk.attname)\n77: \n78: def _empty(of_cls):\n79:     new = Empty()\n80:     new.__class__ = of_cls\n81:     return new\n82: \n83: \n84: def return_None():\n85:     return None\n86: \n87: \n88: @total_ordering\n89: class Field(RegisterLookupMixin):\nCode replaced for brevity.\n898: \n899: \n900: \n901: class BooleanField(Field):\nCode replaced for brevity.\n944: \n945: \n946: \n947:     descript\nCode replaced for brevity.\n1007: \n1008: \n1009: \n1010:     default_\nCode replaced for brevity.\n1023: \n1024: \n1025: \n1026:     def chec\nCode replaced for brevity.\n1055: \n1056: \n1057: \n1058:     empty_st\nCode replaced for brevity.\n1198: \n1199: \n1200: \n1201:     empty_strings_al\nCode replaced for brevity.\n1354: \n1355: \n1356: \n1357: empty_strings_allowed = Fals\nCode replaced for brevity.\n1489: \n1490: \n1491: \n1492:     Store timedelta objects.\nCode replaced for brevity.\n1549: \n1550: \n1551: \n1552: ault_validators = [validators.val\nCode replaced for brevity.\n1573: \n1574: \n1575: \n1576:     de\nCode replaced for brevity.\n1637: \n1638: \n1639: \n1640:     de\nCode replaced for brevity.\n1677: \n1678: \n1679: \n1680:     defaul\nCode replaced for brevity.\n1764: \n1765: \n1766: \n1767: cription = _(\"Big (8 byte) integer\")\nCode replaced for brevity.\n1779: \n1780: \n1781: \n1782:     descriptio\nCode replaced for brevity.\n1810: \n1811: \n1812: \n1813:     descriptio\nCode replaced for brevity.\n1890: \n1891: \n1892: \n1893:         'invalid'\nCode replaced for brevity.\n1912: \n1913: \n1914: \n1915:         \"\"\"\nCode replaced for brevity.\n1929: \n1930: \n1931: \n1932:     def get_int\nCode replaced for brevity.\n1942: \n1943: \n1944: \n1945:     def g\nCode replaced for brevity.\n1955: \n1956: \n1957: \n1958: [validators.validate_slug]\nCode replaced for brevity.\n1988: \n1989: \n1990: \n1991:     def get_intern\nCode replaced for brevity.\n1995: \n1996: \n1997: \n1998: def get_internal_type(s\nCode replaced for brevity.\n2021: \n2022: \n2023: \n2024:     default_error_mess\nCode replaced for brevity.\n2153: \n2154: \n2155: \n2156:     descrip\nCode replaced for brevity.\n2176: \n2177: \n2178: \n2179:     empty_values = [None,\nCode replaced for brevity.\n2240: \n2241: \n2242: \n2243: 'invalid': _('“%(value)s”\nCode replaced for brevity.\n2293: \n2294: \n2295: \n2296: class AutoFieldMixin:\n2297: \n2298:     def __init__(self, *args, **kwargs):\n2299:         kwargs['blank'] = True\n2300:         super().__init__(*args, **kwargs)\n2301: \n2302:     def check(self, **kwargs):\n2303:         return [\n2304:             *super().check(**kwargs),\n2305:             *self._check_primary_key(),\n2306:         ]\n2307: \n2308:     def _check_primary_key(self):\n2309:         if not self.primary_key:\n2310:             return [\n2311:                 checks.Error(\n2312:                     'AutoFields must set primary_key=True.',\n2313:                     obj=self,\n2314:                     id='fields.E100',\n2315:                 ),\n2316:             ]\n2317:         else:\n2318:             return []\n2319: \n2320:     def deconstruct(self):\n2321:         name, path, args, kwargs = super().deconstruct()\n2322:         del kwargs['blank']\n2323:         kwargs['primary_key'] = True\n2324:         return name, path, args, kwargs\n2325: \n2326:     def validate(self, value, model_instance):\n2327:         pass\n2328: \n2329:     def get_db_prep_value(self, value, connection, prepared=False):\n2330:         if not prepared:\n2331:             value = self.get_prep_value(value)\n2332:             value = connection.ops.validate_autopk_value(value)\n2333:         return value\n2334: \n2335:     def get_prep_value(self, value):\n2336:         from django.db.models.expressions import OuterRef\n2337:         return value if isinstance(value, OuterRef) else super().get_prep_value(value)\n2338: \n2339:     def contribute_to_class(self, cls, name, **kwargs):\n2340:         assert not cls._meta.auto_field, (\n2341:             \"Model %s can't have more than one auto-generated field.\"\n2342:             % cls._meta.label\n2343:         )\n2344:         super().contribute_to_class(cls, name, **kwargs)\n2345:         cls._meta.auto_field = self\n2346: \n2347:     def formfield(self, **kwargs):\n2348:         return None\n2349: \n2350: \n2351: kward inheritance compatibility\nCode replaced for brevity.\n2376: \n2377: \n2378: \n2379:     def\nCode replaced for brevity.\n2385: \n2386: \n2387: \n2388:         return 'BigAutoField'\nCode replaced for brevity.\n2394: \n2395: \n2396: \n2397:         return 'SmallAutoField'\nCode replaced for brevity.\n2403: \n\n[start of django/db/models/fields/related_lookups.py]\n1: from django.db.models.lookups import (\n2:     Exact, GreaterThan, GreaterThanOrEqual, In, IsNull, LessThan,\n3:     LessThanOrEqual,\n4: )\n5: \n6: \n7: class MultiColSource:\n8:     contains_aggregate = False\n9: \n10:     def __init__(self, alias, targets, sources, field):\n11:         self.targets, self.sources, self.field, self.alias = targets, sources, field, alias\n12:         self.output_field = self.field\n13: \n14:     def __repr__(self):\n15:         return \"{}({}, {})\".format(\n16:             self.__class__.__name__, self.alias, self.field)\n17: \n18:     def relabeled_clone(self, relabels):\n19:         return self.__class__(relabels.get(self.alias, self.alias),\n20:                               self.targets, self.sources, self.field)\n21: \n22:     def get_lookup(self, lookup):\n23:         return self.output_field.get_lookup(lookup)\n24: \n25: \n26: def get_normalized_value(value, lhs):\n27:     from django.db.models import Model\n28:     if isinstance(value, Model):\n29:         value_list = []\n30:         sources = lhs.output_field.get_path_info()[-1].target_fields\n31:         for source in sources:\n32:             while not isinstance(value, source.model) and source.remote_field:\n33:                 source = source.remote_field.model._meta.get_field(source.remote_field.field_name)\n34:             try:\n35:                 value_list.append(getattr(value, source.attname))\n36:             except AttributeError:\n37:                 # A case like Restaurant.objects.filter(place=restaurant_instance),\n38:                 # where place is a OneToOneField and the primary key of Restaurant.\n39:                 return (value.pk,)\n40:         return tuple(value_list)\n41:     if not isinstance(value, tuple):\n42:         return (value,)\n43:     return value\n44: \n45: \n46: class RelatedIn(In):\n47:     def get_prep_lookup(self):\n48:         if not isinstance(self.lhs, MultiColSource) and self.rhs_is_direct_value():\n49:             # If we get here, we are dealing with single-column relations.\n50:             self.rhs = [get_normalized_value(val, self.lhs)[0] for val in self.rhs]\n51:             # We need to run the related field's get_prep_value(). Consider case\n52:             # ForeignKey to IntegerField given value 'abc'. The ForeignKey itself\n53:             # doesn't have validation for non-integers, so we must run validation\n54:             # using the target field.\n55:             if hasattr(self.lhs.output_field, 'get_path_info'):\n56:                 # Run the target field's get_prep_value. We can safely assume there is\n57:                 # only one as we don't get to the direct value branch otherwise.\n58:                 target_field = self.lhs.output_field.get_path_info()[-1].target_fields[-1]\n59:                 self.rhs = [target_field.get_prep_value(v) for v in self.rhs]\n60:         return super().get_prep_lookup()\n61: \n62:     def as_sql(self, compiler, connection):\n63:         if isinstance(self.lhs, MultiColSource):\n64:             # For multicolumn lookups we need to build a multicolumn where clause.\n65:             # This clause is either a SubqueryConstraint (for values that need to be compiled to\n66:             # SQL) or an OR-combined list of (col1 = val1 AND col2 = val2 AND ...) clauses.\n67:             from django.db.models.sql.where import WhereNode, SubqueryConstraint, AND, OR\n68: \n69:             root_constraint = WhereNode(connector=OR)\n70:             if self.rhs_is_direct_value():\n71:                 values = [get_normalized_value(value, self.lhs) for value in self.rhs]\n72:                 for value in values:\n73:                     value_constraint = WhereNode()\n74:                     for source, target, val in zip(self.lhs.sources, self.lhs.targets, value):\n75:                         lookup_class = target.get_lookup('exact')\n76:                         lookup = lookup_class(target.get_col(self.lhs.alias, source), val)\n77:                         value_constraint.add(lookup, AND)\n78:                     root_constraint.add(value_constraint, OR)\n79:             else:\n80:                 root_constraint.add(\n81:                     SubqueryConstraint(\n82:                         self.lhs.alias, [target.column for target in self.lhs.targets],\n83:                         [source.name for source in self.lhs.sources], self.rhs),\n84:                     AND)\n85:             return root_constraint.as_sql(compiler, connection)\n86:         else:\n87:             if (not getattr(self.rhs, 'has_select_fields', True) and\n88:                     not getattr(self.lhs.field.target_field, 'primary_key', False)):\n89:                 self.rhs.clear_select_clause()\n90:                 if (getattr(self.lhs.output_field, 'primary_key', False) and\n91:                         self.lhs.output_field.model == self.rhs.model):\n92:                     # A case like Restaurant.objects.filter(place__in=restaurant_qs),\n93:                     # where place is a OneToOneField and the primary key of\n94:                     # Restaurant.\n95:                     target_field = self.lhs.field.name\n96:                 else:\n97:                     target_field = self.lhs.field.target_field.name\n98:                 self.rhs.add_fields([target_field], True)\n99:             return super().as_sql(compiler, connection)\n100: \n101: \n102: class RelatedLookupMixin:\n103:     def get_prep_lookup(self):\n104:         if not isinstance(self.lhs, MultiColSource) and self.rhs_is_direct_value():\n105:             # If we get here, we are dealing with single-column relations.\n... Code Truncated ...\n\n[start of django/db/models/sql/query.py]\n1: \"\"\"\n2: Create SQL statements for QuerySets.\n3: \n4: The code in here encapsulates all of the SQL construction so that QuerySets\n5: themselves do not have to (and could be backed by things other than SQL\n6: databases). The abstraction barrier only works one way: this module has to know\n7: all about the internals of models in order to get the information it needs.\n8: \"\"\"\n9: import difflib\n10: import functools\n11: import inspect\n12: import sys\n13: import warnings\n14: from collections import Counter, namedtuple\n15: from collections.abc import Iterator, Mapping\n16: from itertools import chain, count, product\n17: from string import ascii_uppercase\n18: \n19: from django.core.exceptions import (\n20:     EmptyResultSet, FieldDoesNotExist, FieldError,\n21: )\n22: from django.db import DEFAULT_DB_ALIAS, NotSupportedError, connections\n23: from django.db.models.aggregates import Count\n24: from django.db.models.constants import LOOKUP_SEP\n25: from django.db.models.expressions import (\n26:     BaseExpression, Col, F, OuterRef, Ref, SimpleCol,\n27: )\n28: from django.db.models.fields import Field\n29: from django.db.models.fields.related_lookups import MultiColSource\n30: from django.db.models.lookups import Lookup\n31: from django.db.models.query_utils import (\n32:     Q, check_rel_lookup_compatibility, refs_expression,\n33: )\n34: from django.db.models.sql.constants import (\n35:     INNER, LOUTER, ORDER_DIR, ORDER_PATTERN, SINGLE,\n36: )\n37: from django.db.models.sql.datastructures import (\n38:     BaseTable, Empty, Join, MultiJoin,\n39: )\n40: from django.db.models.sql.where import (\n41:     AND, OR, ExtraWhere, NothingNode, WhereNode,\n42: )\n43: from django.utils.deprecation import RemovedInDjango40Warning\n44: from django.utils.functional import cached_property\n45: from django.utils.tree import Node\n46: \n47: __all__ = ['Query', 'RawQuery']\n48: \n49: \n50: def get_field_names_from_opts(opts):\n51:     return set(chain.from_iterable(\n52:         (f.name, f.attname) if f.concrete else (f.name,)\n53:         for f in opts.get_fields()\n54:     ))\n55: \n56: \n57: def get_children_from_q(q):\nCode replaced for brevity.\n62: \n63: \n64: \n65: JoinInfo = namedtuple(\n66:     'JoinInfo',\n67:     ('final_field', 'targets', 'opts', 'joins', 'path', 'transform_function')\n68: )\n69: \n70: \n71: def _get_col(target, field, alias, simple_col):\nCode replaced for brevity.\n74: \n75: \n76: \n77: class RawQuery:\nCode replaced for brevity.\n142: \n143: \n144: \n145: class Query(BaseExpression):\n146:     \"\"\"A single SQL query.\"\"\"\n147: \n148:     alias_prefix = 'T'\n149:     subq_aliases = frozenset([alias_prefix])\n150: \n151:     compiler = 'SQLCompiler'\n152: \n153:     def __init__(self, model, where=WhereNode):\n154:         self.model = model\n155:         self.alias_refcount = {}\n156:         # alias_map is the most important data structure regarding joins.\n157:         # It's used for recording which joins exist in the query and what\n158:         # types they are. The key is the alias of the joined table (possibly\n159:         # the table name) and the value is a Join-like object (see\n160:         # sql.datastructures.Join for more information).\n161:         self.alias_map = {}\n162:         # Sometimes the query contains references to aliases in outer queries (as\n163:         # a result of split_exclude). Correct alias quoting needs to know these\n164:         # aliases too.\n165:         self.external_aliases = set()\n166:         self.table_map = {}     # Maps table names to list of aliases.\n167:         self.default_cols = True\n168:         self.default_ordering = True\n169:         self.standard_ordering = True\n170:         self.used_aliases = set()\n171:         self.filter_is_sticky = False\n172:         self.subquery = False\n173: \n174:         # SQL-related attributes\n175:         # Select and related select clauses are expressions to use in the\n176:         # SELECT clause of the query.\n177:         # The select is used for cases where we want to set up the select\n178:         # clause to contain other than default fields (values(), subqueries...)\n179:         # Note that annotations go to annotations dictionary.\n180:         self.select = ()\n181:         self.where = where()\n182:         self.where_class = where\n183:         # The group_by attribute can have one of the following forms:\n184:         #  - None: no group by at all in the query\n185:         #  - A tuple of expressions: group by (at least) those expressions.\n186:         #    String refs are also allowed for now.\n187:         #  - True: group by all select fields of the model\n188:         # See compiler.get_group_by() for details.\n189:         self.group_by = None\n190:         self.order_by = ()\n191:         self.low_mark, self.high_mark = 0, None  # Used for offset/limit\n192:         self.distinct = False\n193:         self.distinct_fields = ()\n194:         self.select_for_update = False\n195:         self.select_for_update_nowait = False\n196:         self.select_for_update_skip_locked = False\n197:         self.select_for_update_of = ()\n198: \n199:         self.select_related = False\n200:         # Arbitrary limit for select_related to prevents infinite recursion.\n201:         self.max_depth = 5\n202: \n203:         # Holds the selects defined by a call to values() or values_list()\n204:         # excluding annotation_select and extra_select.\n205:         self.values_select = ()\n206: \n207:         # SQL annotation-related attributes\n208:         self.annotations = {}  # Maps alias -> Annotation Expression\n209:         self.annotation_select_mask = None\n210:         self._annotation_select_cache = None\n211: \n212:         # Set combination attributes\n213:         self.combinator = None\n214:         self.combinator_all = False\n215:         self.combined_queries = ()\n216: \n217:         # These are for extensions. The contents are more or less appended\n218:         # verbatim to the appropriate clause.\n219:         self.extra = {}  # Maps col_alias -> (col_sql, params).\n220:         self.extra_select_mask = None\n221:         self._extra_select_cache = None\n222: \n223:         self.extra_tables = ()\n224:         self.extra_order_by = ()\n225: \n226:         # A tuple that is a set of model field names and either True, if these\n227:         # are the fields to defer, or False if these are the only fields to\n228:         # load.\n229:         self.deferred_loading = (frozenset(), True)\n230: \n231:         self._filtered_relations = {}\n232: \n233:         self.explain_query = False\n234:         self.explain_format = None\n235:         self.explain_options = {}\n236: \n237:     @property\n238:     def output_field(self):\n239:         if len(self.select) == 1:\n240:             return self.select[0].field\n241:         elif len(self.annotation_select) == 1:\n242:             return next(iter(self.annotation_select.values())).output_field\n243: \n244:     @property\n245:     def has_select_fields(self):\n246:         return bool(self.select or self.annotation_select_mask or self.extra_select_mask)\n247: \n248:     @cached_property\n249:     def base_table(self):\n250:         for alias in self.alias_map:\n251:             return alias\n252: \n253:     def __str__(self):\n254:         \"\"\"\n255:         Return the query as a string of SQL with the parameter values\n256:         substituted in (use sql_with_params() to see the unsubstituted string).\n257: \n258:         Parameter values won't necessarily be quoted correctly, since that is\n259:         done by the database interface at execution time.\n260:         \"\"\"\n261:         sql, params = self.sql_with_params()\n262:         return sql % params\n263: \n264:     def sql_with_params(self):\n265:         \"\"\"\n266:         Return the query as an SQL string and the parameters that will be\n267:         substituted into the query.\n268:         \"\"\"\n269:         return self.get_compiler(DEFAULT_DB_ALIAS).as_sql()\n270: \n271:     def __deepcopy__(self, memo):\n272:         \"\"\"Limit the amount of work when a Query is deepcopied.\"\"\"\n273:         result = self.clone()\n274:         memo[id(self)] = result\n275:         return result\n276: \n277:     def get_compiler(self, using=None, connection=None):\n278:         if using is None and connection is None:\n279:             raise ValueError(\"Need either using or connection\")\n280:         if using:\n281:             connection = connections[using]\n282:         return connection.ops.compiler(self.compiler)(self, connection, using)\n283: \n284:     def get_meta(self):\n285:         \"\"\"\n286:         Return the Options instance (the model._meta) from which to start\n287:         processing. Normally, this is self.model._meta, but it can be changed\n288:         by subclasses.\n289:         \"\"\"\n290:         return self.model._meta\n291: \n292:     def clone(self):\n293:         \"\"\"\n294:         Return a copy of the current Query. A lightweight alternative to\n295:         to deepcopy().\n296:         \"\"\"\n297:         obj = Empty()\n298:         obj.__class__ = self.__class__\n299:         # Copy references to everything.\n300:         obj.__dict__ = self.__dict__.copy()\n301:         # Clone attributes that can't use shallow copy.\n302:         obj.alias_refcount = self.alias_refcount.copy()\n303:         obj.alias_map = self.alias_map.copy()\n304:         obj.external_aliases = self.external_aliases.copy()\n305:         obj.table_map = self.table_map.copy()\n306:         obj.where = self.where.clone()\n307:         obj.annotations = self.annotations.copy()\n308:         if self.annotation_select_mask is None:\n309:             obj.annotation_select_mask = None\n310:         else:\n311:             obj.annotation_select_mask = self.annotation_select_mask.copy()\n312:         # _annotation_select_cache cannot be copied, as doing so breaks the\n313:         # (necessary) state in which both annotations and\n314:         # _annotation_select_cache point to the same underlying objects.\n315:         # It will get re-populated in the cloned queryset the next time it's\n316:         # used.\n317:         obj._annotation_select_cache = None\n318:         obj.extra = self.extra.copy()\n319:         if self.extra_select_mask is None:\n320:             obj.extra_select_mask = None\n321:         else:\n322:             obj.extra_select_mask = self.extra_select_mask.copy()\n323:         if self._extra_select_cache is None:\n324:             obj._extra_select_cache = None\n325:         else:\n326:             obj._extra_select_cache = self._extra_select_cache.copy()\n327:         if 'subq_aliases' in self.__dict__:\n328:             obj.subq_aliases = self.subq_aliases.copy()\n329:         obj.used_aliases = self.used_aliases.copy()\n330:         obj._filtered_relations = self._filtered_relations.copy()\n331:         # Clear the cached_property\n332:         try:\n333:             del obj.base_table\n334:         except AttributeError:\n335:             pass\n336:         return obj\n337: \n338:     def chain(self, klass=None):\n339:         \"\"\"\n340:         Return a copy of the current Query that's ready for another operation.\n341:         The klass argument changes the type of the Query, e.g. UpdateQuery.\n342:         \"\"\"\n343:         obj = self.clone()\n344:         if klass and obj.__class__ != klass:\n345:             obj.__class__ = klass\n346:         if not obj.filter_is_sticky:\n347:             obj.used_aliases = set()\n348:         obj.filter_is_sticky = False\n349:         if hasattr(obj, '_setup_query'):\n350:             obj._setup_query()\n351:         return obj\n352: \n353:     def relabeled_clone(self, change_map):\n354:         clone = self.clone()\n355:         clone.change_aliases(change_map)\n356:         return clone\n357: \n358:     def rewrite_cols(self, annotation, col_cnt):\n359:         # We must make sure the inner query has the referred columns in it.\n360:         # If we are aggregating over an annotation, then Django uses Ref()\n361:         # instances to note this. However, if we are annotating over a column\n362:         # of a related model, then it might be that column isn't part of the\n363:         # SELECT clause of the inner query, and we must manually make sure\n364:         # the column is selected. An example case is:\n365:         #    .aggregate(Sum('author__awards'))\n366:         # Resolving this expression results in a join to author, but there\n367:         # is no guarantee the awards column of author is in the select clause\n368:         # of the query. Thus we must manually add the column to the inner\n369:         # query.\n370:         orig_exprs = annotation.get_source_expressions()\n371:         new_exprs = []\n372:         for expr in orig_exprs:\n373:             # FIXME: These conditions are fairly arbitrary. Identify a better\n374:             # method of having expressions decide which code path they should\n375:             # take.\n376:             if isinstance(expr, Ref):\n377:                 # Its already a Ref to subquery (see resolve_ref() for\n378:                 # details)\n379:                 new_exprs.append(expr)\n380:             elif isinstance(expr, (WhereNode, Lookup)):\n381:                 # Decompose the subexpressions further. The code here is\n382:                 # copied from the else clause, but this condition must appear\n383:                 # before the contains_aggregate/is_summary condition below.\n384:                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)\n385:                 new_exprs.append(new_expr)\n386:             else:\n387:                 # Reuse aliases of expressions already selected in subquery.\n388:                 for col_alias, selected_annotation in self.annotation_select.items():\n389:                     if selected_annotation == expr:\n390:                         new_expr = Ref(col_alias, expr)\n391:                         break\n392:                 else:\n393:                     # An expression that is not selected the subquery.\n394:                     if isinstance(expr, Col) or (expr.contains_aggregate and not expr.is_summary):\n395:                         # Reference column or another aggregate. Select it\n396:                         # under a non-conflicting alias.\n397:                         col_cnt += 1\n398:                         col_alias = '__col%d' % col_cnt\n399:                         self.annotations[col_alias] = expr\n400:                         self.append_annotation_mask([col_alias])\n401:                         new_expr = Ref(col_alias, expr)\n402:                     else:\n403:                         # Some other expression not referencing database values\n404:                         # directly. Its subexpression might contain Cols.\n405:                         new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)\n406:                 new_exprs.append(new_expr)\n407:         annotation.set_source_expressions(new_exprs)\n408:         return annotation, col_cnt\n409: \n410:     def get_aggregation(self, using, added_aggregate_names):\n411:         \"\"\"\n412:         Return the dictionary with the values of the existing aggregations.\n413:         \"\"\"\n414:         if not self.annotation_select:\n415:             return {}\n416:         existing_annotations = [\n417:             annotation for alias, annotation\n418:             in self.annotations.items()\n419:             if alias not in added_aggregate_names\n420:         ]\n421:         # Decide if we need to use a subquery.\n422:         #\n423:         # Existing annotations would cause incorrect results as get_aggregation()\n424:         # must produce just one result and thus must not use GROUP BY. But we\n425:         # aren't smart enough to remove the existing annotations from the\n426:         # query, so those would force us to use GROUP BY.\n427:         #\n428:         # If the query has limit or distinct, or uses set operations, then\n429:         # those operations must be done in a subquery so that the query\n430:         # aggregates on the limit and/or distinct results instead of applying\n431:         # the distinct and limit after the aggregation.\n432:         if (isinstance(self.group_by, tuple) or self.is_sliced or existing_annotations or\n433:                 self.distinct or self.combinator):\n434:             from django.db.models.sql.subqueries import AggregateQuery\n435:             outer_query = AggregateQuery(self.model)\n436:             inner_query = self.clone()\n437:             inner_query.select_for_update = False\n438:             inner_query.select_related = False\n439:             inner_query.set_annotation_mask(self.annotation_select)\n440:             if not self.is_sliced and not self.distinct_fields:\n441:                 # Queries with distinct_fields need ordering and when a limit\n442:                 # is applied we must take the slice from the ordered query.\n443:                 # Otherwise no need for ordering.\n444:                 inner_query.clear_ordering(True)\n445:             if not inner_query.distinct:\n446:                 # If the inner query uses default select and it has some\n447:                 # aggregate annotations, then we must make sure the inner\n448:                 # query is grouped by the main model's primary key. However,\n449:                 # clearing the select clause can alter results if distinct is\n450:                 # used.\n451:                 has_existing_aggregate_annotations = any(\n452:                     annotation for annotation in existing_annotations\n453:                     if getattr(annotation, 'contains_aggregate', True)\n454:                 )\n455:                 if inner_query.default_cols and has_existing_aggregate_annotations:\n456:                     inner_query.group_by = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)\n457:                 inner_query.default_cols = False\n458: \n459:             relabels = {t: 'subquery' for t in inner_query.alias_map}\n460:             relabels[None] = 'subquery'\n461:             # Remove any aggregates marked for reduction from the subquery\n462:             # and move them to the outer AggregateQuery.\n463:             col_cnt = 0\n464:             for alias, expression in list(inner_query.annotation_select.items()):\n465:                 annotation_select_mask = inner_query.annotation_select_mask\n466:                 if expression.is_summary:\n467:                     expression, col_cnt = inner_query.rewrite_cols(expression, col_cnt)\n468:                     outer_query.annotations[alias] = expression.relabeled_clone(relabels)\n469:                     del inner_query.annotations[alias]\n470:                     annotation_select_mask.remove(alias)\n471:                 # Make sure the annotation_select wont use cached results.\n472:                 inner_query.set_annotation_mask(inner_query.annotation_select_mask)\n473:             if inner_query.select == () and not inner_query.default_cols and not inner_query.annotation_select_mask:\n474:                 # In case of Model.objects[0:3].count(), there would be no\n475:                 # field selected in the inner query, yet we must use a subquery.\n476:                 # So, make sure at least one field is selected.\n477:                 inner_query.select = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)\n478:             try:\n479:                 outer_query.add_subquery(inner_query, using)\n480:             except EmptyResultSet:\n481:                 return {\n482:                     alias: None\n483:                     for alias in outer_query.annotation_select\n484:                 }\n485:         else:\n486:             outer_query = self\n487:             self.select = ()\n488:             self.default_cols = False\n489:             self.extra = {}\n490: \n491:         outer_query.clear_ordering(True)\n492:         outer_query.clear_limits()\n493:         outer_query.select_for_update = False\n494:         outer_query.select_related = False\n495:         compiler = outer_query.get_compiler(using)\n496:         result = compiler.execute_sql(SINGLE)\n497:         if result is None:\n498:             result = [None] * len(outer_query.annotation_select)\n499: \n500:         converters = compiler.get_converters(outer_query.annotation_select.values())\n501:         result = next(compiler.apply_converters((result,), converters))\n502: \n503:         return dict(zip(outer_query.annotation_select, result))\n504: \n505:     def get_count(self, using):\n506:         \"\"\"\n507:         Perform a COUNT() query using the current filter constraints.\n508:         \"\"\"\n509:         obj = self.clone()\n510:         obj.add_annotation(Count('*'), alias='__count', is_summary=True)\n511:         number = obj.get_aggregation(using, ['__count'])['__count']\n512:         if number is None:\n513:             number = 0\n514:         return number\n515: \n516:     def has_filters(self):\n517:         return self.where\n518: \n519:     def has_results(self, using):\n520:         q = self.clone()\n521:         if not q.distinct:\n522:             if q.group_by is True:\n523:                 q.add_fields((f.attname for f in self.model._meta.concrete_fields), False)\n524:                 q.set_group_by()\n525:             q.clear_select_clause()\n526:         q.clear_ordering(True)\n527:         q.set_limits(high=1)\n528:         compiler = q.get_compiler(using=using)\n529:         return compiler.has_results()\n530: \n531:     def explain(self, using, format=None, **options):\n532:         q = self.clone()\n533:         q.explain_query = True\n534:         q.explain_format = format\n535:         q.explain_options = options\n536:         compiler = q.get_compiler(using=using)\n537:         return '\\n'.join(compiler.explain_query())\n538: \n539:     def combine(self, rhs, connector):\n540:         \"\"\"\n541:         Merge the 'rhs' query into the current one (with any 'rhs' effects\n542:         being applied *after* (that is, \"to the right of\") anything in the\n543:         current query. 'rhs' is not modified during a call to this function.\n544: \n545:         The 'connector' parameter describes how to connect filters from the\n546:         'rhs' query.\n547:         \"\"\"\n548:         assert self.model == rhs.model, \\\n549:             \"Cannot combine queries on two different base models.\"\n550:         assert not self.is_sliced, \\\n551:             \"Cannot combine queries once a slice has been taken.\"\n552:         assert self.distinct == rhs.distinct, \\\n553:             \"Cannot combine a unique query with a non-unique query.\"\n554:         assert self.distinct_fields == rhs.distinct_fields, \\\n555:             \"Cannot combine queries with different distinct fields.\"\n556: \n557:         # Work out how to relabel the rhs aliases, if necessary.\n558:         change_map = {}\n559:         conjunction = (connector == AND)\n560: \n561:         # Determine which existing joins can be reused. When combining the\n562:         # query with AND we must recreate all joins for m2m filters. When\n563:         # combining with OR we can reuse joins. The reason is that in AND\n564:         # case a single row can't fulfill a condition like:\n565:         #     revrel__col=1 & revrel__col=2\n566:         # But, there might be two different related rows matching this\n567:         # condition. In OR case a single True is enough, so single row is\n568:         # enough, too.\n569:         #\n570:         # Note that we will be creating duplicate joins for non-m2m joins in\n571:         # the AND case. The results will be correct but this creates too many\n572:         # joins. This is something that could be fixed later on.\n573:         reuse = set() if conjunction else set(self.alias_map)\n574:         # Base table must be present in the query - this is the same\n575:         # table on both sides.\n576:         self.get_initial_alias()\n577:         joinpromoter = JoinPromoter(connector, 2, False)\n578:         joinpromoter.add_votes(\n579:             j for j in self.alias_map if self.alias_map[j].join_type == INNER)\n580:         rhs_votes = set()\n581:         # Now, add the joins from rhs query into the new query (skipping base\n582:         # table).\n583:         rhs_tables = list(rhs.alias_map)[1:]\n584:         for alias in rhs_tables:\n585:             join = rhs.alias_map[alias]\n586:             # If the left side of the join was already relabeled, use the\n587:             # updated alias.\n588:             join = join.relabeled_clone(change_map)\n589:             new_alias = self.join(join, reuse=reuse)\n590:             if join.join_type == INNER:\n591:                 rhs_votes.add(new_alias)\n592:             # We can't reuse the same join again in the query. If we have two\n593:             # distinct joins for the same connection in rhs query, then the\n594:             # combined query must have two joins, too.\n595:             reuse.discard(new_alias)\n596:             if alias != new_alias:\n597:                 change_map[alias] = new_alias\n598:             if not rhs.alias_refcount[alias]:\n599:                 # The alias was unused in the rhs query. Unref it so that it\n600:                 # will be unused in the new query, too. We have to add and\n601:                 # unref the alias so that join promotion has information of\n602:                 # the join type for the unused alias.\n603:                 self.unref_alias(new_alias)\n604:         joinpromoter.add_votes(rhs_votes)\n605:         joinpromoter.update_join_types(self)\n606: \n607:         # Now relabel a copy of the rhs where-clause and add it to the current\n608:         # one.\n609:         w = rhs.where.clone()\n610:         w.relabel_aliases(change_map)\n611:         self.where.add(w, connector)\n612: \n613:         # Selection columns and extra extensions are those provided by 'rhs'.\n614:         if rhs.select:\n615:             self.set_select([col.relabeled_clone(change_map) for col in rhs.select])\n616:         else:\n617:             self.select = ()\n618: \n619:         if connector == OR:\n620:             # It would be nice to be able to handle this, but the queries don't\n621:             # really make sense (or return consistent value sets). Not worth\n622:             # the extra complexity when you can write a real query instead.\n623:             if self.extra and rhs.extra:\n624:                 raise ValueError(\"When merging querysets using 'or', you cannot have extra(select=...) on both sides.\")\n625:         self.extra.update(rhs.extra)\n626:         extra_select_mask = set()\n627:         if self.extra_select_mask is not None:\n628:             extra_select_mask.update(self.extra_select_mask)\n629:         if rhs.extra_select_mask is not None:\n630:             extra_select_mask.update(rhs.extra_select_mask)\n631:         if extra_select_mask:\n632:             self.set_extra_mask(extra_select_mask)\n633:         self.extra_tables += rhs.extra_tables\n634: \n635:         # Ordering uses the 'rhs' ordering, unless it has none, in which case\n636:         # the current ordering is used.\n637:         self.order_by = rhs.order_by or self.order_by\n638:         self.extra_order_by = rhs.extra_order_by or self.extra_order_by\n639: \n640:     def deferred_to_data(self, target, callback):\n641:         \"\"\"\n642:         Convert the self.deferred_loading data structure to an alternate data\n643:         structure, describing the field that *will* be loaded. This is used to\n644:         compute the columns to select from the database and also by the\n645:         QuerySet class to work out which fields are being initialized on each\n646:         model. Models that have all their fields included aren't mentioned in\n647:         the result, only those that have field restrictions in place.\n648: \n649:         The \"target\" parameter is the instance that is populated (in place).\n650:         The \"callback\" is a function that is called whenever a (model, field)\n651:         pair need to be added to \"target\". It accepts three parameters:\n652:         \"target\", and the model and list of fields being added for that model.\n653:         \"\"\"\n654:         field_names, defer = self.deferred_loading\n655:         if not field_names:\n656:             return\n657:         orig_opts = self.get_meta()\n658:         seen = {}\n659:         must_include = {orig_opts.concrete_model: {orig_opts.pk}}\n660:         for field_name in field_names:\n661:             parts = field_name.split(LOOKUP_SEP)\n662:             cur_model = self.model._meta.concrete_model\n663:             opts = orig_opts\n664:             for name in parts[:-1]:\n665:                 old_model = cur_model\n666:                 if name in self._filtered_relations:\n667:                     name = self._filtered_relations[name].relation_name\n668:                 source = opts.get_field(name)\n669:                 if is_reverse_o2o(source):\n670:                     cur_model = source.related_model\n671:                 else:\n672:                     cur_model = source.remote_field.model\n673:                 opts = cur_model._meta\n674:                 # Even if we're \"just passing through\" this model, we must add\n675:                 # both the current model's pk and the related reference field\n676:                 # (if it's not a reverse relation) to the things we select.\n677:                 if not is_reverse_o2o(source):\n678:                     must_include[old_model].add(source)\n679:                 add_to_dict(must_include, cur_model, opts.pk)\n680:             field = opts.get_field(parts[-1])\n681:             is_reverse_object = field.auto_created and not field.concrete\n682:             model = field.related_model if is_reverse_object else field.model\n683:             model = model._meta.concrete_model\n684:             if model == opts.model:\n685:                 model = cur_model\n686:             if not is_reverse_o2o(field):\n687:                 add_to_dict(seen, model, field)\n688: \n689:         if defer:\n690:             # We need to load all fields for each model, except those that\n691:             # appear in \"seen\" (for all models that appear in \"seen\"). The only\n692:             # slight complexity here is handling fields that exist on parent\n693:             # models.\n694:             workset = {}\n695:             for model, values in seen.items():\n696:                 for field in model._meta.local_fields:\n697:                     if field not in values:\n698:                         m = field.model._meta.concrete_model\n699:                         add_to_dict(workset, m, field)\n700:             for model, values in must_include.items():\n701:                 # If we haven't included a model in workset, we don't add the\n702:                 # corresponding must_include fields for that model, since an\n703:                 # empty set means \"include all fields\". That's why there's no\n704:                 # \"else\" branch here.\n705:                 if model in workset:\n706:                     workset[model].update(values)\n707:             for model, values in workset.items():\n708:                 callback(target, model, values)\n709:         else:\n710:             for model, values in must_include.items():\n711:                 if model in seen:\n712:                     seen[model].update(values)\n713:                 else:\n714:                     # As we've passed through this model, but not explicitly\n715:                     # included any fields, we have to make sure it's mentioned\n716:                     # so that only the \"must include\" fields are pulled in.\n717:                     seen[model] = values\n718:             # Now ensure that every model in the inheritance chain is mentioned\n719:             # in the parent list. Again, it must be mentioned to ensure that\n720:             # only \"must include\" fields are pulled in.\n721:             for model in orig_opts.get_parent_list():\n722:                 seen.setdefault(model, set())\n723:             for model, values in seen.items():\n724:                 callback(target, model, values)\n725: \n726:     def table_alias(self, table_name, create=False, filtered_relation=None):\n727:         \"\"\"\n728:         Return a table alias for the given table_name and whether this is a\n729:         new alias or not.\n730: \n731:         If 'create' is true, a new alias is always created. Otherwise, the\n732:         most recently created alias for the table (if one exists) is reused.\n733:         \"\"\"\n734:         alias_list = self.table_map.get(table_name)\n735:         if not create and alias_list:\n736:             alias = alias_list[0]\n737:             self.alias_refcount[alias] += 1\n738:             return alias, False\n739: \n740:         # Create a new alias for this table.\n741:         if alias_list:\n742:             alias = '%s%d' % (self.alias_prefix, len(self.alias_map) + 1)\n743:             alias_list.append(alias)\n744:         else:\n745:             # The first occurrence of a table uses the table name directly.\n746:             alias = filtered_relation.alias if filtered_relation is not None else table_name\n747:             self.table_map[table_name] = [alias]\n748:         self.alias_refcount[alias] = 1\n749:         return alias, True\n750: \n751:     def ref_alias(self, alias):\n752:         \"\"\"Increases the reference count for this alias.\"\"\"\n753:         self.alias_refcount[alias] += 1\n754: \n755:     def unref_alias(self, alias, amount=1):\n756:         \"\"\"Decreases the reference count for this alias.\"\"\"\n757:         self.alias_refcount[alias] -= amount\n758: \n759:     def promote_joins(self, aliases):\n760:         \"\"\"\n761:         Promote recursively the join type of given aliases and its children to\n762:         an outer join. If 'unconditional' is False, only promote the join if\n763:         it is nullable or the parent join is an outer join.\n764: \n765:         The children promotion is done to avoid join chains that contain a LOUTER\n766:         b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,\n767:         then we must also promote b->c automatically, or otherwise the promotion\n768:         of a->b doesn't actually change anything in the query results.\n769:         \"\"\"\n770:         aliases = list(aliases)\n771:         while aliases:\n772:             alias = aliases.pop(0)\n773:             if self.alias_map[alias].join_type is None:\n774:                 # This is the base table (first FROM entry) - this table\n775:                 # isn't really joined at all in the query, so we should not\n776:                 # alter its join type.\n777:                 continue\n778:             # Only the first alias (skipped above) should have None join_type\n779:             assert self.alias_map[alias].join_type is not None\n780:             parent_alias = self.alias_map[alias].parent_alias\n781:             parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER\n782:             already_louter = self.alias_map[alias].join_type == LOUTER\n783:             if ((self.alias_map[alias].nullable or parent_louter) and\n784:                     not already_louter):\n785:                 self.alias_map[alias] = self.alias_map[alias].promote()\n786:                 # Join type of 'alias' changed, so re-examine all aliases that\n787:                 # refer to this one.\n788:                 aliases.extend(\n789:                     join for join in self.alias_map\n790:                     if self.alias_map[join].parent_alias == alias and join not in aliases\n791:                 )\n792: \n793:     def demote_joins(self, aliases):\n794:         \"\"\"\n795:         Change join type from LOUTER to INNER for all joins in aliases.\n796: \n797:         Similarly to promote_joins(), this method must ensure no join chains\n798:         containing first an outer, then an inner join are generated. If we\n799:         are demoting b->c join in chain a LOUTER b LOUTER c then we must\n800:         demote a->b automatically, or otherwise the demotion of b->c doesn't\n801:         actually change anything in the query results. .\n802:         \"\"\"\n803:         aliases = list(aliases)\n804:         while aliases:\n805:             alias = aliases.pop(0)\n806:             if self.alias_map[alias].join_type == LOUTER:\n807:                 self.alias_map[alias] = self.alias_map[alias].demote()\n808:                 parent_alias = self.alias_map[alias].parent_alias\n809:                 if self.alias_map[parent_alias].join_type == INNER:\n810:                     aliases.append(parent_alias)\n811: \n812:     def reset_refcounts(self, to_counts):\n813:         \"\"\"\n814:         Reset reference counts for aliases so that they match the value passed\n815:         in `to_counts`.\n816:         \"\"\"\n817:         for alias, cur_refcount in self.alias_refcount.copy().items():\n818:             unref_amount = cur_refcount - to_counts.get(alias, 0)\n819:             self.unref_alias(alias, unref_amount)\n820: \n821:     def change_aliases(self, change_map):\n822:         \"\"\"\n823:         Change the aliases in change_map (which maps old-alias -> new-alias),\n824:         relabelling any references to them in select columns and the where\n825:         clause.\n826:         \"\"\"\n827:         assert set(change_map).isdisjoint(change_map.values())\n828: \n829:         # 1. Update references in \"select\" (normal columns plus aliases),\n830:         # \"group by\" and \"where\".\n831:         self.where.relabel_aliases(change_map)\n832:         if isinstance(self.group_by, tuple):\n833:             self.group_by = tuple([col.relabeled_clone(change_map) for col in self.group_by])\n834:         self.select = tuple([col.relabeled_clone(change_map) for col in self.select])\n835:         self.annotations = self.annotations and {\n836:             key: col.relabeled_clone(change_map) for key, col in self.annotations.items()\n837:         }\n838: \n839:         # 2. Rename the alias in the internal table/alias datastructures.\n840:         for old_alias, new_alias in change_map.items():\n841:             if old_alias not in self.alias_map:\n842:                 continue\n843:             alias_data = self.alias_map[old_alias].relabeled_clone(change_map)\n844:             self.alias_map[new_alias] = alias_data\n845:             self.alias_refcount[new_alias] = self.alias_refcount[old_alias]\n846:             del self.alias_refcount[old_alias]\n847:             del self.alias_map[old_alias]\n848: \n849:             table_aliases = self.table_map[alias_data.table_name]\n850:             for pos, alias in enumerate(table_aliases):\n851:                 if alias == old_alias:\n852:                     table_aliases[pos] = new_alias\n853:                     break\n854:         self.external_aliases = {change_map.get(alias, alias)\n855:                                  for alias in self.external_aliases}\n856: \n857:     def bump_prefix(self, outer_query):\n858:         \"\"\"\n859:         Change the alias prefix to the next letter in the alphabet in a way\n860:         that the outer query's aliases and this query's aliases will not\n861:         conflict. Even tables that previously had no alias will get an alias\n862:         after this call.\n863:         \"\"\"\n864:         def prefix_gen():\n865:             \"\"\"\n866:             Generate a sequence of characters in alphabetical order:\n867:                 -> 'A', 'B', 'C', ...\n868: \n869:             When the alphabet is finished, the sequence will continue with the\n870:             Cartesian product:\n871:                 -> 'AA', 'AB', 'AC', ...\n872:             \"\"\"\n873:             alphabet = ascii_uppercase\n874:             prefix = chr(ord(self.alias_prefix) + 1)\n875:             yield prefix\n876:             for n in count(1):\n877:                 seq = alphabet[alphabet.index(prefix):] if prefix else alphabet\n878:                 for s in product(seq, repeat=n):\n879:                     yield ''.join(s)\n880:                 prefix = None\n881: \n882:         if self.alias_prefix != outer_query.alias_prefix:\n883:             # No clashes between self and outer query should be possible.\n884:             return\n885: \n886:         # Explicitly avoid infinite loop. The constant divider is based on how\n887:         # much depth recursive subquery references add to the stack. This value\n888:         # might need to be adjusted when adding or removing function calls from\n889:         # the code path in charge of performing these operations.\n890:         local_recursion_limit = sys.getrecursionlimit() // 16\n891:         for pos, prefix in enumerate(prefix_gen()):\n892:             if prefix not in self.subq_aliases:\n893:                 self.alias_prefix = prefix\n894:                 break\n895:             if pos > local_recursion_limit:\n896:                 raise RecursionError(\n897:                     'Maximum recursion depth exceeded: too many subqueries.'\n898:                 )\n899:         self.subq_aliases = self.subq_aliases.union([self.alias_prefix])\n900:         outer_query.subq_aliases = outer_query.subq_aliases.union(self.subq_aliases)\n901:         self.change_aliases({\n902:             alias: '%s%d' % (self.alias_prefix, pos)\n903:             for pos, alias in enumerate(self.alias_map)\n904:         })\n905: \n906:     def get_initial_alias(self):\n907:         \"\"\"\n908:         Return the first alias for this query, after increasing its reference\n909:         count.\n910:         \"\"\"\n911:         if self.alias_map:\n912:             alias = self.base_table\n913:             self.ref_alias(alias)\n914:         else:\n915:             alias = self.join(BaseTable(self.get_meta().db_table, None))\n916:         return alias\n917: \n918:     def count_active_tables(self):\n919:         \"\"\"\n920:         Return the number of tables in this query with a non-zero reference\n921:         count. After execution, the reference counts are zeroed, so tables\n922:         added in compiler will not be seen by this method.\n923:         \"\"\"\n924:         return len([1 for count in self.alias_refcount.values() if count])\n925: \n926:     def join(self, join, reuse=None, reuse_with_filtered_relation=False):\n927:         \"\"\"\n928:         Return an alias for the 'join', either reusing an existing alias for\n929:         that join or creating a new one. 'join' is either a\n930:         sql.datastructures.BaseTable or Join.\n931: \n932:         The 'reuse' parameter can be either None which means all joins are\n933:         reusable, or it can be a set containing the aliases that can be reused.\n934: \n935:         The 'reuse_with_filtered_relation' parameter is used when computing\n936:         FilteredRelation instances.\n937: \n938:         A join is always created as LOUTER if the lhs alias is LOUTER to make\n939:         sure chains like t1 LOUTER t2 INNER t3 aren't generated. All new\n940:         joins are created as LOUTER if the join is nullable.\n941:         \"\"\"\n942:         if reuse_with_filtered_relation and reuse:\n943:             reuse_aliases = [\n944:                 a for a, j in self.alias_map.items()\n945:                 if a in reuse and j.equals(join, with_filtered_relation=False)\n946:             ]\n947:         else:\n948:             reuse_aliases = [\n949:                 a for a, j in self.alias_map.items()\n950:                 if (reuse is None or a in reuse) and j == join\n951:             ]\n952:         if reuse_aliases:\n953:             if join.table_alias in reuse_aliases:\n954:                 reuse_alias = join.table_alias\n955:             else:\n956:                 # Reuse the most recent alias of the joined table\n957:                 # (a many-to-many relation may be joined multiple times).\n958:                 reuse_alias = reuse_aliases[-1]\n959:             self.ref_alias(reuse_alias)\n960:             return reuse_alias\n961: \n962:         # No reuse is possible, so we need a new alias.\n963:         alias, _ = self.table_alias(join.table_name, create=True, filtered_relation=join.filtered_relation)\n964:         if join.join_type:\n965:             if self.alias_map[join.parent_alias].join_type == LOUTER or join.nullable:\n966:                 join_type = LOUTER\n967:             else:\n968:                 join_type = INNER\n969:             join.join_type = join_type\n970:         join.table_alias = alias\n971:         self.alias_map[alias] = join\n972:         return alias\n973: \n974:     def join_parent_model(self, opts, model, alias, seen):\n975:         \"\"\"\n976:         Make sure the given 'model' is joined in the query. If 'model' isn't\n977:         a parent of 'opts' or if it is None this method is a no-op.\n978: \n979:         The 'alias' is the root alias for starting the join, 'seen' is a dict\n980:         of model -> alias of existing joins. It must also contain a mapping\n981:         of None -> some alias. This will be returned in the no-op case.\n982:         \"\"\"\n983:         if model in seen:\n984:             return seen[model]\n985:         chain = opts.get_base_chain(model)\n986:         if not chain:\n987:             return alias\n988:         curr_opts = opts\n989:         for int_model in chain:\n990:             if int_model in seen:\n991:                 curr_opts = int_model._meta\n992:                 alias = seen[int_model]\n993:                 continue\n994:             # Proxy model have elements in base chain\n995:             # with no parents, assign the new options\n996:             # object and skip to the next base in that\n997:             # case\n998:             if not curr_opts.parents[int_model]:\n999:                 curr_opts = int_model._meta\n1000:                 continue\n1001:             link_field = curr_opts.get_ancestor_link(int_model)\n1002:             join_info = self.setup_joins([link_field.name], curr_opts, alias)\n1003:             curr_opts = int_model._meta\n1004:             alias = seen[int_model] = join_info.joins[-1]\n1005:         return alias or seen[None]\n1006: \n1007:     def add_annotation(self, annotation, alias, is_summary=False):\n1008:         \"\"\"Add a single annotation expression to the Query.\"\"\"\n1009:         annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,\n1010:                                                    summarize=is_summary)\n1011:         self.append_annotation_mask([alias])\n1012:         self.annotations[alias] = annotation\n1013: \n1014:     def resolve_expression(self, query, *args, **kwargs):\n1015:         clone = self.clone()\n1016:         # Subqueries need to use a different set of aliases than the outer query.\n1017:         clone.bump_prefix(query)\n1018:         clone.subquery = True\n1019:         # It's safe to drop ordering if the queryset isn't using slicing,\n1020:         # distinct(*fields) or select_for_update().\n1021:         if (self.low_mark == 0 and self.high_mark is None and\n1022:                 not self.distinct_fields and\n1023:                 not self.select_for_update):\n1024:             clone.clear_ordering(True)\n1025:         clone.where.resolve_expression(query, *args, **kwargs)\n1026:         for key, value in clone.annotations.items():\n1027:             resolved = value.resolve_expression(query, *args, **kwargs)\n1028:             if hasattr(resolved, 'external_aliases'):\n1029:                 resolved.external_aliases.update(clone.alias_map)\n1030:             clone.annotations[key] = resolved\n1031:         # Outer query's aliases are considered external.\n1032:         clone.external_aliases.update(\n1033:             alias for alias, table in query.alias_map.items()\n1034:             if (\n1035:                 isinstance(table, Join) and table.join_field.related_model._meta.db_table != alias\n1036:             ) or (\n1037:                 isinstance(table, BaseTable) and table.table_name != table.table_alias\n1038:             )\n1039:         )\n1040:         return clone\n1041: \n1042:     def as_sql(self, compiler, connection):\n1043:         sql, params = self.get_compiler(connection=connection).as_sql()\n1044:         if self.subquery:\n1045:             sql = '(%s)' % sql\n1046:         return sql, params\n1047: \n1048:     def resolve_lookup_value(self, value, can_reuse, allow_joins, simple_col):\n1049:         if hasattr(value, 'resolve_expression'):\n1050:             kwargs = {'reuse': can_reuse, 'allow_joins': allow_joins}\n1051:             if isinstance(value, F):\n1052:                 kwargs['simple_col'] = simple_col\n1053:             value = value.resolve_expression(self, **kwargs)\n1054:         elif isinstance(value, (list, tuple)):\n1055:             # The items of the iterable may be expressions and therefore need\n1056:             # to be resolved independently.\n1057:             resolved_values = []\n1058:             for sub_value in value:\n1059:                 if hasattr(sub_value, 'resolve_expression'):\n1060:                     if isinstance(sub_value, F):\n1061:                         resolved_values.append(sub_value.resolve_expression(\n1062:                             self, reuse=can_reuse, allow_joins=allow_joins,\n1063:                             simple_col=simple_col,\n1064:                         ))\n1065:                     else:\n1066:                         resolved_values.append(sub_value.resolve_expression(\n1067:                             self, reuse=can_reuse, allow_joins=allow_joins,\n1068:                         ))\n1069:                 else:\n1070:                     resolved_values.append(sub_value)\n1071:             value = tuple(resolved_values)\n1072:         return value\n1073: \n1074:     def solve_lookup_type(self, lookup):\n1075:         \"\"\"\n1076:         Solve the lookup type from the lookup (e.g.: 'foobar__id__icontains').\n1077:         \"\"\"\n1078:         lookup_splitted = lookup.split(LOOKUP_SEP)\n1079:         if self.annotations:\n1080:             expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)\n1081:             if expression:\n1082:                 return expression_lookups, (), expression\n1083:         _, field, _, lookup_parts = self.names_to_path(lookup_splitted, self.get_meta())\n1084:         field_parts = lookup_splitted[0:len(lookup_splitted) - len(lookup_parts)]\n1085:         if len(lookup_parts) > 1 and not field_parts:\n1086:             raise FieldError(\n1087:                 'Invalid lookup \"%s\" for model %s\".' %\n1088:                 (lookup, self.get_meta().model.__name__)\n1089:             )\n1090:         return lookup_parts, field_parts, False\n1091: \n1092:     def check_query_object_type(self, value, opts, field):\n1093:         \"\"\"\n1094:         Check whether the object passed while querying is of the correct type.\n1095:         If not, raise a ValueError specifying the wrong object.\n1096:         \"\"\"\n1097:         if hasattr(value, '_meta'):\n1098:             if not check_rel_lookup_compatibility(value._meta.model, opts, field):\n1099:                 raise ValueError(\n1100:                     'Cannot query \"%s\": Must be \"%s\" instance.' %\n1101:                     (value, opts.object_name))\n1102: \n1103:     def check_related_objects(self, field, value, opts):\n1104:         \"\"\"Check the type of object passed to query relations.\"\"\"\n1105:         if field.is_relation:\n1106:             # Check that the field and the queryset use the same model in a\n1107:             # query like .filter(author=Author.objects.all()). For example, the\n1108:             # opts would be Author's (from the author field) and value.model\n1109:             # would be Author.objects.all() queryset's .model (Author also).\n1110:             # The field is the related field on the lhs side.\n1111:             if (isinstance(value, Query) and not value.has_select_fields and\n1112:                     not check_rel_lookup_compatibility(value.model, opts, field)):\n1113:                 raise ValueError(\n1114:                     'Cannot use QuerySet for \"%s\": Use a QuerySet for \"%s\".' %\n1115:                     (value.model._meta.object_name, opts.object_name)\n1116:                 )\n1117:             elif hasattr(value, '_meta'):\n1118:                 self.check_query_object_type(value, opts, field)\n1119:             elif hasattr(value, '__iter__'):\n1120:                 for v in value:\n1121:                     self.check_query_object_type(v, opts, field)\n1122: \n1123:     def check_filterable(self, expression):\n1124:         \"\"\"Raise an error if expression cannot be used in a WHERE clause.\"\"\"\n1125:         if not getattr(expression, 'filterable', 'True'):\n1126:             raise NotSupportedError(\n1127:                 expression.__class__.__name__ + ' is disallowed in the filter '\n1128:                 'clause.'\n1129:             )\n1130:         if hasattr(expression, 'get_source_expressions'):\n1131:             for expr in expression.get_source_expressions():\n1132:                 self.check_filterable(expr)\n1133: \n1134:     def build_lookup(self, lookups, lhs, rhs):\n1135:         \"\"\"\n1136:         Try to extract transforms and lookup from given lhs.\n1137: \n1138:         The lhs value is something that works like SQLExpression.\n1139:         The rhs value is what the lookup is going to compare against.\n1140:         The lookups is a list of names to extract using get_lookup()\n1141:         and get_transform().\n1142:         \"\"\"\n1143:         # __exact is the default lookup if one isn't given.\n1144:         *transforms, lookup_name = lookups or ['exact']\n1145:         for name in transforms:\n1146:             lhs = self.try_transform(lhs, name)\n1147:         # First try get_lookup() so that the lookup takes precedence if the lhs\n1148:         # supports both transform and lookup for the name.\n1149:         lookup_class = lhs.get_lookup(lookup_name)\n1150:         if not lookup_class:\n1151:             if lhs.field.is_relation:\n1152:                 raise FieldError('Related Field got invalid lookup: {}'.format(lookup_name))\n1153:             # A lookup wasn't found. Try to interpret the name as a transform\n1154:             # and do an Exact lookup against it.\n1155:             lhs = self.try_transform(lhs, lookup_name)\n1156:             lookup_name = 'exact'\n1157:             lookup_class = lhs.get_lookup(lookup_name)\n1158:             if not lookup_class:\n1159:                 return\n1160: \n1161:         lookup = lookup_class(lhs, rhs)\n1162:         # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all\n1163:         # uses of None as a query value unless the lookup supports it.\n1164:         if lookup.rhs is None and not lookup.can_use_none_as_rhs:\n1165:             if lookup_name not in ('exact', 'iexact'):\n1166:                 raise ValueError(\"Cannot use None as a query value\")\n1167:             return lhs.get_lookup('isnull')(lhs, True)\n1168: \n1169:         # For Oracle '' is equivalent to null. The check must be done at this\n1170:         # stage because join promotion can't be done in the compiler. Using\n1171:         # DEFAULT_DB_ALIAS isn't nice but it's the best that can be done here.\n1172:         # A similar thing is done in is_nullable(), too.\n1173:         if (connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and\n1174:                 lookup_name == 'exact' and lookup.rhs == ''):\n1175:             return lhs.get_lookup('isnull')(lhs, True)\n1176: \n1177:         return lookup\n1178: \n1179:     def try_transform(self, lhs, name):\n1180:         \"\"\"\n1181:         Helper method for build_lookup(). Try to fetch and initialize\n1182:         a transform for name parameter from lhs.\n1183:         \"\"\"\n1184:         transform_class = lhs.get_transform(name)\n1185:         if transform_class:\n1186:             return transform_class(lhs)\n1187:         else:\n1188:             output_field = lhs.output_field.__class__\n1189:             suggested_lookups = difflib.get_close_matches(name, output_field.get_lookups())\n1190:             if suggested_lookups:\n1191:                 suggestion = ', perhaps you meant %s?' % ' or '.join(suggested_lookups)\n1192:             else:\n1193:                 suggestion = '.'\n1194:             raise FieldError(\n1195:                 \"Unsupported lookup '%s' for %s or join on the field not \"\n1196:                 \"permitted%s\" % (name, output_field.__name__, suggestion)\n1197:             )\n1198: \n1199:     def build_filter(self, filter_expr, branch_negated=False, current_negated=False,\n1200:                      can_reuse=None, allow_joins=True, split_subq=True,\n1201:                      reuse_with_filtered_relation=False, simple_col=False):\n1202:         \"\"\"\n1203:         Build a WhereNode for a single filter clause but don't add it\n1204:         to this Query. Query.add_q() will then add this filter to the where\n1205:         Node.\n1206: \n1207:         The 'branch_negated' tells us if the current branch contains any\n1208:         negations. This will be used to determine if subqueries are needed.\n1209: \n1210:         The 'current_negated' is used to determine if the current filter is\n1211:         negated or not and this will be used to determine if IS NULL filtering\n1212:         is needed.\n1213: \n1214:         The difference between current_negated and branch_negated is that\n1215:         branch_negated is set on first negation, but current_negated is\n1216:         flipped for each negation.\n1217: \n1218:         Note that add_filter will not do any negating itself, that is done\n1219:         upper in the code by add_q().\n1220: \n1221:         The 'can_reuse' is a set of reusable joins for multijoins.\n1222: \n1223:         If 'reuse_with_filtered_relation' is True, then only joins in can_reuse\n1224:         will be reused.\n1225: \n1226:         The method will create a filter clause that can be added to the current\n1227:         query. However, if the filter isn't added to the query then the caller\n1228:         is responsible for unreffing the joins used.\n1229:         \"\"\"\n1230:         if isinstance(filter_expr, dict):\n1231:             raise FieldError(\"Cannot parse keyword query as dict\")\n1232:         if hasattr(filter_expr, 'resolve_expression') and getattr(filter_expr, 'conditional', False):\n1233:             if connections[DEFAULT_DB_ALIAS].ops.conditional_expression_supported_in_where_clause(filter_expr):\n1234:                 condition = filter_expr.resolve_expression(self)\n1235:             else:\n1236:                 # Expression is not supported in the WHERE clause, add\n1237:                 # comparison with True.\n1238:                 condition = self.build_lookup(['exact'], filter_expr.resolve_expression(self), True)\n1239:             clause = self.where_class()\n1240:             clause.add(condition, AND)\n1241:             return clause, []\n1242:         arg, value = filter_expr\n1243:         if not arg:\n1244:             raise FieldError(\"Cannot parse keyword query %r\" % arg)\n1245:         lookups, parts, reffed_expression = self.solve_lookup_type(arg)\n1246: \n1247:         self.check_filterable(reffed_expression)\n1248: \n1249:         if not allow_joins and len(parts) > 1:\n1250:             raise FieldError(\"Joined field references are not permitted in this query\")\n1251: \n1252:         pre_joins = self.alias_refcount.copy()\n1253:         value = self.resolve_lookup_value(value, can_reuse, allow_joins, simple_col)\n1254:         used_joins = {k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)}\n1255: \n1256:         self.check_filterable(value)\n1257: \n1258:         clause = self.where_class()\n1259:         if reffed_expression:\n1260:             condition = self.build_lookup(lookups, reffed_expression, value)\n1261:             clause.add(condition, AND)\n1262:             return clause, []\n1263: \n1264:         opts = self.get_meta()\n1265:         alias = self.get_initial_alias()\n1266:         allow_many = not branch_negated or not split_subq\n1267: \n1268:         try:\n1269:             join_info = self.setup_joins(\n1270:                 parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many,\n1271:                 reuse_with_filtered_relation=reuse_with_filtered_relation,\n1272:             )\n1273: \n1274:             # Prevent iterator from being consumed by check_related_objects()\n1275:             if isinstance(value, Iterator):\n1276:                 value = list(value)\n1277:             self.check_related_objects(join_info.final_field, value, join_info.opts)\n1278: \n1279:             # split_exclude() needs to know which joins were generated for the\n1280:             # lookup parts\n1281:             self._lookup_joins = join_info.joins\n1282:         except MultiJoin as e:\n1283:             return self.split_exclude(filter_expr, can_reuse, e.names_with_path)\n1284: \n1285:         # Update used_joins before trimming since they are reused to determine\n1286:         # which joins could be later promoted to INNER.\n1287:         used_joins.update(join_info.joins)\n1288:         targets, alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)\n1289:         if can_reuse is not None:\n1290:             can_reuse.update(join_list)\n1291: \n1292:         if join_info.final_field.is_relation:\n1293:             # No support for transforms for relational fields\n1294:             num_lookups = len(lookups)\n1295:             if num_lookups > 1:\n1296:                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))\n1297:             if len(targets) == 1:\n1298:                 col = _get_col(targets[0], join_info.final_field, alias, simple_col)\n1299:             else:\n1300:                 col = MultiColSource(alias, targets, join_info.targets, join_info.final_field)\n1301:         else:\n1302:             col = _get_col(targets[0], join_info.final_field, alias, simple_col)\n1303: \n1304:         condition = self.build_lookup(lookups, col, value)\n1305:         lookup_type = condition.lookup_name\n1306:         clause.add(condition, AND)\n1307: \n1308:         require_outer = lookup_type == 'isnull' and condition.rhs is True and not current_negated\n1309:         if current_negated and (lookup_type != 'isnull' or condition.rhs is False) and condition.rhs is not None:\n1310:             require_outer = True\n1311:             if (lookup_type != 'isnull' and (\n1312:                     self.is_nullable(targets[0]) or\n1313:                     self.alias_map[join_list[-1]].join_type == LOUTER)):\n1314:                 # The condition added here will be SQL like this:\n1315:                 # NOT (col IS NOT NULL), where the first NOT is added in\n1316:                 # upper layers of code. The reason for addition is that if col\n1317:                 # is null, then col != someval will result in SQL \"unknown\"\n1318:                 # which isn't the same as in Python. The Python None handling\n1319:                 # is wanted, and it can be gotten by\n1320:                 # (col IS NULL OR col != someval)\n1321:                 #   <=>\n1322:                 # NOT (col IS NOT NULL AND col = someval).\n1323:                 lookup_class = targets[0].get_lookup('isnull')\n1324:                 col = _get_col(targets[0], join_info.targets[0], alias, simple_col)\n1325:                 clause.add(lookup_class(col, False), AND)\n1326:         return clause, used_joins if not require_outer else ()\n1327: \n1328:     def add_filter(self, filter_clause):\n1329:         self.add_q(Q(**{filter_clause[0]: filter_clause[1]}))\n1330: \n1331:     def add_q(self, q_object):\n1332:         \"\"\"\n1333:         A preprocessor for the internal _add_q(). Responsible for doing final\n1334:         join promotion.\n1335:         \"\"\"\n1336:         # For join promotion this case is doing an AND for the added q_object\n1337:         # and existing conditions. So, any existing inner join forces the join\n1338:         # type to remain inner. Existing outer joins can however be demoted.\n1339:         # (Consider case where rel_a is LOUTER and rel_a__col=1 is added - if\n1340:         # rel_a doesn't produce any rows, then the whole condition must fail.\n1341:         # So, demotion is OK.\n1342:         existing_inner = {a for a in self.alias_map if self.alias_map[a].join_type == INNER}\n1343:         clause, _ = self._add_q(q_object, self.used_aliases)\n1344:         if clause:\n1345:             self.where.add(clause, AND)\n1346:         self.demote_joins(existing_inner)\n1347: \n1348:     def build_where(self, q_object):\n1349:         return self._add_q(q_object, used_aliases=set(), allow_joins=False, simple_col=True)[0]\n1350: \n1351:     def _add_q(self, q_object, used_aliases, branch_negated=False,\n1352:                current_negated=False, allow_joins=True, split_subq=True,\n1353:                simple_col=False):\n1354:         \"\"\"Add a Q-object to the current filter.\"\"\"\n1355:         connector = q_object.connector\n1356:         current_negated = current_negated ^ q_object.negated\n1357:         branch_negated = branch_negated or q_object.negated\n1358:         target_clause = self.where_class(connector=connector,\n1359:                                          negated=q_object.negated)\n1360:         joinpromoter = JoinPromoter(q_object.connector, len(q_object.children), current_negated)\n1361:         for child in q_object.children:\n1362:             if isinstance(child, Node):\n1363:                 child_clause, needed_inner = self._add_q(\n1364:                     child, used_aliases, branch_negated,\n1365:                     current_negated, allow_joins, split_subq, simple_col)\n1366:                 joinpromoter.add_votes(needed_inner)\n1367:             else:\n1368:                 child_clause, needed_inner = self.build_filter(\n1369:                     child, can_reuse=used_aliases, branch_negated=branch_negated,\n1370:                     current_negated=current_negated, allow_joins=allow_joins,\n1371:                     split_subq=split_subq, simple_col=simple_col,\n1372:                 )\n1373:                 joinpromoter.add_votes(needed_inner)\n1374:             if child_clause:\n1375:                 target_clause.add(child_clause, connector)\n1376:         needed_inner = joinpromoter.update_join_types(self)\n1377:         return target_clause, needed_inner\n1378: \n1379:     def build_filtered_relation_q(self, q_object, reuse, branch_negated=False, current_negated=False):\n1380:         \"\"\"Add a FilteredRelation object to the current filter.\"\"\"\n1381:         connector = q_object.connector\n1382:         current_negated ^= q_object.negated\n1383:         branch_negated = branch_negated or q_object.negated\n1384:         target_clause = self.where_class(connector=connector, negated=q_object.negated)\n1385:         for child in q_object.children:\n1386:             if isinstance(child, Node):\n1387:                 child_clause = self.build_filtered_relation_q(\n1388:                     child, reuse=reuse, branch_negated=branch_negated,\n1389:                     current_negated=current_negated,\n1390:                 )\n1391:             else:\n1392:                 child_clause, _ = self.build_filter(\n1393:                     child, can_reuse=reuse, branch_negated=branch_negated,\n1394:                     current_negated=current_negated,\n1395:                     allow_joins=True, split_subq=False,\n1396:                     reuse_with_filtered_relation=True,\n1397:                 )\n1398:             target_clause.add(child_clause, connector)\n1399:         return target_clause\n1400: \n1401:     def add_filtered_relation(self, filtered_relation, alias):\n1402:         filtered_relation.alias = alias\n1403:         lookups = dict(get_children_from_q(filtered_relation.condition))\n1404:         for lookup in chain((filtered_relation.relation_name,), lookups):\n1405:             lookup_parts, field_parts, _ = self.solve_lookup_type(lookup)\n1406:             shift = 2 if not lookup_parts else 1\n1407:             if len(field_parts) > (shift + len(lookup_parts)):\n1408:                 raise ValueError(\n1409:                     \"FilteredRelation's condition doesn't support nested \"\n1410:                     \"relations (got %r).\" % lookup\n1411:                 )\n1412:         self._filtered_relations[filtered_relation.alias] = filtered_relation\n1413: \n1414:     def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):\n1415:         \"\"\"\n1416:         Walk the list of names and turns them into PathInfo tuples. A single\n1417:         name in 'names' can generate multiple PathInfos (m2m, for example).\n1418: \n1419:         'names' is the path of names to travel, 'opts' is the model Options we\n1420:         start the name resolving from, 'allow_many' is as for setup_joins().\n1421:         If fail_on_missing is set to True, then a name that can't be resolved\n1422:         will generate a FieldError.\n1423: \n1424:         Return a list of PathInfo tuples. In addition return the final field\n1425:         (the last used join field) and target (which is a field guaranteed to\n1426:         contain the same value as the final field). Finally, return those names\n1427:         that weren't found (which are likely transforms and the final lookup).\n1428:         \"\"\"\n1429:         path, names_with_path = [], []\n1430:         for pos, name in enumerate(names):\n1431:             cur_names_with_path = (name, [])\n1432:             if name == 'pk':\n1433:                 name = opts.pk.name\n1434: \n1435:             field = None\n1436:             filtered_relation = None\n1437:             try:\n1438:                 field = opts.get_field(name)\n1439:             except FieldDoesNotExist:\n1440:                 if name in self.annotation_select:\n1441:                     field = self.annotation_select[name].output_field\n1442:                 elif name in self._filtered_relations and pos == 0:\n1443:                     filtered_relation = self._filtered_relations[name]\n1444:                     field = opts.get_field(filtered_relation.relation_name)\n1445:             if field is not None:\n1446:                 # Fields that contain one-to-many relations with a generic\n1447:                 # model (like a GenericForeignKey) cannot generate reverse\n1448:                 # relations and therefore cannot be used for reverse querying.\n1449:                 if field.is_relation and not field.related_model:\n1450:                     raise FieldError(\n1451:                         \"Field %r does not generate an automatic reverse \"\n1452:                         \"relation and therefore cannot be used for reverse \"\n1453:                         \"querying. If it is a GenericForeignKey, consider \"\n1454:                         \"adding a GenericRelation.\" % name\n1455:                     )\n1456:                 try:\n1457:                     model = field.model._meta.concrete_model\n1458:                 except AttributeError:\n1459:                     # QuerySet.annotate() may introduce fields that aren't\n1460:                     # attached to a model.\n1461:                     model = None\n1462:             else:\n1463:                 # We didn't find the current field, so move position back\n1464:                 # one step.\n1465:                 pos -= 1\n1466:                 if pos == -1 or fail_on_missing:\n1467:                     available = sorted([\n1468:                         *get_field_names_from_opts(opts),\n1469:                         *self.annotation_select,\n1470:                         *self._filtered_relations,\n1471:                     ])\n1472:                     raise FieldError(\"Cannot resolve keyword '%s' into field. \"\n1473:                                      \"Choices are: %s\" % (name, \", \".join(available)))\n1474:                 break\n1475:             # Check if we need any joins for concrete inheritance cases (the\n1476:             # field lives in parent, but we are currently in one of its\n1477:             # children)\n1478:             if model is not opts.model:\n1479:                 path_to_parent = opts.get_path_to_parent(model)\n1480:                 if path_to_parent:\n1481:                     path.extend(path_to_parent)\n1482:                     cur_names_with_path[1].extend(path_to_parent)\n1483:                     opts = path_to_parent[-1].to_opts\n1484:             if hasattr(field, 'get_path_info'):\n1485:                 pathinfos = field.get_path_info(filtered_relation)\n1486:                 if not allow_many:\n1487:                     for inner_pos, p in enumerate(pathinfos):\n1488:                         if p.m2m:\n1489:                             cur_names_with_path[1].extend(pathinfos[0:inner_pos + 1])\n1490:                             names_with_path.append(cur_names_with_path)\n1491:                             raise MultiJoin(pos + 1, names_with_path)\n1492:                 last = pathinfos[-1]\n1493:                 path.extend(pathinfos)\n1494:                 final_field = last.join_field\n1495:                 opts = last.to_opts\n1496:                 targets = last.target_fields\n1497:                 cur_names_with_path[1].extend(pathinfos)\n1498:                 names_with_path.append(cur_names_with_path)\n1499:             else:\n1500:                 # Local non-relational field.\n1501:                 final_field = field\n1502:                 targets = (field,)\n1503:                 if fail_on_missing and pos + 1 != len(names):\n1504:                     raise FieldError(\n1505:                         \"Cannot resolve keyword %r into field. Join on '%s'\"\n1506:                         \" not permitted.\" % (names[pos + 1], name))\n1507:                 break\n1508:         return path, final_field, targets, names[pos + 1:]\n1509: \n1510:     def setup_joins(self, names, opts, alias, can_reuse=None, allow_many=True,\n1511:                     reuse_with_filtered_relation=False):\n1512:         \"\"\"\n1513:         Compute the necessary table joins for the passage through the fields\n1514:         given in 'names'. 'opts' is the Options class for the current model\n1515:         (which gives the table we are starting from), 'alias' is the alias for\n1516:         the table to start the joining from.\n1517: \n1518:         The 'can_reuse' defines the reverse foreign key joins we can reuse. It\n1519:         can be None in which case all joins are reusable or a set of aliases\n1520:         that can be reused. Note that non-reverse foreign keys are always\n1521:         reusable when using setup_joins().\n1522: \n1523:         The 'reuse_with_filtered_relation' can be used to force 'can_reuse'\n1524:         parameter and force the relation on the given connections.\n1525: \n1526:         If 'allow_many' is False, then any reverse foreign key seen will\n1527:         generate a MultiJoin exception.\n1528: \n1529:         Return the final field involved in the joins, the target field (used\n1530:         for any 'where' constraint), the final 'opts' value, the joins, the\n1531:         field path traveled to generate the joins, and a transform function\n1532:         that takes a field and alias and is equivalent to `field.get_col(alias)`\n1533:         in the simple case but wraps field transforms if they were included in\n1534:         names.\n1535: \n1536:         The target field is the field containing the concrete value. Final\n1537:         field can be something different, for example foreign key pointing to\n1538:         that value. Final field is needed for example in some value\n1539:         conversions (convert 'obj' in fk__id=obj to pk val using the foreign\n1540:         key field for example).\n1541:         \"\"\"\n1542:         joins = [alias]\n1543:         # The transform can't be applied yet, as joins must be trimmed later.\n1544:         # To avoid making every caller of this method look up transforms\n1545:         # directly, compute transforms here and create a partial that converts\n1546:         # fields to the appropriate wrapped version.\n1547: \n1548:         def final_transformer(field, alias):\n1549:             return field.get_col(alias)\n1550: \n1551:         # Try resolving all the names as fields first. If there's an error,\n1552:         # treat trailing names as lookups until a field can be resolved.\n1553:         last_field_exception = None\n1554:         for pivot in range(len(names), 0, -1):\n1555:             try:\n1556:                 path, final_field, targets, rest = self.names_to_path(\n1557:                     names[:pivot], opts, allow_many, fail_on_missing=True,\n1558:                 )\n1559:             except FieldError as exc:\n1560:                 if pivot == 1:\n1561:                     # The first item cannot be a lookup, so it's safe\n1562:                     # to raise the field error here.\n1563:                     raise\n1564:                 else:\n1565:                     last_field_exception = exc\n1566:             else:\n1567:                 # The transforms are the remaining items that couldn't be\n1568:                 # resolved into fields.\n1569:                 transforms = names[pivot:]\n1570:                 break\n1571:         for name in transforms:\n1572:             def transform(field, alias, *, name, previous):\n1573:                 try:\n1574:                     wrapped = previous(field, alias)\n1575:                     return self.try_transform(wrapped, name)\n1576:                 except FieldError:\n1577:                     # FieldError is raised if the transform doesn't exist.\n1578:                     if isinstance(final_field, Field) and last_field_exception:\n1579:                         raise last_field_exception\n1580:                     else:\n1581:                         raise\n1582:             final_transformer = functools.partial(transform, name=name, previous=final_transformer)\n1583:         # Then, add the path to the query's joins. Note that we can't trim\n1584:         # joins at this stage - we will need the information about join type\n1585:         # of the trimmed joins.\n1586:         for join in path:\n1587:             if join.filtered_relation:\n1588:                 filtered_relation = join.filtered_relation.clone()\n1589:                 table_alias = filtered_relation.alias\n1590:             else:\n1591:                 filtered_relation = None\n1592:                 table_alias = None\n1593:             opts = join.to_opts\n1594:             if join.direct:\n1595:                 nullable = self.is_nullable(join.join_field)\n1596:             else:\n1597:                 nullable = True\n1598:             connection = Join(\n1599:                 opts.db_table, alias, table_alias, INNER, join.join_field,\n1600:                 nullable, filtered_relation=filtered_relation,\n1601:             )\n1602:             reuse = can_reuse if join.m2m or reuse_with_filtered_relation else None\n1603:             alias = self.join(\n1604:                 connection, reuse=reuse,\n1605:                 reuse_with_filtered_relation=reuse_with_filtered_relation,\n1606:             )\n1607:             joins.append(alias)\n1608:             if filtered_relation:\n1609:                 filtered_relation.path = joins[:]\n1610:         return JoinInfo(final_field, targets, opts, joins, path, final_transformer)\n1611: \n1612:     def trim_joins(self, targets, joins, path):\n1613:         \"\"\"\n1614:         The 'target' parameter is the final field being joined to, 'joins'\n1615:         is the full list of join aliases. The 'path' contain the PathInfos\n1616:         used to create the joins.\n1617: \n1618:         Return the final target field and table alias and the new active\n1619:         joins.\n1620: \n1621:         Always trim any direct join if the target column is already in the\n1622:         previous table. Can't trim reverse joins as it's unknown if there's\n1623:         anything on the other side of the join.\n1624:         \"\"\"\n1625:         joins = joins[:]\n1626:         for pos, info in enumerate(reversed(path)):\n1627:             if len(joins) == 1 or not info.direct:\n1628:                 break\n1629:             if info.filtered_relation:\n1630:                 break\n1631:             join_targets = {t.column for t in info.join_field.foreign_related_fields}\n1632:             cur_targets = {t.column for t in targets}\n1633:             if not cur_targets.issubset(join_targets):\n1634:                 break\n1635:             targets_dict = {r[1].column: r[0] for r in info.join_field.related_fields if r[1].column in cur_targets}\n1636:             targets = tuple(targets_dict[t.column] for t in targets)\n1637:             self.unref_alias(joins.pop())\n1638:         return targets, joins[-1], joins\n1639: \n1640:     @classmethod\n1641:     def _gen_col_aliases(cls, exprs):\n1642:         for expr in exprs:\n1643:             if isinstance(expr, Col):\n1644:                 yield expr.alias\n1645:             else:\n1646:                 yield from cls._gen_col_aliases(expr.get_source_expressions())\n1647: \n1648:     def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False, simple_col=False):\n1649:         if not allow_joins and LOOKUP_SEP in name:\n1650:             raise FieldError(\"Joined field references are not permitted in this query\")\n1651:         annotation = self.annotations.get(name)\n1652:         if annotation is not None:\n1653:             if not allow_joins:\n1654:                 for alias in self._gen_col_aliases([annotation]):\n1655:                     if isinstance(self.alias_map[alias], Join):\n1656:                         raise FieldError(\n1657:                             'Joined field references are not permitted in '\n1658:                             'this query'\n1659:                         )\n1660:             if summarize:\n1661:                 # Summarize currently means we are doing an aggregate() query\n1662:                 # which is executed as a wrapped subquery if any of the\n1663:                 # aggregate() elements reference an existing annotation. In\n1664:                 # that case we need to return a Ref to the subquery's annotation.\n1665:                 return Ref(name, self.annotation_select[name])\n1666:             else:\n1667:                 return annotation\n1668:         else:\n1669:             field_list = name.split(LOOKUP_SEP)\n1670:             join_info = self.setup_joins(field_list, self.get_meta(), self.get_initial_alias(), can_reuse=reuse)\n1671:             targets, final_alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)\n1672:             if not allow_joins and len(join_list) > 1:\n1673:                 raise FieldError('Joined field references are not permitted in this query')\n1674:             if len(targets) > 1:\n1675:                 raise FieldError(\"Referencing multicolumn fields with F() objects \"\n1676:                                  \"isn't supported\")\n1677:             # Verify that the last lookup in name is a field or a transform:\n1678:             # transform_function() raises FieldError if not.\n1679:             join_info.transform_function(targets[0], final_alias)\n1680:             if reuse is not None:\n1681:                 reuse.update(join_list)\n1682:             col = _get_col(targets[0], join_info.targets[0], join_list[-1], simple_col)\n1683:             return col\n1684: \n1685:     def split_exclude(self, filter_expr, can_reuse, names_with_path):\n1686:         \"\"\"\n1687:         When doing an exclude against any kind of N-to-many relation, we need\n1688:         to use a subquery. This method constructs the nested query, given the\n1689:         original exclude filter (filter_expr) and the portion up to the first\n1690:         N-to-many relation field.\n1691: \n1692:         For example, if the origin filter is ~Q(child__name='foo'), filter_expr\n1693:         is ('child__name', 'foo') and can_reuse is a set of joins usable for\n1694:         filters in the original query.\n1695: \n1696:         We will turn this into equivalent of:\n1697:             WHERE NOT (pk IN (SELECT parent_id FROM thetable\n1698:                               WHERE name = 'foo' AND parent_id IS NOT NULL))\n1699: \n1700:         It might be worth it to consider using WHERE NOT EXISTS as that has\n1701:         saner null handling, and is easier for the backend's optimizer to\n1702:         handle.\n1703:         \"\"\"\n1704:         filter_lhs, filter_rhs = filter_expr\n1705:         if isinstance(filter_rhs, F):\n1706:             filter_expr = (filter_lhs, OuterRef(filter_rhs.name))\n1707:         # Generate the inner query.\n1708:         query = Query(self.model)\n1709:         query._filtered_relations = self._filtered_relations\n1710:         query.add_filter(filter_expr)\n1711:         query.clear_ordering(True)\n1712:         # Try to have as simple as possible subquery -> trim leading joins from\n1713:         # the subquery.\n1714:         trimmed_prefix, contains_louter = query.trim_start(names_with_path)\n1715: \n1716:         # Add extra check to make sure the selected field will not be null\n1717:         # since we are adding an IN <subquery> clause. This prevents the\n1718:         # database from tripping over IN (...,NULL,...) selects and returning\n1719:         # nothing\n1720:         col = query.select[0]\n1721:         select_field = col.target\n1722:         alias = col.alias\n1723:         if self.is_nullable(select_field):\n1724:             lookup_class = select_field.get_lookup('isnull')\n1725:             lookup = lookup_class(select_field.get_col(alias), False)\n1726:             query.where.add(lookup, AND)\n1727:         if alias in can_reuse:\n1728:             pk = select_field.model._meta.pk\n1729:             # Need to add a restriction so that outer query's filters are in effect for\n1730:             # the subquery, too.\n1731:             query.bump_prefix(self)\n1732:             lookup_class = select_field.get_lookup('exact')\n1733:             # Note that the query.select[0].alias is different from alias\n1734:             # due to bump_prefix above.\n1735:             lookup = lookup_class(pk.get_col(query.select[0].alias),\n1736:                                   pk.get_col(alias))\n1737:             query.where.add(lookup, AND)\n1738:             query.external_aliases.add(alias)\n1739: \n1740:         condition, needed_inner = self.build_filter(\n1741:             ('%s__in' % trimmed_prefix, query),\n1742:             current_negated=True, branch_negated=True, can_reuse=can_reuse)\n1743:         if contains_louter:\n1744:             or_null_condition, _ = self.build_filter(\n1745:                 ('%s__isnull' % trimmed_prefix, True),\n1746:                 current_negated=True, branch_negated=True, can_reuse=can_reuse)\n1747:             condition.add(or_null_condition, OR)\n1748:             # Note that the end result will be:\n1749:             # (outercol NOT IN innerq AND outercol IS NOT NULL) OR outercol IS NULL.\n1750:             # This might look crazy but due to how IN works, this seems to be\n1751:             # correct. If the IS NOT NULL check is removed then outercol NOT\n1752:             # IN will return UNKNOWN. If the IS NULL check is removed, then if\n1753:             # outercol IS NULL we will not match the row.\n1754:         return condition, needed_inner\n1755: \n1756:     def set_empty(self):\n1757:         self.where.add(NothingNode(), AND)\n1758: \n1759:     def is_empty(self):\n1760:         return any(isinstance(c, NothingNode) for c in self.where.children)\n1761: \n1762:     def set_limits(self, low=None, high=None):\n1763:         \"\"\"\n1764:         Adjust the limits on the rows retrieved. Use low/high to set these,\n1765:         as it makes it more Pythonic to read and write. When the SQL query is\n1766:         created, convert them to the appropriate offset and limit values.\n1767: \n1768:         Apply any limits passed in here to the existing constraints. Add low\n1769:         to the current low value and clamp both to any existing high value.\n1770:         \"\"\"\n1771:         if high is not None:\n1772:             if self.high_mark is not None:\n1773:                 self.high_mark = min(self.high_mark, self.low_mark + high)\n... Code Truncated ...\n\n",
  "line_level_localization": [
    {
      "filename": "/django/db/models/fields/__init__.py",
      "suspect_lines": [
        2335,
        2336,
        2337,
        2338
      ]
    },
    {
      "filename": "/django/db/models/fields/related_lookups.py",
      "suspect_lines": [
        104
      ]
    },
    {
      "filename": "/django/db/models/sql/query.py",
      "suspect_lines": [
        1705
      ]
    }
  ]
}