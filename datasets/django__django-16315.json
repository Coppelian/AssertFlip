{
  "instance_id": "django__django-16315",
  "problem_statement": "QuerySet.bulk_create() crashes on mixed case columns in unique_fields/update_fields.\nDescription\n\t\nNot sure exactly how to phrase this, but when I I'm calling bulk_update on the manager for a class with db_column set on fields the SQL is invalid. Ellipses indicate other fields excluded for clarity.\nclass ActivityBlackListed(models.Model):\n\t\"\"\"\n\tOriginally sourced from Activity_BlackListed in /home/josh/PNDS_Interim_MIS-Data.accdb (13 records)\n\t\"\"\"\n\tclass Meta:\n\t\tdb_table = \"Activity_BlackListed\"\n\tblacklistid = models.IntegerField(primary_key=True, db_column=\"BlacklistID\")\n\tsectorid = models.IntegerField(null=True, blank=True, db_column=\"SectorID\")\n\t...\nqs.bulk_create(instances, update_conflicts=True, update_fields=[\"sectorid\", ...], unique_fields=[\"blacklistid\"])\nThe \"INSERT\" code does take into account the db_columns\nINSERT INTO \"Activity_BlackListed\" (\"BlacklistID\",...) VALUES (%s, ...),\nThe code which is generated for \"ON CONFLICT\" uses the field name and not the db_column which leads to a syntax error\n'ON CONFLICT(\"blacklistid\") DO UPDATE SET \"sectorid\" = EXCLUDED.\"sectorid\", ...\nPostgreSQL returns ERROR: column \"blacklistid\" does not exist at character 1508\nWhat should be generated is I think:\n'ON CONFLICT(\"BlacklistID\") DO UPDATE SET \"SectorID\" = EXCLUDED.\"SectorID\", ...\n",
  "localized_code": "[start of django/db/models/query.py]\n1: \"\"\"\n2: The main QuerySet implementation. This provides the public API for the ORM.\n3: \"\"\"\n4: \n5: import copy\n6: import operator\n7: import warnings\n8: from itertools import chain, islice\n9: \n10: from asgiref.sync import sync_to_async\n11: \n12: import django\n13: from django.conf import settings\n14: from django.core import exceptions\n15: from django.db import (\n16:     DJANGO_VERSION_PICKLE_KEY,\n17:     IntegrityError,\n18:     NotSupportedError,\n19:     connections,\n20:     router,\n21:     transaction,\n22: )\n23: from django.db.models import AutoField, DateField, DateTimeField, Field, sql\n24: from django.db.models.constants import LOOKUP_SEP, OnConflict\n25: from django.db.models.deletion import Collector\n26: from django.db.models.expressions import Case, F, Ref, Value, When\n27: from django.db.models.functions import Cast, Trunc\n28: from django.db.models.query_utils import FilteredRelation, Q\n29: from django.db.models.sql.constants import CURSOR, GET_ITERATOR_CHUNK_SIZE\n30: from django.db.models.utils import (\n31:     AltersData,\n32:     create_namedtuple_class,\n33:     resolve_callables,\n34: )\n35: from django.utils import timezone\n36: from django.utils.deprecation import RemovedInDjango50Warning\n37: from django.utils.functional import cached_property, partition\n38: \n39: # The maximum number of results to fetch in a get() query.\n40: MAX_GET_RESULTS = 21\n41: \n42: # The maximum number of items to display in a QuerySet.__repr__\n43: REPR_OUTPUT_SIZE = 20\n44: \n45: \n46: class BaseIterable:\n47:     def __init__(\n48:         self, queryset, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE\n49:     ):\n50:         self.queryset = queryset\n51:         self.chunked_fetch = chunked_fetch\n52:         self.chunk_size = chunk_size\n53: \n54:     async def _async_generator(self):\n55:         # Generators don't actually start running until the first time you call\n56:         # next() on them, so make the generator object in the async thread and\n57:         # then repeatedly dispatch to it in a sync thread.\n58:         sync_generator = self.__iter__()\n59: \n60:         def next_slice(gen):\n61:             return list(islice(gen, self.chunk_size))\n62: \n63:         while True:\n64:             chunk = await sync_to_async(next_slice)(sync_generator)\n65:             for item in chunk:\n66:                 yield item\n67:             if len(chunk) < self.chunk_size:\n68:                 break\n69: \n70:     # __aiter__() is a *synchronous* method that has to then return an\n71:     # *asynchronous* iterator/generator. Thus, nest an async generator inside\n72:     # it.\n73:     # This is a generic iterable converter for now, and is going to suffer a\n74:     # performance penalty on large sets of items due to the cost of crossing\n75:     # over the sync barrier for each chunk. Custom __aiter__() methods should\n76:     # be added to each Iterable subclass, but that needs some work in the\n77:     # Compiler first.\n78:     def __aiter__(self):\n79:         return self._async_generator()\n80: \n81: \n82: class ModelIterable(BaseIterable):\nCode replaced for brevity.\n144: \n145: \n146: \n147: class RawModelIterable(BaseIterable):\nCode replaced for brevity.\n188: \n189: \n190: \n191: class ValuesIterable(BaseIterable):\nCode replaced for brevity.\n211: \n212: \n213: \n214: class ValuesListIterable(BaseIterable):\nCode replaced for brevity.\n250: \n251: \n252: \n253: class NamedValuesListIterable(ValuesListIterable):\nCode replaced for brevity.\n273: \n274: \n275: \n276: class FlatValuesListIterable(BaseIterable):\nCode replaced for brevity.\n288: \n289: \n290: \n291: class QuerySet(AltersData):\n292:     \"\"\"Represent a lazy database lookup for a set of objects.\"\"\"\n293: \n294:     def __init__(self, model=None, query=None, using=None, hints=None):\n295:         self.model = model\n296:         self._db = using\n297:         self._hints = hints or {}\n298:         self._query = query or sql.Query(self.model)\n299:         self._result_cache = None\n300:         self._sticky_filter = False\n301:         self._for_write = False\n302:         self._prefetch_related_lookups = ()\n303:         self._prefetch_done = False\n304:         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}\n305:         self._iterable_class = ModelIterable\n306:         self._fields = None\n307:         self._defer_next_filter = False\n308:         self._deferred_filter = None\n309: \n310:     @property\n311:     def query(self):\n312:         if self._deferred_filter:\n313:             negate, args, kwargs = self._deferred_filter\n314:             self._filter_or_exclude_inplace(negate, args, kwargs)\n315:             self._deferred_filter = None\n316:         return self._query\n317: \n318:     @query.setter\n319:     def query(self, value):\n320:         if value.values_select:\n321:             self._iterable_class = ValuesIterable\n322:         self._query = value\n323: \n324:     def as_manager(cls):\n325:         # Address the circular dependency between `Queryset` and `Manager`.\n326:         from django.db.models.manager import Manager\n327: \n328:         manager = Manager.from_queryset(cls)()\n329:         manager._built_with_as_manager = True\n330:         return manager\n331: \n332:     as_manager.queryset_only = True\n333:     as_manager = classmethod(as_manager)\n334: \n335:     ########################\n336:     # PYTHON MAGIC METHODS #\n337:     ########################\n338: \n339:     def __deepcopy__(self, memo):\n340:         \"\"\"Don't populate the QuerySet's cache.\"\"\"\n341:         obj = self.__class__()\n342:         for k, v in self.__dict__.items():\n343:             if k == \"_result_cache\":\n344:                 obj.__dict__[k] = None\n345:             else:\n346:                 obj.__dict__[k] = copy.deepcopy(v, memo)\n347:         return obj\n348: \n349:     def __getstate__(self):\n350:         # Force the cache to be fully populated.\n351:         self._fetch_all()\n352:         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}\n353: \n354:     def __setstate__(self, state):\n355:         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)\n356:         if pickled_version:\n357:             if pickled_version != django.__version__:\n358:                 warnings.warn(\n359:                     \"Pickled queryset instance's Django version %s does not \"\n360:                     \"match the current version %s.\"\n361:                     % (pickled_version, django.__version__),\n362:                     RuntimeWarning,\n363:                     stacklevel=2,\n364:                 )\n365:         else:\n366:             warnings.warn(\n367:                 \"Pickled queryset instance's Django version is not specified.\",\n368:                 RuntimeWarning,\n369:                 stacklevel=2,\n370:             )\n371:         self.__dict__.update(state)\n372: \n373:     def __repr__(self):\n374:         data = list(self[: REPR_OUTPUT_SIZE + 1])\n375:         if len(data) > REPR_OUTPUT_SIZE:\n376:             data[-1] = \"...(remaining elements truncated)...\"\n377:         return \"<%s %r>\" % (self.__class__.__name__, data)\n378: \n379:     def __len__(self):\n380:         self._fetch_all()\n381:         return len(self._result_cache)\n382: \n383:     def __iter__(self):\n384:         \"\"\"\n385:         The queryset iterator protocol uses three nested iterators in the\n386:         default case:\n387:             1. sql.compiler.execute_sql()\n388:                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)\n389:                  using cursor.fetchmany(). This part is responsible for\n390:                  doing some column masking, and returning the rows in chunks.\n391:             2. sql.compiler.results_iter()\n392:                - Returns one row at time. At this point the rows are still just\n393:                  tuples. In some cases the return values are converted to\n394:                  Python values at this location.\n395:             3. self.iterator()\n396:                - Responsible for turning the rows into model objects.\n397:         \"\"\"\n398:         self._fetch_all()\n399:         return iter(self._result_cache)\n400: \n401:     def __aiter__(self):\n402:         # Remember, __aiter__ itself is synchronous, it's the thing it returns\n403:         # that is async!\n404:         async def generator():\n405:             await sync_to_async(self._fetch_all)()\n406:             for item in self._result_cache:\n407:                 yield item\n408: \n409:         return generator()\n410: \n411:     def __bool__(self):\n412:         self._fetch_all()\n413:         return bool(self._result_cache)\n414: \n415:     def __getitem__(self, k):\n416:         \"\"\"Retrieve an item or slice from the set of results.\"\"\"\n417:         if not isinstance(k, (int, slice)):\n418:             raise TypeError(\n419:                 \"QuerySet indices must be integers or slices, not %s.\"\n420:                 % type(k).__name__\n421:             )\n422:         if (isinstance(k, int) and k < 0) or (\n423:             isinstance(k, slice)\n424:             and (\n425:                 (k.start is not None and k.start < 0)\n426:                 or (k.stop is not None and k.stop < 0)\n427:             )\n428:         ):\n429:             raise ValueError(\"Negative indexing is not supported.\")\n430: \n431:         if self._result_cache is not None:\n432:             return self._result_cache[k]\n433: \n434:         if isinstance(k, slice):\n435:             qs = self._chain()\n436:             if k.start is not None:\n437:                 start = int(k.start)\n438:             else:\n439:                 start = None\n440:             if k.stop is not None:\n441:                 stop = int(k.stop)\n442:             else:\n443:                 stop = None\n444:             qs.query.set_limits(start, stop)\n445:             return list(qs)[:: k.step] if k.step else qs\n446: \n447:         qs = self._chain()\n448:         qs.query.set_limits(k, k + 1)\n449:         qs._fetch_all()\n450:         return qs._result_cache[0]\n451: \n452:     def __class_getitem__(cls, *args, **kwargs):\n453:         return cls\n454: \n455:     def __and__(self, other):\n456:         self._check_operator_queryset(other, \"&\")\n457:         self._merge_sanity_check(other)\n458:         if isinstance(other, EmptyQuerySet):\n459:             return other\n460:         if isinstance(self, EmptyQuerySet):\n461:             return self\n462:         combined = self._chain()\n463:         combined._merge_known_related_objects(other)\n464:         combined.query.combine(other.query, sql.AND)\n465:         return combined\n466: \n467:     def __or__(self, other):\n468:         self._check_operator_queryset(other, \"|\")\n469:         self._merge_sanity_check(other)\n470:         if isinstance(self, EmptyQuerySet):\n471:             return other\n472:         if isinstance(other, EmptyQuerySet):\n473:             return self\n474:         query = (\n475:             self\n476:             if self.query.can_filter()\n477:             else self.model._base_manager.filter(pk__in=self.values(\"pk\"))\n478:         )\n479:         combined = query._chain()\n480:         combined._merge_known_related_objects(other)\n481:         if not other.query.can_filter():\n482:             other = other.model._base_manager.filter(pk__in=other.values(\"pk\"))\n483:         combined.query.combine(other.query, sql.OR)\n484:         return combined\n485: \n486:     def __xor__(self, other):\n487:         self._check_operator_queryset(other, \"^\")\n488:         self._merge_sanity_check(other)\n489:         if isinstance(self, EmptyQuerySet):\n490:             return other\n491:         if isinstance(other, EmptyQuerySet):\n492:             return self\n493:         query = (\n494:             self\n495:             if self.query.can_filter()\n496:             else self.model._base_manager.filter(pk__in=self.values(\"pk\"))\n497:         )\n498:         combined = query._chain()\n499:         combined._merge_known_related_objects(other)\n500:         if not other.query.can_filter():\n501:             other = other.model._base_manager.filter(pk__in=other.values(\"pk\"))\n502:         combined.query.combine(other.query, sql.XOR)\n503:         return combined\n504: \n505:     ####################################\n506:     # METHODS THAT DO DATABASE QUERIES #\n507:     ####################################\n508: \n509:     def _iterator(self, use_chunked_fetch, chunk_size):\n510:         iterable = self._iterable_class(\n511:             self,\n512:             chunked_fetch=use_chunked_fetch,\n513:             chunk_size=chunk_size or 2000,\n514:         )\n515:         if not self._prefetch_related_lookups or chunk_size is None:\n516:             yield from iterable\n517:             return\n518: \n519:         iterator = iter(iterable)\n520:         while results := list(islice(iterator, chunk_size)):\n521:             prefetch_related_objects(results, *self._prefetch_related_lookups)\n522:             yield from results\n523: \n524:     def iterator(self, chunk_size=None):\n525:         \"\"\"\n526:         An iterator over the results from applying this QuerySet to the\n527:         database. chunk_size must be provided for QuerySets that prefetch\n528:         related objects. Otherwise, a default chunk_size of 2000 is supplied.\n529:         \"\"\"\n530:         if chunk_size is None:\n531:             if self._prefetch_related_lookups:\n532:                 # When the deprecation ends, replace with:\n533:                 # raise ValueError(\n534:                 #     'chunk_size must be provided when using '\n535:                 #     'QuerySet.iterator() after prefetch_related().'\n536:                 # )\n537:                 warnings.warn(\n538:                     \"Using QuerySet.iterator() after prefetch_related() \"\n539:                     \"without specifying chunk_size is deprecated.\",\n540:                     category=RemovedInDjango50Warning,\n541:                     stacklevel=2,\n542:                 )\n543:         elif chunk_size <= 0:\n544:             raise ValueError(\"Chunk size must be strictly positive.\")\n545:         use_chunked_fetch = not connections[self.db].settings_dict.get(\n546:             \"DISABLE_SERVER_SIDE_CURSORS\"\n547:         )\n548:         return self._iterator(use_chunked_fetch, chunk_size)\n549: \n550:     async def aiterator(self, chunk_size=2000):\n551:         \"\"\"\n552:         An asynchronous iterator over the results from applying this QuerySet\n553:         to the database.\n554:         \"\"\"\n555:         if self._prefetch_related_lookups:\n556:             raise NotSupportedError(\n557:                 \"Using QuerySet.aiterator() after prefetch_related() is not supported.\"\n558:             )\n559:         if chunk_size <= 0:\n560:             raise ValueError(\"Chunk size must be strictly positive.\")\n561:         use_chunked_fetch = not connections[self.db].settings_dict.get(\n562:             \"DISABLE_SERVER_SIDE_CURSORS\"\n563:         )\n564:         async for item in self._iterable_class(\n565:             self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size\n566:         ):\n567:             yield item\n568: \n569:     def aggregate(self, *args, **kwargs):\n570:         \"\"\"\n571:         Return a dictionary containing the calculations (aggregation)\n572:         over the current queryset.\n573: \n574:         If args is present the expression is passed as a kwarg using\n575:         the Aggregate object's default alias.\n576:         \"\"\"\n577:         if self.query.distinct_fields:\n578:             raise NotImplementedError(\"aggregate() + distinct(fields) not implemented.\")\n579:         self._validate_values_are_expressions(\n580:             (*args, *kwargs.values()), method_name=\"aggregate\"\n581:         )\n582:         for arg in args:\n583:             # The default_alias property raises TypeError if default_alias\n584:             # can't be set automatically or AttributeError if it isn't an\n585:             # attribute.\n586:             try:\n587:                 arg.default_alias\n588:             except (AttributeError, TypeError):\n589:                 raise TypeError(\"Complex aggregates require an alias\")\n590:             kwargs[arg.default_alias] = arg\n591: \n592:         query = self.query.chain()\n593:         for (alias, aggregate_expr) in kwargs.items():\n594:             query.add_annotation(aggregate_expr, alias, is_summary=True)\n595:             annotation = query.annotations[alias]\n596:             if not annotation.contains_aggregate:\n597:                 raise TypeError(\"%s is not an aggregate expression\" % alias)\n598:             for expr in annotation.get_source_expressions():\n599:                 if (\n600:                     expr.contains_aggregate\n601:                     and isinstance(expr, Ref)\n602:                     and expr.refs in kwargs\n603:                 ):\n604:                     name = expr.refs\n605:                     raise exceptions.FieldError(\n606:                         \"Cannot compute %s('%s'): '%s' is an aggregate\"\n607:                         % (annotation.name, name, name)\n608:                     )\n609:         return query.get_aggregation(self.db, kwargs)\n610: \n611:     async def aaggregate(self, *args, **kwargs):\n612:         return await sync_to_async(self.aggregate)(*args, **kwargs)\n613: \n614:     def count(self):\n615:         \"\"\"\n616:         Perform a SELECT COUNT() and return the number of records as an\n617:         integer.\n618: \n619:         If the QuerySet is already fully cached, return the length of the\n620:         cached results set to avoid multiple SELECT COUNT(*) calls.\n621:         \"\"\"\n622:         if self._result_cache is not None:\n623:             return len(self._result_cache)\n624: \n625:         return self.query.get_count(using=self.db)\n626: \n627:     async def acount(self):\n628:         return await sync_to_async(self.count)()\n629: \n630:     def get(self, *args, **kwargs):\n631:         \"\"\"\n632:         Perform the query and return a single object matching the given\n633:         keyword arguments.\n634:         \"\"\"\n635:         if self.query.combinator and (args or kwargs):\n636:             raise NotSupportedError(\n637:                 \"Calling QuerySet.get(...) with filters after %s() is not \"\n638:                 \"supported.\" % self.query.combinator\n639:             )\n640:         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)\n641:         if self.query.can_filter() and not self.query.distinct_fields:\n642:             clone = clone.order_by()\n643:         limit = None\n644:         if (\n645:             not clone.query.select_for_update\n646:             or connections[clone.db].features.supports_select_for_update_with_limit\n647:         ):\n648:             limit = MAX_GET_RESULTS\n649:             clone.query.set_limits(high=limit)\n650:         num = len(clone)\n651:         if num == 1:\n652:             return clone._result_cache[0]\n653:         if not num:\n654:             raise self.model.DoesNotExist(\n655:                 \"%s matching query does not exist.\" % self.model._meta.object_name\n656:             )\n657:         raise self.model.MultipleObjectsReturned(\n658:             \"get() returned more than one %s -- it returned %s!\"\n659:             % (\n660:                 self.model._meta.object_name,\n661:                 num if not limit or num < limit else \"more than %s\" % (limit - 1),\n662:             )\n663:         )\n664: \n665:     async def aget(self, *args, **kwargs):\n666:         return await sync_to_async(self.get)(*args, **kwargs)\n667: \n668:     def create(self, **kwargs):\n669:         \"\"\"\n670:         Create a new object with the given kwargs, saving it to the database\n671:         and returning the created object.\n672:         \"\"\"\n673:         obj = self.model(**kwargs)\n674:         self._for_write = True\n675:         obj.save(force_insert=True, using=self.db)\n676:         return obj\n677: \n678:     async def acreate(self, **kwargs):\n679:         return await sync_to_async(self.create)(**kwargs)\n680: \n681:     def _prepare_for_bulk_create(self, objs):\n682:         for obj in objs:\n683:             if obj.pk is None:\n684:                 # Populate new PK values.\n685:                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)\n686:             obj._prepare_related_fields_for_save(operation_name=\"bulk_create\")\n687: \n688:     def _check_bulk_create_options(\n689:         self, ignore_conflicts, update_conflicts, update_fields, unique_fields\n690:     ):\n691:         if ignore_conflicts and update_conflicts:\n692:             raise ValueError(\n693:                 \"ignore_conflicts and update_conflicts are mutually exclusive.\"\n694:             )\n695:         db_features = connections[self.db].features\n696:         if ignore_conflicts:\n697:             if not db_features.supports_ignore_conflicts:\n698:                 raise NotSupportedError(\n699:                     \"This database backend does not support ignoring conflicts.\"\n700:                 )\n701:             return OnConflict.IGNORE\n702:         elif update_conflicts:\n703:             if not db_features.supports_update_conflicts:\n704:                 raise NotSupportedError(\n705:                     \"This database backend does not support updating conflicts.\"\n706:                 )\n707:             if not update_fields:\n708:                 raise ValueError(\n709:                     \"Fields that will be updated when a row insertion fails \"\n710:                     \"on conflicts must be provided.\"\n711:                 )\n712:             if unique_fields and not db_features.supports_update_conflicts_with_target:\n713:                 raise NotSupportedError(\n714:                     \"This database backend does not support updating \"\n715:                     \"conflicts with specifying unique fields that can trigger \"\n716:                     \"the upsert.\"\n717:                 )\n718:             if not unique_fields and db_features.supports_update_conflicts_with_target:\n719:                 raise ValueError(\n720:                     \"Unique fields that can trigger the upsert must be provided.\"\n721:                 )\n722:             # Updating primary keys and non-concrete fields is forbidden.\n723:             update_fields = [self.model._meta.get_field(name) for name in update_fields]\n724:             if any(not f.concrete or f.many_to_many for f in update_fields):\n725:                 raise ValueError(\n726:                     \"bulk_create() can only be used with concrete fields in \"\n727:                     \"update_fields.\"\n728:                 )\n729:             if any(f.primary_key for f in update_fields):\n730:                 raise ValueError(\n731:                     \"bulk_create() cannot be used with primary keys in \"\n732:                     \"update_fields.\"\n733:                 )\n734:             if unique_fields:\n735:                 unique_fields = [\n736:                     self.model._meta.get_field(name) for name in unique_fields\n737:                 ]\n738:                 if any(not f.concrete or f.many_to_many for f in unique_fields):\n739:                     raise ValueError(\n740:                         \"bulk_create() can only be used with concrete fields \"\n741:                         \"in unique_fields.\"\n742:                     )\n743:             return OnConflict.UPDATE\n744:         return None\n745: \n746:     def bulk_create(\n747:         self,\n748:         objs,\n749:         batch_size=None,\n750:         ignore_conflicts=False,\n751:         update_conflicts=False,\n752:         update_fields=None,\n753:         unique_fields=None,\n754:     ):\n755:         \"\"\"\n756:         Insert each of the instances into the database. Do *not* call\n757:         save() on each of the instances, do not send any pre/post_save\n758:         signals, and do not set the primary key attribute if it is an\n759:         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).\n760:         Multi-table models are not supported.\n761:         \"\"\"\n762:         # When you bulk insert you don't get the primary keys back (if it's an\n763:         # autoincrement, except if can_return_rows_from_bulk_insert=True), so\n764:         # you can't insert into the child tables which references this. There\n765:         # are two workarounds:\n766:         # 1) This could be implemented if you didn't have an autoincrement pk\n767:         # 2) You could do it by doing O(n) normal inserts into the parent\n768:         #    tables to get the primary keys back and then doing a single bulk\n769:         #    insert into the childmost table.\n770:         # We currently set the primary keys on the objects when using\n771:         # PostgreSQL via the RETURNING ID clause. It should be possible for\n772:         # Oracle as well, but the semantics for extracting the primary keys is\n773:         # trickier so it's not done yet.\n774:         if batch_size is not None and batch_size <= 0:\n775:             raise ValueError(\"Batch size must be a positive integer.\")\n776:         # Check that the parents share the same concrete model with the our\n777:         # model to detect the inheritance pattern ConcreteGrandParent ->\n778:         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy\n779:         # would not identify that case as involving multiple tables.\n780:         for parent in self.model._meta.get_parent_list():\n781:             if parent._meta.concrete_model is not self.model._meta.concrete_model:\n782:                 raise ValueError(\"Can't bulk create a multi-table inherited model\")\n783:         if not objs:\n784:             return objs\n785:         opts = self.model._meta\n786:         if unique_fields:\n787:             # Primary key is allowed in unique_fields.\n788:             unique_fields = [\n789:                 opts.pk.name if name == \"pk\" else name for name in unique_fields\n790:             ]\n791:         on_conflict = self._check_bulk_create_options(\n792:             ignore_conflicts,\n793:             update_conflicts,\n794:             update_fields,\n795:             unique_fields,\n796:         )\n797:         self._for_write = True\n798:         fields = opts.concrete_fields\n799:         objs = list(objs)\n800:         self._prepare_for_bulk_create(objs)\n801:         with transaction.atomic(using=self.db, savepoint=False):\n802:             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n803:             if objs_with_pk:\n804:                 returned_columns = self._batched_insert(\n805:                     objs_with_pk,\n806:                     fields,\n807:                     batch_size,\n808:                     on_conflict=on_conflict,\n809:                     update_fields=update_fields,\n810:                     unique_fields=unique_fields,\n811:                 )\n812:                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):\n813:                     for result, field in zip(results, opts.db_returning_fields):\n814:                         if field != opts.pk:\n815:                             setattr(obj_with_pk, field.attname, result)\n816:                 for obj_with_pk in objs_with_pk:\n817:                     obj_with_pk._state.adding = False\n818:                     obj_with_pk._state.db = self.db\n819:             if objs_without_pk:\n820:                 fields = [f for f in fields if not isinstance(f, AutoField)]\n821:                 returned_columns = self._batched_insert(\n822:                     objs_without_pk,\n823:                     fields,\n824:                     batch_size,\n825:                     on_conflict=on_conflict,\n826:                     update_fields=update_fields,\n827:                     unique_fields=unique_fields,\n828:                 )\n829:                 connection = connections[self.db]\n830:                 if (\n831:                     connection.features.can_return_rows_from_bulk_insert\n832:                     and on_conflict is None\n833:                 ):\n834:                     assert len(returned_columns) == len(objs_without_pk)\n835:                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):\n836:                     for result, field in zip(results, opts.db_returning_fields):\n837:                         setattr(obj_without_pk, field.attname, result)\n838:                     obj_without_pk._state.adding = False\n839:                     obj_without_pk._state.db = self.db\n840: \n841:         return objs\n842: \n843:     async def abulk_create(\n844:         self,\n845:         objs,\n846:         batch_size=None,\n847:         ignore_conflicts=False,\n848:         update_conflicts=False,\n849:         update_fields=None,\n850:         unique_fields=None,\n851:     ):\n852:         return await sync_to_async(self.bulk_create)(\n853:             objs=objs,\n854:             batch_size=batch_size,\n855:             ignore_conflicts=ignore_conflicts,\n856:             update_conflicts=update_conflicts,\n857:             update_fields=update_fields,\n858:             unique_fields=unique_fields,\n859:         )\n860: \n861:     def bulk_update(self, objs, fields, batch_size=None):\n862:         \"\"\"\n863:         Update the given fields in each of the given objects in the database.\n864:         \"\"\"\n865:         if batch_size is not None and batch_size <= 0:\n866:             raise ValueError(\"Batch size must be a positive integer.\")\n867:         if not fields:\n868:             raise ValueError(\"Field names must be given to bulk_update().\")\n869:         objs = tuple(objs)\n870:         if any(obj.pk is None for obj in objs):\n871:             raise ValueError(\"All bulk_update() objects must have a primary key set.\")\n872:         fields = [self.model._meta.get_field(name) for name in fields]\n873:         if any(not f.concrete or f.many_to_many for f in fields):\n874:             raise ValueError(\"bulk_update() can only be used with concrete fields.\")\n875:         if any(f.primary_key for f in fields):\n876:             raise ValueError(\"bulk_update() cannot be used with primary key fields.\")\n877:         if not objs:\n878:             return 0\n879:         for obj in objs:\n880:             obj._prepare_related_fields_for_save(\n881:                 operation_name=\"bulk_update\", fields=fields\n882:             )\n883:         # PK is used twice in the resulting update query, once in the filter\n884:         # and once in the WHEN. Each field will also have one CAST.\n885:         self._for_write = True\n886:         connection = connections[self.db]\n887:         max_batch_size = connection.ops.bulk_batch_size([\"pk\", \"pk\"] + fields, objs)\n888:         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n889:         requires_casting = connection.features.requires_casted_case_in_updates\n890:         batches = (objs[i : i + batch_size] for i in range(0, len(objs), batch_size))\n891:         updates = []\n892:         for batch_objs in batches:\n893:             update_kwargs = {}\n894:             for field in fields:\n895:                 when_statements = []\n896:                 for obj in batch_objs:\n897:                     attr = getattr(obj, field.attname)\n898:                     if not hasattr(attr, \"resolve_expression\"):\n899:                         attr = Value(attr, output_field=field)\n900:                     when_statements.append(When(pk=obj.pk, then=attr))\n901:                 case_statement = Case(*when_statements, output_field=field)\n902:                 if requires_casting:\n903:                     case_statement = Cast(case_statement, output_field=field)\n904:                 update_kwargs[field.attname] = case_statement\n905:             updates.append(([obj.pk for obj in batch_objs], update_kwargs))\n906:         rows_updated = 0\n907:         queryset = self.using(self.db)\n908:         with transaction.atomic(using=self.db, savepoint=False):\n909:             for pks, update_kwargs in updates:\n910:                 rows_updated += queryset.filter(pk__in=pks).update(**update_kwargs)\n911:         return rows_updated\n912: \n913:     bulk_update.alters_data = True\n914: \n915:     async def abulk_update(self, objs, fields, batch_size=None):\n916:         return await sync_to_async(self.bulk_update)(\n917:             objs=objs,\n918:             fields=fields,\n919:             batch_size=batch_size,\n920:         )\n921: \n922:     abulk_update.alters_data = True\n923: \n924:     def get_or_create(self, defaults=None, **kwargs):\n925:         \"\"\"\n926:         Look up an object with the given kwargs, creating one if necessary.\n927:         Return a tuple of (object, created), where created is a boolean\n928:         specifying whether an object was created.\n929:         \"\"\"\n930:         # The get() needs to be targeted at the write database in order\n931:         # to avoid potential transaction consistency problems.\n932:         self._for_write = True\n933:         try:\n934:             return self.get(**kwargs), False\n935:         except self.model.DoesNotExist:\n936:             params = self._extract_model_params(defaults, **kwargs)\n937:             # Try to create an object using passed params.\n938:             try:\n939:                 with transaction.atomic(using=self.db):\n940:                     params = dict(resolve_callables(params))\n941:                     return self.create(**params), True\n942:             except IntegrityError:\n943:                 try:\n944:                     return self.get(**kwargs), False\n945:                 except self.model.DoesNotExist:\n946:                     pass\n947:                 raise\n948: \n949:     async def aget_or_create(self, defaults=None, **kwargs):\n950:         return await sync_to_async(self.get_or_create)(\n951:             defaults=defaults,\n952:             **kwargs,\n953:         )\n954: \n955:     def update_or_create(self, defaults=None, **kwargs):\n956:         \"\"\"\n957:         Look up an object with the given kwargs, updating one with defaults\n958:         if it exists, otherwise create a new one.\n959:         Return a tuple (object, created), where created is a boolean\n960:         specifying whether an object was created.\n961:         \"\"\"\n962:         defaults = defaults or {}\n963:         self._for_write = True\n964:         with transaction.atomic(using=self.db):\n965:             # Lock the row so that a concurrent update is blocked until\n966:             # update_or_create() has performed its save.\n967:             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)\n968:             if created:\n969:                 return obj, created\n... Code Truncated ...\n\n[start of django/db/models/sql/compiler.py]\n1: import collections\n2: import json\n3: import re\n4: from functools import partial\n5: from itertools import chain\n6: \n7: from django.core.exceptions import EmptyResultSet, FieldError, FullResultSet\n8: from django.db import DatabaseError, NotSupportedError\n9: from django.db.models.constants import LOOKUP_SEP\n10: from django.db.models.expressions import F, OrderBy, RawSQL, Ref, Value\n11: from django.db.models.functions import Cast, Random\n12: from django.db.models.lookups import Lookup\n13: from django.db.models.query_utils import select_related_descend\n14: from django.db.models.sql.constants import (\n15:     CURSOR,\n16:     GET_ITERATOR_CHUNK_SIZE,\n17:     MULTI,\n18:     NO_RESULTS,\n19:     ORDER_DIR,\n20:     SINGLE,\n21: )\n22: from django.db.models.sql.query import Query, get_order_dir\n23: from django.db.models.sql.where import AND\n24: from django.db.transaction import TransactionManagementError\n25: from django.utils.functional import cached_property\n26: from django.utils.hashable import make_hashable\n27: from django.utils.regex_helper import _lazy_re_compile\n28: \n29: \n30: class SQLCompiler:\n31:     # Multiline ordering SQL clause may appear from RawSQL.\n32:     ordering_parts = _lazy_re_compile(\n33:         r\"^(.*)\\s(?:ASC|DESC).*\",\n34:         re.MULTILINE | re.DOTALL,\n35:     )\n36: \n37:     def __init__(self, query, connection, using, elide_empty=True):\n38:         self.query = query\n39:         self.connection = connection\n40:         self.using = using\n41:         # Some queries, e.g. coalesced aggregation, need to be executed even if\n42:         # they would return an empty result set.\n43:         self.elide_empty = elide_empty\n44:         self.quote_cache = {\"*\": \"*\"}\n45:         # The select, klass_info, and annotations are needed by QuerySet.iterator()\n46:         # these are set as a side-effect of executing the query. Note that we calculate\n47:         # separately a list of extra select columns needed for grammatical correctness\n48:         # of the query, but these columns are not included in self.select.\n49:         self.select = None\n50:         self.annotation_col_map = None\n51:         self.klass_info = None\n52:         self._meta_ordering = None\n53: \n54:     def __repr__(self):\n55:         return (\n56:             f\"<{self.__class__.__qualname__} \"\n57:             f\"model={self.query.model.__qualname__} \"\n58:             f\"connection={self.connection!r} using={self.using!r}>\"\n59:         )\n60: \n61:     def setup_query(self, with_col_aliases=False):\n62:         if all(self.query.alias_refcount[a] == 0 for a in self.query.alias_map):\n63:             self.query.get_initial_alias()\n64:         self.select, self.klass_info, self.annotation_col_map = self.get_select(\n65:             with_col_aliases=with_col_aliases,\n66:         )\n67:         self.col_count = len(self.select)\n68: \n69:     def pre_sql_setup(self, with_col_aliases=False):\n70:         \"\"\"\n71:         Do any necessary class setup immediately prior to producing SQL. This\n72:         is for things that can't necessarily be done in __init__ because we\n73:         might not have all the pieces in place at that time.\n74:         \"\"\"\n75:         self.setup_query(with_col_aliases=with_col_aliases)\n76:         order_by = self.get_order_by()\n77:         self.where, self.having, self.qualify = self.query.where.split_having_qualify(\n78:             must_group_by=self.query.group_by is not None\n79:         )\n80:         extra_select = self.get_extra_select(order_by, self.select)\n81:         self.has_extra_select = bool(extra_select)\n82:         group_by = self.get_group_by(self.select + extra_select, order_by)\n83:         return extra_select, order_by, group_by\n84: \n85:     def get_group_by(self, select, order_by):\n86:         \"\"\"\n87:         Return a list of 2-tuples of form (sql, params).\n88: \n89:         The logic of what exactly the GROUP BY clause contains is hard\n90:         to describe in other words than \"if it passes the test suite,\n91:         then it is correct\".\n92:         \"\"\"\n93:         # Some examples:\n94:         #     SomeModel.objects.annotate(Count('somecol'))\n95:         #     GROUP BY: all fields of the model\n96:         #\n97:         #    SomeModel.objects.values('name').annotate(Count('somecol'))\n98:         #    GROUP BY: name\n99:         #\n100:         #    SomeModel.objects.annotate(Count('somecol')).values('name')\n101:         #    GROUP BY: all cols of the model\n102:         #\n103:         #    SomeModel.objects.values('name', 'pk')\n104:         #    .annotate(Count('somecol')).values('pk')\n105:         #    GROUP BY: name, pk\n106:         #\n107:         #    SomeModel.objects.values('name').annotate(Count('somecol')).values('pk')\n108:         #    GROUP BY: name, pk\n109:         #\n110:         # In fact, the self.query.group_by is the minimal set to GROUP BY. It\n111:         # can't be ever restricted to a smaller set, but additional columns in\n112:         # HAVING, ORDER BY, and SELECT clauses are added to it. Unfortunately\n113:         # the end result is that it is impossible to force the query to have\n114:         # a chosen GROUP BY clause - you can almost do this by using the form:\n115:         #     .values(*wanted_cols).annotate(AnAggregate())\n116:         # but any later annotations, extra selects, values calls that\n117:         # refer some column outside of the wanted_cols, order_by, or even\n118:         # filter calls can alter the GROUP BY clause.\n119: \n120:         # The query.group_by is either None (no GROUP BY at all), True\n121:         # (group by select fields), or a list of expressions to be added\n122:         # to the group by.\n123:         if self.query.group_by is None:\n124:             return []\n125:         expressions = []\n126:         allows_group_by_refs = self.connection.features.allows_group_by_refs\n127:         if self.query.group_by is not True:\n128:             # If the group by is set to a list (by .values() call most likely),\n129:             # then we need to add everything in it to the GROUP BY clause.\n130:             # Backwards compatibility hack for setting query.group_by. Remove\n131:             # when  we have public API way of forcing the GROUP BY clause.\n132:             # Converts string references to expressions.\n133:             for expr in self.query.group_by:\n134:                 if not hasattr(expr, \"as_sql\"):\n135:                     expr = self.query.resolve_ref(expr)\n136:                 if not allows_group_by_refs and isinstance(expr, Ref):\n137:                     expr = expr.source\n138:                 expressions.append(expr)\n139:         # Note that even if the group_by is set, it is only the minimal\n140:         # set to group by. So, we need to add cols in select, order_by, and\n141:         # having into the select in any case.\n142:         ref_sources = {expr.source for expr in expressions if isinstance(expr, Ref)}\n143:         aliased_exprs = {}\n144:         for expr, _, alias in select:\n145:             # Skip members of the select clause that are already included\n146:             # by reference.\n147:             if expr in ref_sources:\n148:                 continue\n149:             if alias:\n150:                 aliased_exprs[expr] = alias\n151:             cols = expr.get_group_by_cols()\n152:             for col in cols:\n153:                 expressions.append(col)\n154:         if not self._meta_ordering:\n155:             for expr, (sql, params, is_ref) in order_by:\n156:                 # Skip references to the SELECT clause, as all expressions in\n157:                 # the SELECT clause are already part of the GROUP BY.\n158:                 if not is_ref:\n159:                     expressions.extend(expr.get_group_by_cols())\n160:         having_group_by = self.having.get_group_by_cols() if self.having else ()\n161:         for expr in having_group_by:\n162:             expressions.append(expr)\n163:         result = []\n164:         seen = set()\n165:         expressions = self.collapse_group_by(expressions, having_group_by)\n166: \n167:         for expr in expressions:\n168:             if allows_group_by_refs and (alias := aliased_exprs.get(expr)):\n169:                 expr = Ref(alias, expr)\n170:             try:\n171:                 sql, params = self.compile(expr)\n172:             except (EmptyResultSet, FullResultSet):\n173:                 continue\n174:             sql, params = expr.select_format(self, sql, params)\n175:             params_hash = make_hashable(params)\n176:             if (sql, params_hash) not in seen:\n177:                 result.append((sql, params))\n178:                 seen.add((sql, params_hash))\n179:         return result\n180: \n181:     def collapse_group_by(self, expressions, having):\n182:         # If the database supports group by functional dependence reduction,\n183:         # then the expressions can be reduced to the set of selected table\n184:         # primary keys as all other columns are functionally dependent on them.\n185:         if self.connection.features.allows_group_by_selected_pks:\n186:             # Filter out all expressions associated with a table's primary key\n187:             # present in the grouped columns. This is done by identifying all\n188:             # tables that have their primary key included in the grouped\n189:             # columns and removing non-primary key columns referring to them.\n190:             # Unmanaged models are excluded because they could be representing\n191:             # database views on which the optimization might not be allowed.\n192:             pks = {\n193:                 expr\n194:                 for expr in expressions\n195:                 if (\n196:                     hasattr(expr, \"target\")\n197:                     and expr.target.primary_key\n198:                     and self.connection.features.allows_group_by_selected_pks_on_model(\n199:                         expr.target.model\n200:                     )\n201:                 )\n202:             }\n203:             aliases = {expr.alias for expr in pks}\n204:             expressions = [\n205:                 expr\n206:                 for expr in expressions\n207:                 if expr in pks\n208:                 or expr in having\n209:                 or getattr(expr, \"alias\", None) not in aliases\n210:             ]\n211:         return expressions\n212: \n213:     def get_select(self, with_col_aliases=False):\n214:         \"\"\"\n215:         Return three values:\n216:         - a list of 3-tuples of (expression, (sql, params), alias)\n217:         - a klass_info structure,\n218:         - a dictionary of annotations\n219: \n220:         The (sql, params) is what the expression will produce, and alias is the\n221:         \"AS alias\" for the column (possibly None).\n222: \n223:         The klass_info structure contains the following information:\n224:         - The base model of the query.\n225:         - Which columns for that model are present in the query (by\n226:           position of the select clause).\n227:         - related_klass_infos: [f, klass_info] to descent into\n228: \n229:         The annotations is a dictionary of {'attname': column position} values.\n230:         \"\"\"\n231:         select = []\n232:         klass_info = None\n233:         annotations = {}\n234:         select_idx = 0\n235:         for alias, (sql, params) in self.query.extra_select.items():\n236:             annotations[alias] = select_idx\n237:             select.append((RawSQL(sql, params), alias))\n238:             select_idx += 1\n239:         assert not (self.query.select and self.query.default_cols)\n240:         select_mask = self.query.get_select_mask()\n241:         if self.query.default_cols:\n242:             cols = self.get_default_columns(select_mask)\n243:         else:\n244:             # self.query.select is a special case. These columns never go to\n245:             # any model.\n246:             cols = self.query.select\n247:         if cols:\n248:             select_list = []\n249:             for col in cols:\n250:                 select_list.append(select_idx)\n251:                 select.append((col, None))\n252:                 select_idx += 1\n253:             klass_info = {\n254:                 \"model\": self.query.model,\n255:                 \"select_fields\": select_list,\n256:             }\n257:         for alias, annotation in self.query.annotation_select.items():\n258:             annotations[alias] = select_idx\n259:             select.append((annotation, alias))\n260:             select_idx += 1\n261: \n262:         if self.query.select_related:\n263:             related_klass_infos = self.get_related_selections(select, select_mask)\n264:             klass_info[\"related_klass_infos\"] = related_klass_infos\n265: \n266:             def get_select_from_parent(klass_info):\n267:                 for ki in klass_info[\"related_klass_infos\"]:\n268:                     if ki[\"from_parent\"]:\n269:                         ki[\"select_fields\"] = (\n270:                             klass_info[\"select_fields\"] + ki[\"select_fields\"]\n271:                         )\n272:                     get_select_from_parent(ki)\n273: \n274:             get_select_from_parent(klass_info)\n275: \n276:         ret = []\n277:         col_idx = 1\n278:         for col, alias in select:\n279:             try:\n280:                 sql, params = self.compile(col)\n281:             except EmptyResultSet:\n282:                 empty_result_set_value = getattr(\n283:                     col, \"empty_result_set_value\", NotImplemented\n284:                 )\n285:                 if empty_result_set_value is NotImplemented:\n286:                     # Select a predicate that's always False.\n287:                     sql, params = \"0\", ()\n288:                 else:\n289:                     sql, params = self.compile(Value(empty_result_set_value))\n290:             except FullResultSet:\n291:                 sql, params = self.compile(Value(True))\n292:             else:\n293:                 sql, params = col.select_format(self, sql, params)\n294:             if alias is None and with_col_aliases:\n295:                 alias = f\"col{col_idx}\"\n296:                 col_idx += 1\n297:             ret.append((col, (sql, params), alias))\n298:         return ret, klass_info, annotations\n299: \n300:     def _order_by_pairs(self):\n301:         if self.query.extra_order_by:\n302:             ordering = self.query.extra_order_by\n303:         elif not self.query.default_ordering:\n304:             ordering = self.query.order_by\n305:         elif self.query.order_by:\n306:             ordering = self.query.order_by\n307:         elif (meta := self.query.get_meta()) and meta.ordering:\n308:             ordering = meta.ordering\n309:             self._meta_ordering = ordering\n310:         else:\n311:             ordering = []\n312:         if self.query.standard_ordering:\n313:             default_order, _ = ORDER_DIR[\"ASC\"]\n314:         else:\n315:             default_order, _ = ORDER_DIR[\"DESC\"]\n316: \n317:         for field in ordering:\n318:             if hasattr(field, \"resolve_expression\"):\n319:                 if isinstance(field, Value):\n320:                     # output_field must be resolved for constants.\n321:                     field = Cast(field, field.output_field)\n322:                 if not isinstance(field, OrderBy):\n323:                     field = field.asc()\n324:                 if not self.query.standard_ordering:\n325:                     field = field.copy()\n326:                     field.reverse_ordering()\n327:                 if isinstance(field.expression, F) and (\n328:                     annotation := self.query.annotation_select.get(\n329:                         field.expression.name\n330:                     )\n331:                 ):\n332:                     field.expression = Ref(field.expression.name, annotation)\n333:                 yield field, isinstance(field.expression, Ref)\n334:                 continue\n335:             if field == \"?\":  # random\n336:                 yield OrderBy(Random()), False\n337:                 continue\n338: \n339:             col, order = get_order_dir(field, default_order)\n340:             descending = order == \"DESC\"\n341: \n342:             if col in self.query.annotation_select:\n343:                 # Reference to expression in SELECT clause\n344:                 yield (\n345:                     OrderBy(\n346:                         Ref(col, self.query.annotation_select[col]),\n347:                         descending=descending,\n348:                     ),\n349:                     True,\n350:                 )\n351:                 continue\n352:             if col in self.query.annotations:\n353:                 # References to an expression which is masked out of the SELECT\n354:                 # clause.\n355:                 if self.query.combinator and self.select:\n356:                     # Don't use the resolved annotation because other\n357:                     # combinated queries might define it differently.\n358:                     expr = F(col)\n359:                 else:\n360:                     expr = self.query.annotations[col]\n361:                     if isinstance(expr, Value):\n362:                         # output_field must be resolved for constants.\n363:                         expr = Cast(expr, expr.output_field)\n364:                 yield OrderBy(expr, descending=descending), False\n365:                 continue\n366: \n367:             if \".\" in field:\n368:                 # This came in through an extra(order_by=...) addition. Pass it\n369:                 # on verbatim.\n370:                 table, col = col.split(\".\", 1)\n371:                 yield (\n372:                     OrderBy(\n373:                         RawSQL(\n374:                             \"%s.%s\" % (self.quote_name_unless_alias(table), col), []\n375:                         ),\n376:                         descending=descending,\n377:                     ),\n378:                     False,\n379:                 )\n380:                 continue\n381: \n382:             if self.query.extra and col in self.query.extra:\n383:                 if col in self.query.extra_select:\n384:                     yield (\n385:                         OrderBy(\n386:                             Ref(col, RawSQL(*self.query.extra[col])),\n387:                             descending=descending,\n388:                         ),\n389:                         True,\n390:                     )\n391:                 else:\n392:                     yield (\n393:                         OrderBy(RawSQL(*self.query.extra[col]), descending=descending),\n394:                         False,\n395:                     )\n396:             else:\n397:                 if self.query.combinator and self.select:\n398:                     # Don't use the first model's field because other\n399:                     # combinated queries might define it differently.\n400:                     yield OrderBy(F(col), descending=descending), False\n401:                 else:\n402:                     # 'col' is of the form 'field' or 'field1__field2' or\n403:                     # '-field1__field2__field', etc.\n404:                     yield from self.find_ordering_name(\n405:                         field,\n406:                         self.query.get_meta(),\n407:                         default_order=default_order,\n408:                     )\n409: \n410:     def get_order_by(self):\n411:         \"\"\"\n412:         Return a list of 2-tuples of the form (expr, (sql, params, is_ref)) for\n413:         the ORDER BY clause.\n414: \n415:         The order_by clause can alter the select clause (for example it can add\n416:         aliases to clauses that do not yet have one, or it can add totally new\n417:         select clauses).\n418:         \"\"\"\n419:         result = []\n420:         seen = set()\n421:         for expr, is_ref in self._order_by_pairs():\n422:             resolved = expr.resolve_expression(self.query, allow_joins=True, reuse=None)\n423:             if not is_ref and self.query.combinator and self.select:\n424:                 src = resolved.expression\n425:                 expr_src = expr.expression\n426:                 for sel_expr, _, col_alias in self.select:\n427:                     if src == sel_expr:\n428:                         # When values() is used the exact alias must be used to\n429:                         # reference annotations.\n430:                         if (\n431:                             self.query.has_select_fields\n432:                             and col_alias in self.query.annotation_select\n433:                             and not (\n434:                                 isinstance(expr_src, F) and col_alias == expr_src.name\n435:                             )\n436:                         ):\n437:                             continue\n438:                         resolved.set_source_expressions(\n439:                             [Ref(col_alias if col_alias else src.target.column, src)]\n440:                         )\n441:                         break\n442:                 else:\n443:                     # Add column used in ORDER BY clause to the selected\n444:                     # columns and to each combined query.\n445:                     order_by_idx = len(self.query.select) + 1\n446:                     col_alias = f\"__orderbycol{order_by_idx}\"\n447:                     for q in self.query.combined_queries:\n448:                         # If fields were explicitly selected through values()\n449:                         # combined queries cannot be augmented.\n450:                         if q.has_select_fields:\n451:                             raise DatabaseError(\n452:                                 \"ORDER BY term does not match any column in \"\n453:                                 \"the result set.\"\n454:                             )\n455:                         q.add_annotation(expr_src, col_alias)\n456:                     self.query.add_select_col(resolved, col_alias)\n457:                     resolved.set_source_expressions([Ref(col_alias, src)])\n458:             sql, params = self.compile(resolved)\n459:             # Don't add the same column twice, but the order direction is\n460:             # not taken into account so we strip it. When this entire method\n461:             # is refactored into expressions, then we can check each part as we\n462:             # generate it.\n463:             without_ordering = self.ordering_parts.search(sql)[1]\n464:             params_hash = make_hashable(params)\n465:             if (without_ordering, params_hash) in seen:\n466:                 continue\n467:             seen.add((without_ordering, params_hash))\n468:             result.append((resolved, (sql, params, is_ref)))\n469:         return result\n470: \n471:     def get_extra_select(self, order_by, select):\n472:         extra_select = []\n473:         if self.query.distinct and not self.query.distinct_fields:\n474:             select_sql = [t[1] for t in select]\n475:             for expr, (sql, params, is_ref) in order_by:\n476:                 without_ordering = self.ordering_parts.search(sql)[1]\n477:                 if not is_ref and (without_ordering, params) not in select_sql:\n478:                     extra_select.append((expr, (without_ordering, params), None))\n479:         return extra_select\n480: \n481:     def quote_name_unless_alias(self, name):\n482:         \"\"\"\n483:         A wrapper around connection.ops.quote_name that doesn't quote aliases\n484:         for table names. This avoids problems with some SQL dialects that treat\n485:         quoted strings specially (e.g. PostgreSQL).\n486:         \"\"\"\n487:         if name in self.quote_cache:\n488:             return self.quote_cache[name]\n489:         if (\n490:             (name in self.query.alias_map and name not in self.query.table_map)\n491:             or name in self.query.extra_select\n492:             or (\n493:                 self.query.external_aliases.get(name)\n494:                 and name not in self.query.table_map\n495:             )\n496:         ):\n497:             self.quote_cache[name] = name\n498:             return name\n499:         r = self.connection.ops.quote_name(name)\n500:         self.quote_cache[name] = r\n501:         return r\n502: \n503:     def compile(self, node):\n504:         vendor_impl = getattr(node, \"as_\" + self.connection.vendor, None)\n505:         if vendor_impl:\n506:             sql, params = vendor_impl(self, self.connection)\n507:         else:\n508:             sql, params = node.as_sql(self, self.connection)\n509:         return sql, params\n510: \n511:     def get_combinator_sql(self, combinator, all):\n512:         features = self.connection.features\n513:         compilers = [\n514:             query.get_compiler(self.using, self.connection, self.elide_empty)\n515:             for query in self.query.combined_queries\n516:         ]\n517:         if not features.supports_slicing_ordering_in_compound:\n518:             for compiler in compilers:\n519:                 if compiler.query.is_sliced:\n520:                     raise DatabaseError(\n521:                         \"LIMIT/OFFSET not allowed in subqueries of compound statements.\"\n522:                     )\n523:                 if compiler.get_order_by():\n524:                     raise DatabaseError(\n525:                         \"ORDER BY not allowed in subqueries of compound statements.\"\n526:                     )\n527:         elif self.query.is_sliced and combinator == \"union\":\n528:             limit = (self.query.low_mark, self.query.high_mark)\n529:             for compiler in compilers:\n530:                 # A sliced union cannot have its parts elided as some of them\n531:                 # might be sliced as well and in the event where only a single\n532:                 # part produces a non-empty resultset it might be impossible to\n533:                 # generate valid SQL.\n534:                 compiler.elide_empty = False\n535:                 if not compiler.query.is_sliced:\n536:                     compiler.query.set_limits(*limit)\n537:         parts = ()\n538:         for compiler in compilers:\n539:             try:\n540:                 # If the columns list is limited, then all combined queries\n541:                 # must have the same columns list. Set the selects defined on\n542:                 # the query on all combined queries, if not already set.\n543:                 if not compiler.query.values_select and self.query.values_select:\n544:                     compiler.query = compiler.query.clone()\n545:                     compiler.query.set_values(\n546:                         (\n547:                             *self.query.extra_select,\n548:                             *self.query.values_select,\n549:                             *self.query.annotation_select,\n550:                         )\n551:                     )\n552:                 part_sql, part_args = compiler.as_sql(with_col_aliases=True)\n553:                 if compiler.query.combinator:\n554:                     # Wrap in a subquery if wrapping in parentheses isn't\n555:                     # supported.\n556:                     if not features.supports_parentheses_in_compound:\n557:                         part_sql = \"SELECT * FROM ({})\".format(part_sql)\n558:                     # Add parentheses when combining with compound query if not\n559:                     # already added for all compound queries.\n560:                     elif (\n561:                         self.query.subquery\n562:                         or not features.supports_slicing_ordering_in_compound\n563:                     ):\n564:                         part_sql = \"({})\".format(part_sql)\n565:                 elif (\n566:                     self.query.subquery\n567:                     and features.supports_slicing_ordering_in_compound\n568:                 ):\n569:                     part_sql = \"({})\".format(part_sql)\n570:                 parts += ((part_sql, part_args),)\n571:             except EmptyResultSet:\n572:                 # Omit the empty queryset with UNION and with DIFFERENCE if the\n573:                 # first queryset is nonempty.\n574:                 if combinator == \"union\" or (combinator == \"difference\" and parts):\n575:                     continue\n576:                 raise\n577:         if not parts:\n578:             raise EmptyResultSet\n579:         combinator_sql = self.connection.ops.set_operators[combinator]\n580:         if all and combinator == \"union\":\n581:             combinator_sql += \" ALL\"\n582:         braces = \"{}\"\n583:         if not self.query.subquery and features.supports_slicing_ordering_in_compound:\n584:             braces = \"({})\"\n585:         sql_parts, args_parts = zip(\n586:             *((braces.format(sql), args) for sql, args in parts)\n587:         )\n588:         result = [\" {} \".format(combinator_sql).join(sql_parts)]\n589:         params = []\n590:         for part in args_parts:\n591:             params.extend(part)\n592:         return result, params\n593: \n594:     def get_qualify_sql(self):\n595:         where_parts = []\n596:         if self.where:\n597:             where_parts.append(self.where)\n598:         if self.having:\n599:             where_parts.append(self.having)\n600:         inner_query = self.query.clone()\n601:         inner_query.subquery = True\n602:         inner_query.where = inner_query.where.__class__(where_parts)\n603:         # Augment the inner query with any window function references that\n604:         # might have been masked via values() and alias(). If any masked\n605:         # aliases are added they'll be masked again to avoid fetching\n606:         # the data in the `if qual_aliases` branch below.\n607:         select = {\n608:             expr: alias for expr, _, alias in self.get_select(with_col_aliases=True)[0]\n609:         }\n610:         select_aliases = set(select.values())\n611:         qual_aliases = set()\n612:         replacements = {}\n613: \n614:         def collect_replacements(expressions):\n615:             while expressions:\n616:                 expr = expressions.pop()\n617:                 if expr in replacements:\n618:                     continue\n619:                 elif select_alias := select.get(expr):\n620:                     replacements[expr] = select_alias\n621:                 elif isinstance(expr, Lookup):\n622:                     expressions.extend(expr.get_source_expressions())\n623:                 elif isinstance(expr, Ref):\n624:                     if expr.refs not in select_aliases:\n625:                         expressions.extend(expr.get_source_expressions())\n626:                 else:\n627:                     num_qual_alias = len(qual_aliases)\n628:                     select_alias = f\"qual{num_qual_alias}\"\n629:                     qual_aliases.add(select_alias)\n630:                     inner_query.add_annotation(expr, select_alias)\n631:                     replacements[expr] = select_alias\n632: \n633:         collect_replacements(list(self.qualify.leaves()))\n634:         self.qualify = self.qualify.replace_expressions(\n635:             {expr: Ref(alias, expr) for expr, alias in replacements.items()}\n636:         )\n637:         order_by = []\n638:         for order_by_expr, *_ in self.get_order_by():\n639:             collect_replacements(order_by_expr.get_source_expressions())\n640:             order_by.append(\n641:                 order_by_expr.replace_expressions(\n642:                     {expr: Ref(alias, expr) for expr, alias in replacements.items()}\n643:                 )\n644:             )\n645:         inner_query_compiler = inner_query.get_compiler(\n646:             self.using, elide_empty=self.elide_empty\n647:         )\n648:         inner_sql, inner_params = inner_query_compiler.as_sql(\n649:             # The limits must be applied to the outer query to avoid pruning\n650:             # results too eagerly.\n651:             with_limits=False,\n652:             # Force unique aliasing of selected columns to avoid collisions\n653:             # and make rhs predicates referencing easier.\n654:             with_col_aliases=True,\n655:         )\n656:         qualify_sql, qualify_params = self.compile(self.qualify)\n657:         result = [\n658:             \"SELECT * FROM (\",\n659:             inner_sql,\n660:             \")\",\n661:             self.connection.ops.quote_name(\"qualify\"),\n662:             \"WHERE\",\n663:             qualify_sql,\n664:         ]\n665:         if qual_aliases:\n666:             # If some select aliases were unmasked for filtering purposes they\n667:             # must be masked back.\n668:             cols = [self.connection.ops.quote_name(alias) for alias in select.values()]\n669:             result = [\n670:                 \"SELECT\",\n671:                 \", \".join(cols),\n672:                 \"FROM (\",\n673:                 *result,\n674:                 \")\",\n675:                 self.connection.ops.quote_name(\"qualify_mask\"),\n676:             ]\n677:         params = list(inner_params) + qualify_params\n678:         # As the SQL spec is unclear on whether or not derived tables\n679:         # ordering must propagate it has to be explicitly repeated on the\n680:         # outer-most query to ensure it's preserved.\n681:         if order_by:\n682:             ordering_sqls = []\n683:             for ordering in order_by:\n684:                 ordering_sql, ordering_params = self.compile(ordering)\n685:                 ordering_sqls.append(ordering_sql)\n686:                 params.extend(ordering_params)\n687:             result.extend([\"ORDER BY\", \", \".join(ordering_sqls)])\n688:         return result, params\n689: \n690:     def as_sql(self, with_limits=True, with_col_aliases=False):\n691:         \"\"\"\n692:         Create the SQL for this query. Return the SQL string and list of\n693:         parameters.\n694: \n695:         If 'with_limits' is False, any limit/offset information is not included\n696:         in the query.\n697:         \"\"\"\n698:         refcounts_before = self.query.alias_refcount.copy()\n699:         try:\n700:             combinator = self.query.combinator\n701:             extra_select, order_by, group_by = self.pre_sql_setup(\n702:                 with_col_aliases=with_col_aliases or bool(combinator),\n703:             )\n704:             for_update_part = None\n705:             # Is a LIMIT/OFFSET clause needed?\n706:             with_limit_offset = with_limits and self.query.is_sliced\n707:             combinator = self.query.combinator\n708:             features = self.connection.features\n709:             if combinator:\n710:                 if not getattr(features, \"supports_select_{}\".format(combinator)):\n711:                     raise NotSupportedError(\n712:                         \"{} is not supported on this database backend.\".format(\n713:                             combinator\n714:                         )\n715:                     )\n716:                 result, params = self.get_combinator_sql(\n717:                     combinator, self.query.combinator_all\n718:                 )\n719:             elif self.qualify:\n720:                 result, params = self.get_qualify_sql()\n721:                 order_by = None\n722:             else:\n723:                 distinct_fields, distinct_params = self.get_distinct()\n724:                 # This must come after 'select', 'ordering', and 'distinct'\n725:                 # (see docstring of get_from_clause() for details).\n726:                 from_, f_params = self.get_from_clause()\n727:                 try:\n728:                     where, w_params = (\n729:                         self.compile(self.where) if self.where is not None else (\"\", [])\n730:                     )\n731:                 except EmptyResultSet:\n732:                     if self.elide_empty:\n733:                         raise\n734:                     # Use a predicate that's always False.\n735:                     where, w_params = \"0 = 1\", []\n736:                 except FullResultSet:\n737:                     where, w_params = \"\", []\n738:                 try:\n739:                     having, h_params = (\n740:                         self.compile(self.having)\n741:                         if self.having is not None\n742:                         else (\"\", [])\n743:                     )\n744:                 except FullResultSet:\n745:                     having, h_params = \"\", []\n746:                 result = [\"SELECT\"]\n747:                 params = []\n748: \n749:                 if self.query.distinct:\n750:                     distinct_result, distinct_params = self.connection.ops.distinct_sql(\n751:                         distinct_fields,\n752:                         distinct_params,\n753:                     )\n754:                     result += distinct_result\n755:                     params += distinct_params\n756: \n757:                 out_cols = []\n758:                 for _, (s_sql, s_params), alias in self.select + extra_select:\n759:                     if alias:\n760:                         s_sql = \"%s AS %s\" % (\n761:                             s_sql,\n762:                             self.connection.ops.quote_name(alias),\n763:                         )\n764:                     params.extend(s_params)\n765:                     out_cols.append(s_sql)\n766: \n767:                 result += [\", \".join(out_cols)]\n768:                 if from_:\n769:                     result += [\"FROM\", *from_]\n770:                 elif self.connection.features.bare_select_suffix:\n771:                     result += [self.connection.features.bare_select_suffix]\n772:                 params.extend(f_params)\n773: \n774:                 if self.query.select_for_update and features.has_select_for_update:\n775:                     if (\n776:                         self.connection.get_autocommit()\n777:                         # Don't raise an exception when database doesn't\n778:                         # support transactions, as it's a noop.\n779:                         and features.supports_transactions\n780:                     ):\n781:                         raise TransactionManagementError(\n782:                             \"select_for_update cannot be used outside of a transaction.\"\n783:                         )\n784: \n785:                     if (\n786:                         with_limit_offset\n787:                         and not features.supports_select_for_update_with_limit\n788:                     ):\n789:                         raise NotSupportedError(\n790:                             \"LIMIT/OFFSET is not supported with \"\n791:                             \"select_for_update on this database backend.\"\n792:                         )\n793:                     nowait = self.query.select_for_update_nowait\n794:                     skip_locked = self.query.select_for_update_skip_locked\n795:                     of = self.query.select_for_update_of\n796:                     no_key = self.query.select_for_no_key_update\n797:                     # If it's a NOWAIT/SKIP LOCKED/OF/NO KEY query but the\n798:                     # backend doesn't support it, raise NotSupportedError to\n799:                     # prevent a possible deadlock.\n800:                     if nowait and not features.has_select_for_update_nowait:\n801:                         raise NotSupportedError(\n802:                             \"NOWAIT is not supported on this database backend.\"\n803:                         )\n804:                     elif skip_locked and not features.has_select_for_update_skip_locked:\n805:                         raise NotSupportedError(\n806:                             \"SKIP LOCKED is not supported on this database backend.\"\n807:                         )\n808:                     elif of and not features.has_select_for_update_of:\n809:                         raise NotSupportedError(\n810:                             \"FOR UPDATE OF is not supported on this database backend.\"\n811:                         )\n812:                     elif no_key and not features.has_select_for_no_key_update:\n813:                         raise NotSupportedError(\n814:                             \"FOR NO KEY UPDATE is not supported on this \"\n815:                             \"database backend.\"\n816:                         )\n817:                     for_update_part = self.connection.ops.for_update_sql(\n818:                         nowait=nowait,\n819:                         skip_locked=skip_locked,\n820:                         of=self.get_select_for_update_of_arguments(),\n821:                         no_key=no_key,\n822:                     )\n823: \n824:                 if for_update_part and features.for_update_after_from:\n825:                     result.append(for_update_part)\n826: \n827:                 if where:\n828:                     result.append(\"WHERE %s\" % where)\n829:                     params.extend(w_params)\n830: \n831:                 grouping = []\n832:                 for g_sql, g_params in group_by:\n833:                     grouping.append(g_sql)\n834:                     params.extend(g_params)\n835:                 if grouping:\n836:                     if distinct_fields:\n837:                         raise NotImplementedError(\n838:                             \"annotate() + distinct(fields) is not implemented.\"\n839:                         )\n840:                     order_by = order_by or self.connection.ops.force_no_ordering()\n841:                     result.append(\"GROUP BY %s\" % \", \".join(grouping))\n842:                     if self._meta_ordering:\n843:                         order_by = None\n844:                 if having:\n845:                     result.append(\"HAVING %s\" % having)\n846:                     params.extend(h_params)\n847: \n848:             if self.query.explain_info:\n849:                 result.insert(\n850:                     0,\n851:                     self.connection.ops.explain_query_prefix(\n852:                         self.query.explain_info.format,\n853:                         **self.query.explain_info.options,\n854:                     ),\n855:                 )\n856: \n857:             if order_by:\n858:                 ordering = []\n859:                 for _, (o_sql, o_params, _) in order_by:\n860:                     ordering.append(o_sql)\n861:                     params.extend(o_params)\n862:                 order_by_sql = \"ORDER BY %s\" % \", \".join(ordering)\n863:                 if combinator and features.requires_compound_order_by_subquery:\n864:                     result = [\"SELECT * FROM (\", *result, \")\", order_by_sql]\n865:                 else:\n866:                     result.append(order_by_sql)\n867: \n868:             if with_limit_offset:\n869:                 result.append(\n870:                     self.connection.ops.limit_offset_sql(\n871:                         self.query.low_mark, self.query.high_mark\n872:                     )\n873:                 )\n874: \n875:             if for_update_part and not features.for_update_after_from:\n876:                 result.append(for_update_part)\n877: \n878:             if self.query.subquery and extra_select:\n879:                 # If the query is used as a subquery, the extra selects would\n880:                 # result in more columns than the left-hand side expression is\n881:                 # expecting. This can happen when a subquery uses a combination\n882:                 # of order_by() and distinct(), forcing the ordering expressions\n883:                 # to be selected as well. Wrap the query in another subquery\n884:                 # to exclude extraneous selects.\n885:                 sub_selects = []\n886:                 sub_params = []\n887:                 for index, (select, _, alias) in enumerate(self.select, start=1):\n888:                     if alias:\n889:                         sub_selects.append(\n890:                             \"%s.%s\"\n891:                             % (\n892:                                 self.connection.ops.quote_name(\"subquery\"),\n893:                                 self.connection.ops.quote_name(alias),\n894:                             )\n895:                         )\n896:                     else:\n897:                         select_clone = select.relabeled_clone(\n898:                             {select.alias: \"subquery\"}\n899:                         )\n900:                         subselect, subparams = select_clone.as_sql(\n901:                             self, self.connection\n902:                         )\n903:                         sub_selects.append(subselect)\n904:                         sub_params.extend(subparams)\n905:                 return \"SELECT %s FROM (%s) subquery\" % (\n906:                     \", \".join(sub_selects),\n907:                     \" \".join(result),\n908:                 ), tuple(sub_params + params)\n909: \n910:             return \" \".join(result), tuple(params)\n911:         finally:\n912:             # Finally do cleanup - get rid of the joins we created above.\n913:             self.query.reset_refcounts(refcounts_before)\n914: \n915:     def get_default_columns(\n916:         self, select_mask, start_alias=None, opts=None, from_parent=None\n917:     ):\n918:         \"\"\"\n919:         Compute the default columns for selecting every field in the base\n920:         model. Will sometimes be called to pull in related models (e.g. via\n921:         select_related), in which case \"opts\" and \"start_alias\" will be given\n922:         to provide a starting point for the traversal.\n923: \n924:         Return a list of strings, quoted appropriately for use in SQL\n925:         directly, as well as a set of aliases used in the select statement (if\n926:         'as_pairs' is True, return a list of (alias, col_name) pairs instead\n927:         of strings as the first component and None as the second component).\n928:         \"\"\"\n929:         result = []\n930:         if opts is None:\n931:             if (opts := self.query.get_meta()) is None:\n932:                 return result\n933:         start_alias = start_alias or self.query.get_initial_alias()\n934:         # The 'seen_models' is used to optimize checking the needed parent\n935:         # alias for a given field. This also includes None -> start_alias to\n936:         # be used by local fields.\n937:         seen_models = {None: start_alias}\n938: \n939:         for field in opts.concrete_fields:\n940:             model = field.model._meta.concrete_model\n941:             # A proxy model will have a different model and concrete_model. We\n942:             # will assign None if the field belongs to this model.\n943:             if model == opts.model:\n944:                 model = None\n945:             if (\n946:                 from_parent\n947:                 and model is not None\n948:                 and issubclass(\n949:                     from_parent._meta.concrete_model, model._meta.concrete_model\n950:                 )\n951:             ):\n952:                 # Avoid loading data for already loaded parents.\n953:                 # We end up here in the case select_related() resolution\n954:                 # proceeds from parent model to child model. In that case the\n955:                 # parent model data is already present in the SELECT clause,\n956:                 # and we want to avoid reloading the same data again.\n957:                 continue\n958:             if select_mask and field not in select_mask:\n959:                 continue\n960:             alias = self.query.join_parent_model(opts, model, start_alias, seen_models)\n961:             column = field.get_col(alias)\n962:             result.append(column)\n963:         return result\n964: \n965:     def get_distinct(self):\n966:         \"\"\"\n967:         Return a quoted list of fields to use in DISTINCT ON part of the query.\n968: \n969:         This method can alter the tables in the query, and thus it must be\n970:         called before get_from_clause().\n971:         \"\"\"\n972:         result = []\n973:         params = []\n974:         opts = self.query.get_meta()\n975: \n976:         for name in self.query.distinct_fields:\n977:             parts = name.split(LOOKUP_SEP)\n978:             _, targets, alias, joins, path, _, transform_function = self._setup_joins(\n979:                 parts, opts, None\n980:             )\n981:             targets, alias, _ = self.query.trim_joins(targets, joins, path)\n982:             for target in targets:\n983:                 if name in self.query.annotation_select:\n984:                     result.append(self.connection.ops.quote_name(name))\n985:                 else:\n986:                     r, p = self.compile(transform_function(target, alias))\n987:                     result.append(r)\n988:                     params.append(p)\n989:         return result, params\n990: \n991:     def find_ordering_name(\n992:         self, name, opts, alias=None, default_order=\"ASC\", already_seen=None\n993:     ):\n994:         \"\"\"\n995:         Return the table alias (the name might be ambiguous, the alias will\n996:         not be) and column name for ordering by the given 'name' parameter.\n997:         The 'name' is of the form 'field1__field2__...__fieldN'.\n998:         \"\"\"\n999:         name, order = get_order_dir(name, default_order)\n1000:         descending = order == \"DESC\"\n1001:         pieces = name.split(LOOKUP_SEP)\n1002:         (\n1003:             field,\n1004:             targets,\n1005:             alias,\n1006:             joins,\n1007:             path,\n1008:             opts,\n1009:             transform_function,\n1010:         ) = self._setup_joins(pieces, opts, alias)\n1011: \n1012:         # If we get to this point and the field is a relation to another model,\n1013:         # append the default ordering for that model unless it is the pk\n1014:         # shortcut or the attribute name of the field that is specified or\n1015:         # there are transforms to process.\n1016:         if (\n1017:             field.is_relation\n1018:             and opts.ordering\n1019:             and getattr(field, \"attname\", None) != pieces[-1]\n1020:             and name != \"pk\"\n1021:             and not getattr(transform_function, \"has_transforms\", False)\n1022:         ):\n1023:             # Firstly, avoid infinite loops.\n1024:             already_seen = already_seen or set()\n1025:             join_tuple = tuple(\n1026:                 getattr(self.query.alias_map[j], \"join_cols\", None) for j in joins\n1027:             )\n1028:             if join_tuple in already_seen:\n1029:                 raise FieldError(\"Infinite loop caused by ordering.\")\n1030:             already_seen.add(join_tuple)\n1031: \n1032:             results = []\n1033:             for item in opts.ordering:\n1034:                 if hasattr(item, \"resolve_expression\") and not isinstance(\n1035:                     item, OrderBy\n1036:                 ):\n1037:                     item = item.desc() if descending else item.asc()\n1038:                 if isinstance(item, OrderBy):\n1039:                     results.append(\n1040:                         (item.prefix_references(f\"{name}{LOOKUP_SEP}\"), False)\n1041:                     )\n1042:                     continue\n1043:                 results.extend(\n1044:                     (expr.prefix_references(f\"{name}{LOOKUP_SEP}\"), is_ref)\n1045:                     for expr, is_ref in self.find_ordering_name(\n1046:                         item, opts, alias, order, already_seen\n1047:                     )\n1048:                 )\n1049:             return results\n1050:         targets, alias, _ = self.query.trim_joins(targets, joins, path)\n1051:         return [\n1052:             (OrderBy(transform_function(t, alias), descending=descending), False)\n1053:             for t in targets\n1054:         ]\n1055: \n1056:     def _setup_joins(self, pieces, opts, alias):\n1057:         \"\"\"\n1058:         Helper method for get_order_by() and get_distinct().\n1059: \n1060:         get_ordering() and get_distinct() must produce same target columns on\n1061:         same input, as the prefixes of get_ordering() and get_distinct() must\n1062:         match. Executing SQL where this is not true is an error.\n1063:         \"\"\"\n1064:         alias = alias or self.query.get_initial_alias()\n1065:         field, targets, opts, joins, path, transform_function = self.query.setup_joins(\n1066:             pieces, opts, alias\n1067:         )\n1068:         alias = joins[-1]\n1069:         return field, targets, alias, joins, path, opts, transform_function\n1070: \n1071:     def get_from_clause(self):\n1072:         \"\"\"\n1073:         Return a list of strings that are joined together to go after the\n1074:         \"FROM\" part of the query, as well as a list any extra parameters that\n1075:         need to be included. Subclasses, can override this to create a\n1076:         from-clause via a \"select\".\n1077: \n1078:         This should only be called after any SQL construction methods that\n1079:         might change the tables that are needed. This means the select columns,\n1080:         ordering, and distinct must be done first.\n1081:         \"\"\"\n1082:         result = []\n1083:         params = []\n1084:         for alias in tuple(self.query.alias_map):\n1085:             if not self.query.alias_refcount[alias]:\n1086:                 continue\n1087:             try:\n1088:                 from_clause = self.query.alias_map[alias]\n1089:             except KeyError:\n1090:                 # Extra tables can end up in self.tables, but not in the\n1091:                 # alias_map if they aren't in a join. That's OK. We skip them.\n1092:                 continue\n1093:             clause_sql, clause_params = self.compile(from_clause)\n1094:             result.append(clause_sql)\n1095:             params.extend(clause_params)\n1096:         for t in self.query.extra_tables:\n1097:             alias, _ = self.query.table_alias(t)\n1098:             # Only add the alias if it's not already present (the table_alias()\n1099:             # call increments the refcount, so an alias refcount of one means\n1100:             # this is the only reference).\n1101:             if (\n1102:                 alias not in self.query.alias_map\n1103:                 or self.query.alias_refcount[alias] == 1\n1104:             ):\n1105:                 result.append(\", %s\" % self.quote_name_unless_alias(alias))\n1106:         return result, params\n1107: \n1108:     def get_related_selections(\n1109:         self,\n1110:         select,\n1111:         select_mask,\n1112:         opts=None,\n1113:         root_alias=None,\n1114:         cur_depth=1,\n1115:         requested=None,\n1116:         restricted=None,\n1117:     ):\n1118:         \"\"\"\n1119:         Fill in the information needed for a select_related query. The current\n1120:         depth is measured as the number of connections away from the root model\n1121:         (for example, cur_depth=1 means we are looking at models with direct\n1122:         connections to the root model).\n1123:         \"\"\"\n1124: \n1125:         def _get_field_choices():\n1126:             direct_choices = (f.name for f in opts.fields if f.is_relation)\n1127:             reverse_choices = (\n1128:                 f.field.related_query_name()\n1129:                 for f in opts.related_objects\n1130:                 if f.field.unique\n1131:             )\n1132:             return chain(\n1133:                 direct_choices, reverse_choices, self.query._filtered_relations\n1134:             )\n1135: \n1136:         related_klass_infos = []\n1137:         if not restricted and cur_depth > self.query.max_depth:\n1138:             # We've recursed far enough; bail out.\n1139:             return related_klass_infos\n1140: \n1141:         if not opts:\n1142:             opts = self.query.get_meta()\n1143:             root_alias = self.query.get_initial_alias()\n1144: \n1145:         # Setup for the case when only particular related fields should be\n1146:         # included in the related selection.\n1147:         fields_found = set()\n1148:         if requested is None:\n1149:             restricted = isinstance(self.query.select_related, dict)\n1150:             if restricted:\n1151:                 requested = self.query.select_related\n1152: \n1153:         def get_related_klass_infos(klass_info, related_klass_infos):\n1154:             klass_info[\"related_klass_infos\"] = related_klass_infos\n1155: \n1156:         for f in opts.fields:\n1157:             fields_found.add(f.name)\n1158: \n1159:             if restricted:\n1160:                 next = requested.get(f.name, {})\n1161:                 if not f.is_relation:\n1162:                     # If a non-related field is used like a relation,\n1163:                     # or if a single non-relational field is given.\n1164:                     if next or f.name in requested:\n1165:                         raise FieldError(\n1166:                             \"Non-relational field given in select_related: '%s'. \"\n1167:                             \"Choices are: %s\"\n1168:                             % (\n1169:                                 f.name,\n1170:                                 \", \".join(_get_field_choices()) or \"(none)\",\n1171:                             )\n1172:                         )\n1173:             else:\n1174:                 next = False\n1175: \n1176:             if not select_related_descend(f, restricted, requested, select_mask):\n1177:                 continue\n1178:             related_select_mask = select_mask.get(f) or {}\n1179:             klass_info = {\n1180:                 \"model\": f.remote_field.model,\n1181:                 \"field\": f,\n1182:                 \"reverse\": False,\n1183:                 \"local_setter\": f.set_cached_value,\n1184:                 \"remote_setter\": f.remote_field.set_cached_value\n1185:                 if f.unique\n1186:                 else lambda x, y: None,\n1187:                 \"from_parent\": False,\n1188:             }\n1189:             related_klass_infos.append(klass_info)\n1190:             select_fields = []\n1191:             _, _, _, joins, _, _ = self.query.setup_joins([f.name], opts, root_alias)\n1192:             alias = joins[-1]\n1193:             columns = self.get_default_columns(\n1194:                 related_select_mask, start_alias=alias, opts=f.remote_field.model._meta\n1195:             )\n1196:             for col in columns:\n1197:                 select_fields.append(len(select))\n1198:                 select.append((col, None))\n1199:             klass_info[\"select_fields\"] = select_fields\n1200:             next_klass_infos = self.get_related_selections(\n1201:                 select,\n1202:                 related_select_mask,\n1203:                 f.remote_field.model._meta,\n1204:                 alias,\n1205:                 cur_depth + 1,\n1206:                 next,\n1207:                 restricted,\n1208:             )\n1209:             get_related_klass_infos(klass_info, next_klass_infos)\n1210: \n1211:         if restricted:\n1212:             related_fields = [\n1213:                 (o.field, o.related_model)\n1214:                 for o in opts.related_objects\n1215:                 if o.field.unique and not o.many_to_many\n1216:             ]\n1217:             for f, model in related_fields:\n1218:                 related_select_mask = select_mask.get(f) or {}\n1219:                 if not select_related_descend(\n1220:                     f, restricted, requested, related_select_mask, reverse=True\n1221:                 ):\n1222:                     continue\n1223: \n1224:                 related_field_name = f.related_query_name()\n1225:                 fields_found.add(related_field_name)\n1226: \n1227:                 join_info = self.query.setup_joins(\n1228:                     [related_field_name], opts, root_alias\n1229:                 )\n1230:                 alias = join_info.joins[-1]\n1231:                 from_parent = issubclass(model, opts.model) and model is not opts.model\n1232:                 klass_info = {\n1233:                     \"model\": model,\n1234:                     \"field\": f,\n1235:                     \"reverse\": True,\n1236:                     \"local_setter\": f.remote_field.set_cached_value,\n1237:                     \"remote_setter\": f.set_cached_value,\n1238:                     \"from_parent\": from_parent,\n1239:                 }\n1240:                 related_klass_infos.append(klass_info)\n1241:                 select_fields = []\n1242:                 columns = self.get_default_columns(\n1243:                     related_select_mask,\n1244:                     start_alias=alias,\n1245:                     opts=model._meta,\n1246:                     from_parent=opts.model,\n1247:                 )\n1248:                 for col in columns:\n1249:                     select_fields.append(len(select))\n1250:                     select.append((col, None))\n1251:                 klass_info[\"select_fields\"] = select_fields\n1252:                 next = requested.get(f.related_query_name(), {})\n1253:                 next_klass_infos = self.get_related_selections(\n1254:                     select,\n1255:                     related_select_mask,\n1256:                     model._meta,\n1257:                     alias,\n1258:                     cur_depth + 1,\n1259:                     next,\n1260:                     restricted,\n1261:                 )\n1262:                 get_related_klass_infos(klass_info, next_klass_infos)\n1263: \n1264:             def local_setter(obj, from_obj):\n1265:                 # Set a reverse fk object when relation is non-empty.\n1266:                 if from_obj:\n1267:                     f.remote_field.set_cached_value(from_obj, obj)\n1268: \n1269:             def remote_setter(name, obj, from_obj):\n1270:                 setattr(from_obj, name, obj)\n1271: \n1272:             for name in list(requested):\n1273:                 # Filtered relations work only on the topmost level.\n1274:                 if cur_depth > 1:\n1275:                     break\n1276:                 if name in self.query._filtered_relations:\n1277:                     fields_found.add(name)\n1278:                     f, _, join_opts, joins, _, _ = self.query.setup_joins(\n1279:                         [name], opts, root_alias\n1280:                     )\n1281:                     model = join_opts.model\n1282:                     alias = joins[-1]\n1283:                     from_parent = (\n1284:                         issubclass(model, opts.model) and model is not opts.model\n1285:                     )\n1286:                     klass_info = {\n1287:                         \"model\": model,\n1288:                         \"field\": f,\n1289:                         \"reverse\": True,\n1290:                         \"local_setter\": local_setter,\n1291:                         \"remote_setter\": partial(remote_setter, name),\n1292:                         \"from_parent\": from_parent,\n1293:                     }\n1294:                     related_klass_infos.append(klass_info)\n1295:                     select_fields = []\n1296:                     field_select_mask = select_mask.get((name, f)) or {}\n1297:                     columns = self.get_default_columns(\n1298:                         field_select_mask,\n1299:                         start_alias=alias,\n1300:                         opts=model._meta,\n1301:                         from_parent=opts.model,\n1302:                     )\n1303:                     for col in columns:\n1304:                         select_fields.append(len(select))\n1305:                         select.append((col, None))\n1306:                     klass_info[\"select_fields\"] = select_fields\n1307:                     next_requested = requested.get(name, {})\n1308:                     next_klass_infos = self.get_related_selections(\n1309:                         select,\n1310:                         field_select_mask,\n1311:                         opts=model._meta,\n1312:                         root_alias=alias,\n1313:                         cur_depth=cur_depth + 1,\n1314:                         requested=next_requested,\n1315:                         restricted=restricted,\n1316:                     )\n1317:                     get_related_klass_infos(klass_info, next_klass_infos)\n1318:             fields_not_found = set(requested).difference(fields_found)\n1319:             if fields_not_found:\n1320:                 invalid_fields = (\"'%s'\" % s for s in fields_not_found)\n1321:                 raise FieldError(\n1322:                     \"Invalid field name(s) given in select_related: %s. \"\n1323:                     \"Choices are: %s\"\n1324:                     % (\n1325:                         \", \".join(invalid_fields),\n1326:                         \", \".join(_get_field_choices()) or \"(none)\",\n1327:                     )\n1328:                 )\n1329:         return related_klass_infos\n1330: \n1331:     def get_select_for_update_of_arguments(self):\n1332:         \"\"\"\n1333:         Return a quoted list of arguments for the SELECT FOR UPDATE OF part of\n1334:         the query.\n1335:         \"\"\"\n1336: \n1337:         def _get_parent_klass_info(klass_info):\n1338:             concrete_model = klass_info[\"model\"]._meta.concrete_model\n1339:             for parent_model, parent_link in concrete_model._meta.parents.items():\n1340:                 parent_list = parent_model._meta.get_parent_list()\n1341:                 yield {\n1342:                     \"model\": parent_model,\n1343:                     \"field\": parent_link,\n1344:                     \"reverse\": False,\n1345:                     \"select_fields\": [\n1346:                         select_index\n1347:                         for select_index in klass_info[\"select_fields\"]\n1348:                         # Selected columns from a model or its parents.\n1349:                         if (\n1350:                             self.select[select_index][0].target.model == parent_model\n1351:                             or self.select[select_index][0].target.model in parent_list\n1352:                         )\n1353:                     ],\n1354:                 }\n1355: \n1356:         def _get_first_selected_col_from_model(klass_info):\n1357:             \"\"\"\n1358:             Find the first selected column from a model. If it doesn't exist,\n1359:             don't lock a model.\n1360: \n1361:             select_fields is filled recursively, so it also contains fields\n1362:             from the parent models.\n1363:             \"\"\"\n1364:             concrete_model = klass_info[\"model\"]._meta.concrete_model\n1365:             for select_index in klass_info[\"select_fields\"]:\n1366:                 if self.select[select_index][0].target.model == concrete_model:\n1367:                     return self.select[select_index][0]\n1368: \n1369:         def _get_field_choices():\n1370:             \"\"\"Yield all allowed field paths in breadth-first search order.\"\"\"\n1371:             queue = collections.deque([(None, self.klass_info)])\n1372:             while queue:\n1373:                 parent_path, klass_info = queue.popleft()\n1374:                 if parent_path is None:\n1375:                     path = []\n1376:                     yield \"self\"\n1377:                 else:\n1378:                     field = klass_info[\"field\"]\n1379:                     if klass_info[\"reverse\"]:\n1380:                         field = field.remote_field\n1381:                     path = parent_path + [field.name]\n1382:                     yield LOOKUP_SEP.join(path)\n1383:                 queue.extend(\n1384:                     (path, klass_info)\n1385:                     for klass_info in _get_parent_klass_info(klass_info)\n1386:                 )\n1387:                 queue.extend(\n1388:                     (path, klass_info)\n1389:                     for klass_info in klass_info.get(\"related_klass_infos\", [])\n1390:                 )\n1391: \n1392:         if not self.klass_info:\n1393:             return []\n1394:         result = []\n1395:         invalid_names = []\n1396:         for name in self.query.select_for_update_of:\n1397:             klass_info = self.klass_info\n1398:             if name == \"self\":\n1399:                 col = _get_first_selected_col_from_model(klass_info)\n1400:             else:\n1401:                 for part in name.split(LOOKUP_SEP):\n1402:                     klass_infos = (\n1403:                         *klass_info.get(\"related_klass_infos\", []),\n1404:                         *_get_parent_klass_info(klass_info),\n1405:                     )\n1406:                     for related_klass_info in klass_infos:\n1407:                         field = related_klass_info[\"field\"]\n1408:                         if related_klass_info[\"reverse\"]:\n1409:                             field = field.remote_field\n1410:                         if field.name == part:\n1411:                             klass_info = related_klass_info\n1412:                             break\n1413:                     else:\n1414:                         klass_info = None\n1415:                         break\n1416:                 if klass_info is None:\n1417:                     invalid_names.append(name)\n1418:                     continue\n1419:                 col = _get_first_selected_col_from_model(klass_info)\n1420:             if col is not None:\n1421:                 if self.connection.features.select_for_update_of_column:\n1422:                     result.append(self.compile(col)[0])\n1423:                 else:\n1424:                     result.append(self.quote_name_unless_alias(col.alias))\n1425:         if invalid_names:\n1426:             raise FieldError(\n1427:                 \"Invalid field name(s) given in select_for_update(of=(...)): %s. \"\n1428:                 \"Only relational fields followed in the query are allowed. \"\n1429:                 \"Choices are: %s.\"\n1430:                 % (\n1431:                     \", \".join(invalid_names),\n1432:                     \", \".join(_get_field_choices()),\n1433:                 )\n1434:             )\n1435:         return result\n1436: \n1437:     def get_converters(self, expressions):\n1438:         converters = {}\n1439:         for i, expression in enumerate(expressions):\n1440:             if expression:\n1441:                 backend_converters = self.connection.ops.get_db_converters(expression)\n1442:                 field_converters = expression.get_db_converters(self.connection)\n1443:                 if backend_converters or field_converters:\n1444:                     converters[i] = (backend_converters + field_converters, expression)\n1445:         return converters\n1446: \n1447:     def apply_converters(self, rows, converters):\n1448:         connection = self.connection\n1449:         converters = list(converters.items())\n1450:         for row in map(list, rows):\n1451:             for pos, (convs, expression) in converters:\n1452:                 value = row[pos]\n1453:                 for converter in convs:\n1454:                     value = converter(value, expression, connection)\n1455:                 row[pos] = value\n1456:             yield row\n1457: \n1458:     def results_iter(\n1459:         self,\n1460:         results=None,\n1461:         tuple_expected=False,\n1462:         chunked_fetch=False,\n1463:         chunk_size=GET_ITERATOR_CHUNK_SIZE,\n1464:     ):\n1465:         \"\"\"Return an iterator over the results from executing this query.\"\"\"\n1466:         if results is None:\n1467:             results = self.execute_sql(\n1468:                 MULTI, chunked_fetch=chunked_fetch, chunk_size=chunk_size\n1469:             )\n1470:         fields = [s[0] for s in self.select[0 : self.col_count]]\n1471:         converters = self.get_converters(fields)\n1472:         rows = chain.from_iterable(results)\n1473:         if converters:\n1474:             rows = self.apply_converters(rows, converters)\n1475:             if tuple_expected:\n1476:                 rows = map(tuple, rows)\n1477:         return rows\n1478: \n1479:     def has_results(self):\n1480:         \"\"\"\n1481:         Backends (e.g. NoSQL) can override this in order to use optimized\n1482:         versions of \"query has any results.\"\n1483:         \"\"\"\n1484:         return bool(self.execute_sql(SINGLE))\n1485: \n1486:     def execute_sql(\n1487:         self, result_type=MULTI, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE\n1488:     ):\n1489:         \"\"\"\n1490:         Run the query against the database and return the result(s). The\n1491:         return value is a single data item if result_type is SINGLE, or an\n1492:         iterator over the results if the result_type is MULTI.\n1493: \n1494:         result_type is either MULTI (use fetchmany() to retrieve all rows),\n1495:         SINGLE (only retrieve a single row), or None. In this last case, the\n1496:         cursor is returned if any query is executed, since it's used by\n1497:         subclasses such as InsertQuery). It's possible, however, that no query\n1498:         is needed, as the filters describe an empty set. In that case, None is\n1499:         returned, to avoid any unnecessary database interaction.\n1500:         \"\"\"\n1501:         result_type = result_type or NO_RESULTS\n1502:         try:\n1503:             sql, params = self.as_sql()\n1504:             if not sql:\n1505:                 raise EmptyResultSet\n1506:         except EmptyResultSet:\n1507:             if result_type == MULTI:\n1508:                 return iter([])\n1509:             else:\n1510:                 return\n1511:         if chunked_fetch:\n1512:             cursor = self.connection.chunked_cursor()\n1513:         else:\n1514:             cursor = self.connection.cursor()\n1515:         try:\n1516:             cursor.execute(sql, params)\n1517:         except Exception:\n1518:             # Might fail for server-side cursors (e.g. connection closed)\n1519:             cursor.close()\n1520:             raise\n1521: \n1522:         if result_type == CURSOR:\n1523:             # Give the caller the cursor to process and close.\n1524:             return cursor\n1525:         if result_type == SINGLE:\n1526:             try:\n1527:                 val = cursor.fetchone()\n1528:                 if val:\n1529:                     return val[0 : self.col_count]\n1530:                 return val\n1531:             finally:\n1532:                 # done with the cursor\n1533:                 cursor.close()\n1534:         if result_type == NO_RESULTS:\n1535:             cursor.close()\n1536:             return\n1537: \n1538:         result = cursor_iter(\n1539:             cursor,\n1540:             self.connection.features.empty_fetchmany_value,\n1541:             self.col_count if self.has_extra_select else None,\n1542:             chunk_size,\n1543:         )\n1544:         if not chunked_fetch or not self.connection.features.can_use_chunked_reads:\n1545:             # If we are using non-chunked reads, we return the same data\n1546:             # structure as normally, but ensure it is all read into memory\n1547:             # before going any further. Use chunked_fetch if requested,\n1548:             # unless the database doesn't support it.\n1549:             return list(result)\n1550:         return result\n1551: \n1552:     def as_subquery_condition(self, alias, columns, compiler):\n1553:         qn = compiler.quote_name_unless_alias\n1554:         qn2 = self.connection.ops.quote_name\n1555: \n1556:         for index, select_col in enumerate(self.query.select):\n1557:             lhs_sql, lhs_params = self.compile(select_col)\n1558:             rhs = \"%s.%s\" % (qn(alias), qn2(columns[index]))\n1559:             self.query.where.add(RawSQL(\"%s = %s\" % (lhs_sql, rhs), lhs_params), AND)\n1560: \n1561:         sql, params = self.as_sql()\n1562:         return \"EXISTS (%s)\" % sql, params\n1563: \n1564:     def explain_query(self):\n1565:         result = list(self.execute_sql())\n1566:         # Some backends return 1 item tuples with strings, and others return\n1567:         # tuples with integers and strings. Flatten them out into strings.\n1568:         format_ = self.query.explain_info.format\n1569:         output_formatter = json.dumps if format_ and format_.lower() == \"json\" else str\n1570:         for row in result[0]:\n1571:             if not isinstance(row, str):\n1572:                 yield \" \".join(output_formatter(c) for c in row)\n1573:             else:\n1574:                 yield row\n1575: \n1576: \n1577: class SQLInsertCompiler(SQLCompiler):\n1578:     returning_fields = None\n1579:     returning_params = ()\n1580: \n1581:     def field_as_sql(self, field, val):\n1582:         \"\"\"\n1583:         Take a field and a value intended to be saved on that field, and\n1584:         return placeholder SQL and accompanying params. Check for raw values,\n1585:         expressions, and fields with get_placeholder() defined in that order.\n1586: \n1587:         When field is None, consider the value raw and use it as the\n1588:         placeholder, with no corresponding parameters returned.\n1589:         \"\"\"\n1590:         if field is None:\n1591:             # A field value of None means the value is raw.\n1592:             sql, params = val, []\n1593:         elif hasattr(val, \"as_sql\"):\n1594:             # This is an expression, let's compile it.\n1595:             sql, params = self.compile(val)\n1596:         elif hasattr(field, \"get_placeholder\"):\n1597:             # Some fields (e.g. geo fields) need special munging before\n1598:             # they can be inserted.\n1599:             sql, params = field.get_placeholder(val, self, self.connection), [val]\n1600:         else:\n1601:             # Return the common case for the placeholder\n1602:             sql, params = \"%s\", [val]\n1603: \n1604:         # The following hook is only used by Oracle Spatial, which sometimes\n1605:         # needs to yield 'NULL' and [] as its placeholder and params instead\n1606:         # of '%s' and [None]. The 'NULL' placeholder is produced earlier by\n1607:         # OracleOperations.get_geom_placeholder(). The following line removes\n1608:         # the corresponding None parameter. See ticket #10888.\n1609:         params = self.connection.ops.modify_insert_params(sql, params)\n1610: \n1611:         return sql, params\n1612: \n1613:     def prepare_value(self, field, value):\n1614:         \"\"\"\n1615:         Prepare a value to be used in a query by resolving it if it is an\n1616:         expression and otherwise calling the field's get_db_prep_save().\n1617:         \"\"\"\n1618:         if hasattr(value, \"resolve_expression\"):\n1619:             value = value.resolve_expression(\n1620:                 self.query, allow_joins=False, for_save=True\n1621:             )\n1622:             # Don't allow values containing Col expressions. They refer to\n1623:             # existing columns on a row, but in the case of insert the row\n1624:             # doesn't exist yet.\n1625:             if value.contains_column_references:\n1626:                 raise ValueError(\n1627:                     'Failed to insert expression \"%s\" on %s. F() expressions '\n1628:                     \"can only be used to update, not to insert.\" % (value, field)\n1629:                 )\n1630:             if value.contains_aggregate:\n1631:                 raise FieldError(\n1632:                     \"Aggregate functions are not allowed in this query \"\n1633:                     \"(%s=%r).\" % (field.name, value)\n1634:                 )\n1635:             if value.contains_over_clause:\n1636:                 raise FieldError(\n1637:                     \"Window expressions are not allowed in this query (%s=%r).\"\n1638:                     % (field.name, value)\n1639:                 )\n1640:         else:\n1641:             value = field.get_db_prep_save(value, connection=self.connection)\n1642:         return value\n1643: \n1644:     def pre_save_val(self, field, obj):\n1645:         \"\"\"\n1646:         Get the given field's value off the given obj. pre_save() is used for\n1647:         things like auto_now on DateTimeField. Skip it if this is a raw query.\n1648:         \"\"\"\n1649:         if self.query.raw:\n1650:             return getattr(obj, field.attname)\n1651:         return field.pre_save(obj, add=True)\n1652: \n1653:     def assemble_as_sql(self, fields, value_rows):\n1654:         \"\"\"\n1655:         Take a sequence of N fields and a sequence of M rows of values, and\n1656:         generate placeholder SQL and parameters for each field and value.\n1657:         Return a pair containing:\n1658:          * a sequence of M rows of N SQL placeholder strings, and\n1659:          * a sequence of M rows of corresponding parameter values.\n1660: \n1661:         Each placeholder string may contain any number of '%s' interpolation\n1662:         strings, and each parameter row will contain exactly as many params\n1663:         as the total number of '%s's in the corresponding placeholder row.\n1664:         \"\"\"\n1665:         if not value_rows:\n1666:             return [], []\n1667: \n1668:         # list of (sql, [params]) tuples for each object to be saved\n1669:         # Shape: [n_objs][n_fields][2]\n1670:         rows_of_fields_as_sql = (\n1671:             (self.field_as_sql(field, v) for field, v in zip(fields, row))\n1672:             for row in value_rows\n1673:         )\n1674: \n1675:         # tuple like ([sqls], [[params]s]) for each object to be saved\n1676:         # Shape: [n_objs][2][n_fields]\n1677:         sql_and_param_pair_rows = (zip(*row) for row in rows_of_fields_as_sql)\n1678: \n1679:         # Extract separate lists for placeholders and params.\n1680:         # Each of these has shape [n_objs][n_fields]\n1681:         placeholder_rows, param_rows = zip(*sql_and_param_pair_rows)\n1682: \n1683:         # Params for each field are still lists, and need to be flattened.\n1684:         param_rows = [[p for ps in row for p in ps] for row in param_rows]\n1685: \n1686:         return placeholder_rows, param_rows\n1687: \n1688:     def as_sql(self):\n1689:         # We don't need quote_name_unless_alias() here, since these are all\n1690:         # going to be column names (so we can avoid the extra overhead).\n1691:         qn = self.connection.ops.quote_name\n1692:         opts = self.query.get_meta()\n1693:         insert_statement = self.connection.ops.insert_statement(\n1694:             on_conflict=self.query.on_conflict,\n1695:         )\n1696:         result = [\"%s %s\" % (insert_statement, qn(opts.db_table))]\n1697:         fields = self.query.fields or [opts.pk]\n1698:         result.append(\"(%s)\" % \", \".join(qn(f.column) for f in fields))\n1699: \n1700:         if self.query.fields:\n1701:             value_rows = [\n1702:                 [\n1703:                     self.prepare_value(field, self.pre_save_val(field, obj))\n1704:                     for field in fields\n1705:                 ]\n1706:                 for obj in self.query.objs\n1707:             ]\n1708:         else:\n1709:             # An empty object.\n1710:             value_rows = [\n1711:                 [self.connection.ops.pk_default_value()] for _ in self.query.objs\n1712:             ]\n1713:             fields = [None]\n1714: \n1715:         # Currently the backends just accept values when generating bulk\n1716:         # queries and generate their own placeholders. Doing that isn't\n1717:         # necessary and it should be possible to use placeholders and\n1718:         # expressions in bulk inserts too.\n1719:         can_bulk = (\n1720:             not self.returning_fields and self.connection.features.has_bulk_insert\n1721:         )\n1722: \n1723:         placeholder_rows, param_rows = self.assemble_as_sql(fields, value_rows)\n1724: \n1725:         on_conflict_suffix_sql = self.connection.ops.on_conflict_suffix_sql(\n1726:             fields,\n1727:             self.query.on_conflict,\n1728:             self.query.update_fields,\n1729:             self.query.unique_fields,\n1730:         )\n... Code Truncated ...\n\n",
  "line_level_localization": [
    {
      "filename": "/django/db/models/query.py",
      "suspect_lines": [
        723,
        735,
        736,
        737,
        789
      ]
    },
    {
      "filename": "/django/db/models/sql/compiler.py",
      "suspect_lines": [
        1728,
        1729
      ]
    }
  ]
}