{
  "instance_id": "scikit-learn__scikit-learn-10297",
  "problem_statement": "linear_model.RidgeClassifierCV's Parameter store_cv_values issue\n#### Description\r\nParameter store_cv_values error on sklearn.linear_model.RidgeClassifierCV\r\n\r\n#### Steps/Code to Reproduce\r\nimport numpy as np\r\nfrom sklearn import linear_model as lm\r\n\r\n#test database\r\nn = 100\r\nx = np.random.randn(n, 30)\r\ny = np.random.normal(size = n)\r\n\r\nrr = lm.RidgeClassifierCV(alphas = np.arange(0.1, 1000, 0.1), normalize = True, \r\n                                         store_cv_values = True).fit(x, y)\r\n\r\n#### Expected Results\r\nExpected to get the usual ridge regression model output, keeping the cross validation predictions as attribute.\r\n\r\n#### Actual Results\r\nTypeError: __init__() got an unexpected keyword argument 'store_cv_values'\r\n\r\nlm.RidgeClassifierCV actually has no parameter store_cv_values, even though some attributes depends on it.\r\n\r\n#### Versions\r\nWindows-10-10.0.14393-SP0\r\nPython 3.6.3 |Anaconda, Inc.| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]\r\nNumPy 1.13.3\r\nSciPy 0.19.1\r\nScikit-Learn 0.19.1\r\n\r\n\nAdd store_cv_values boolean flag support to RidgeClassifierCV\nAdd store_cv_values support to RidgeClassifierCV - documentation claims that usage of this flag is possible:\n\n> cv_values_ : array, shape = [n_samples, n_alphas] or shape = [n_samples, n_responses, n_alphas], optional\n> Cross-validation values for each alpha (if **store_cv_values**=True and `cv=None`).\n\nWhile actually usage of this flag gives \n\n> TypeError: **init**() got an unexpected keyword argument 'store_cv_values'\n\n",
  "localized_code": "[start of sklearn/linear_model/ridge.py]\n1: \"\"\"\n2: Ridge regression\n3: \"\"\"\n4: \n5: # Author: Mathieu Blondel <mathieu@mblondel.org>\n6: #         Reuben Fletcher-Costin <reuben.fletchercostin@gmail.com>\n7: #         Fabian Pedregosa <fabian@fseoane.net>\n8: #         Michael Eickenberg <michael.eickenberg@nsup.org>\n9: # License: BSD 3 clause\n10: \n11: \n12: from abc import ABCMeta, abstractmethod\n13: import warnings\n14: \n15: import numpy as np\n16: from scipy import linalg\n17: from scipy import sparse\n18: from scipy.sparse import linalg as sp_linalg\n19: \n20: from .base import LinearClassifierMixin, LinearModel, _rescale_data\n21: from .sag import sag_solver\n22: from ..base import RegressorMixin\n23: from ..utils.extmath import safe_sparse_dot\n24: from ..utils.extmath import row_norms\n25: from ..utils import check_X_y\n26: from ..utils import check_array\n27: from ..utils import check_consistent_length\n28: from ..utils import compute_sample_weight\n29: from ..utils import column_or_1d\n30: from ..preprocessing import LabelBinarizer\n31: from ..model_selection import GridSearchCV\n32: from ..externals import six\n33: from ..metrics.scorer import check_scoring\n34: \n35: \n36: def _solve_sparse_cg(X, y, alpha, max_iter=None, tol=1e-3, verbose=0):\n37:     n_samples, n_features = X.shape\n38:     X1 = sp_linalg.aslinearoperator(X)\n39:     coefs = np.empty((y.shape[1], n_features), dtype=X.dtype)\n40: \n41:     if n_features > n_samples:\n42:         def create_mv(curr_alpha):\n43:             def _mv(x):\n44:                 return X1.matvec(X1.rmatvec(x)) + curr_alpha * x\n45:             return _mv\n46:     else:\n47:         def create_mv(curr_alpha):\n48:             def _mv(x):\n49:                 return X1.rmatvec(X1.matvec(x)) + curr_alpha * x\n50:             return _mv\n51: \n52:     for i in range(y.shape[1]):\n53:         y_column = y[:, i]\n54: \n55:         mv = create_mv(alpha[i])\n56:         if n_features > n_samples:\n57:             # kernel ridge\n58:             # w = X.T * inv(X X^t + alpha*Id) y\n59:             C = sp_linalg.LinearOperator(\n60:                 (n_samples, n_samples), matvec=mv, dtype=X.dtype)\n61:             coef, info = sp_linalg.cg(C, y_column, tol=tol)\n62:             coefs[i] = X1.rmatvec(coef)\n63:         else:\n64:             # linear ridge\n65:             # w = inv(X^t X + alpha*Id) * X.T y\n66:             y_column = X1.rmatvec(y_column)\n67:             C = sp_linalg.LinearOperator(\n68:                 (n_features, n_features), matvec=mv, dtype=X.dtype)\n69:             coefs[i], info = sp_linalg.cg(C, y_column, maxiter=max_iter,\n70:                                           tol=tol)\n71:         if info < 0:\n72:             raise ValueError(\"Failed with error code %d\" % info)\n73: \n74:         if max_iter is None and info > 0 and verbose:\n75:             warnings.warn(\"sparse_cg did not converge after %d iterations.\" %\n76:                           info)\n77: \n78:     return coefs\n79: \n80: \n81: def _solve_lsqr(X, y, alpha, max_iter=None, tol=1e-3):\nCode replaced for brevity.\n96: \n97: \n98: \n99:     # w = inv(X^t X + alpha*Id) * X.T y\nCode replaced for brevity.\n120: \n121: \n122: \n123:     # dual_coef = inv(X X^t + alpha*Id) y\nCode replaced for brevity.\n181: \n182: \n183: \n184: def _solve_svd(X, y, alpha):\nCode replaced for brevity.\n192: \n193: \n194: \n195:                      return_n_iter=False, return_intercept=False):\nCode replaced for brevity.\n456: \n457: \n458: \n459: class _BaseRidge(six.with_metaclass(ABCMeta, LinearModel)):\nCode replaced for brevity.\n509: \n510: \n511: \n512: class Ridge(_BaseRidge, RegressorMixin):\nCode replaced for brevity.\n672: \n673: \n674: \n675: class RidgeClassifier(LinearClassifierMixin, _BaseRidge):\nCode replaced for brevity.\n840: \n841: \n842: \n843: class _RidgeGCV(LinearModel):\nCode replaced for brevity.\n1084: \n1085: \n1086: \n1087: class _BaseRidgeCV(LinearModel):\nCode replaced for brevity.\n1144: \n1145: \n1146: \n1147: class RidgeCV(_BaseRidgeCV, RegressorMixin):\n1148:     \"\"\"Ridge regression with built-in cross-validation.\n1149: \n1150:     By default, it performs Generalized Cross-Validation, which is a form of\n1151:     efficient Leave-One-Out cross-validation.\n1152: \n1153:     Read more in the :ref:`User Guide <ridge_regression>`.\n1154: \n1155:     Parameters\n1156:     ----------\n1157:     alphas : numpy array of shape [n_alphas]\n1158:         Array of alpha values to try.\n1159:         Regularization strength; must be a positive float. Regularization\n1160:         improves the conditioning of the problem and reduces the variance of\n1161:         the estimates. Larger values specify stronger regularization.\n1162:         Alpha corresponds to ``C^-1`` in other linear models such as\n1163:         LogisticRegression or LinearSVC.\n1164: \n1165:     fit_intercept : boolean\n1166:         Whether to calculate the intercept for this model. If set\n1167:         to false, no intercept will be used in calculations\n1168:         (e.g. data is expected to be already centered).\n1169: \n1170:     normalize : boolean, optional, default False\n1171:         This parameter is ignored when ``fit_intercept`` is set to False.\n1172:         If True, the regressors X will be normalized before regression by\n1173:         subtracting the mean and dividing by the l2-norm.\n1174:         If you wish to standardize, please use\n1175:         :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n1176:         on an estimator with ``normalize=False``.\n1177: \n1178:     scoring : string, callable or None, optional, default: None\n1179:         A string (see model evaluation documentation) or\n1180:         a scorer callable object / function with signature\n1181:         ``scorer(estimator, X, y)``.\n1182: \n1183:     cv : int, cross-validation generator or an iterable, optional\n1184:         Determines the cross-validation splitting strategy.\n1185:         Possible inputs for cv are:\n1186: \n1187:         - None, to use the efficient Leave-One-Out cross-validation\n1188:         - integer, to specify the number of folds.\n1189:         - An object to be used as a cross-validation generator.\n1190:         - An iterable yielding train/test splits.\n1191: \n1192:         For integer/None inputs, if ``y`` is binary or multiclass,\n1193:         :class:`sklearn.model_selection.StratifiedKFold` is used, else,\n1194:         :class:`sklearn.model_selection.KFold` is used.\n1195: \n1196:         Refer :ref:`User Guide <cross_validation>` for the various\n1197:         cross-validation strategies that can be used here.\n1198: \n1199:     gcv_mode : {None, 'auto', 'svd', eigen'}, optional\n1200:         Flag indicating which strategy to use when performing\n1201:         Generalized Cross-Validation. Options are::\n1202: \n1203:             'auto' : use svd if n_samples > n_features or when X is a sparse\n1204:                      matrix, otherwise use eigen\n1205:             'svd' : force computation via singular value decomposition of X\n1206:                     (does not work for sparse matrices)\n1207:             'eigen' : force computation via eigendecomposition of X^T X\n1208: \n1209:         The 'auto' mode is the default and is intended to pick the cheaper\n1210:         option of the two depending upon the shape and format of the training\n1211:         data.\n1212: \n1213:     store_cv_values : boolean, default=False\n1214:         Flag indicating if the cross-validation values corresponding to\n1215:         each alpha should be stored in the `cv_values_` attribute (see\n1216:         below). This flag is only compatible with `cv=None` (i.e. using\n1217:         Generalized Cross-Validation).\n1218: \n1219:     Attributes\n1220:     ----------\n1221:     cv_values_ : array, shape = [n_samples, n_alphas] or \\\n1222:         shape = [n_samples, n_targets, n_alphas], optional\n1223:         Cross-validation values for each alpha (if `store_cv_values=True` and \\\n1224:         `cv=None`). After `fit()` has been called, this attribute will \\\n1225:         contain the mean squared errors (by default) or the values of the \\\n1226:         `{loss,score}_func` function (if provided in the constructor).\n1227: \n1228:     coef_ : array, shape = [n_features] or [n_targets, n_features]\n1229:         Weight vector(s).\n1230: \n1231:     intercept_ : float | array, shape = (n_targets,)\n1232:         Independent term in decision function. Set to 0.0 if\n1233:         ``fit_intercept = False``.\n1234: \n1235:     alpha_ : float\n1236:         Estimated regularization parameter.\n1237: \n1238:     See also\n1239:     --------\n1240:     Ridge : Ridge regression\n1241:     RidgeClassifier : Ridge classifier\n1242:     RidgeClassifierCV : Ridge classifier with built-in cross validation\n1243:     \"\"\"\n1244:     pass\n1245: \n1246: \n1247: class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n1248:     \"\"\"Ridge classifier with built-in cross-validation.\n1249: \n1250:     By default, it performs Generalized Cross-Validation, which is a form of\n1251:     efficient Leave-One-Out cross-validation. Currently, only the n_features >\n1252:     n_samples case is handled efficiently.\n1253: \n1254:     Read more in the :ref:`User Guide <ridge_regression>`.\n1255: \n1256:     Parameters\n1257:     ----------\n1258:     alphas : numpy array of shape [n_alphas]\n1259:         Array of alpha values to try.\n1260:         Regularization strength; must be a positive float. Regularization\n1261:         improves the conditioning of the problem and reduces the variance of\n1262:         the estimates. Larger values specify stronger regularization.\n1263:         Alpha corresponds to ``C^-1`` in other linear models such as\n1264:         LogisticRegression or LinearSVC.\n1265: \n1266:     fit_intercept : boolean\n1267:         Whether to calculate the intercept for this model. If set\n1268:         to false, no intercept will be used in calculations\n1269:         (e.g. data is expected to be already centered).\n1270: \n1271:     normalize : boolean, optional, default False\n1272:         This parameter is ignored when ``fit_intercept`` is set to False.\n1273:         If True, the regressors X will be normalized before regression by\n1274:         subtracting the mean and dividing by the l2-norm.\n1275:         If you wish to standardize, please use\n1276:         :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n1277:         on an estimator with ``normalize=False``.\n1278: \n1279:     scoring : string, callable or None, optional, default: None\n1280:         A string (see model evaluation documentation) or\n1281:         a scorer callable object / function with signature\n1282:         ``scorer(estimator, X, y)``.\n1283: \n1284:     cv : int, cross-validation generator or an iterable, optional\n1285:         Determines the cross-validation splitting strategy.\n1286:         Possible inputs for cv are:\n1287: \n1288:         - None, to use the efficient Leave-One-Out cross-validation\n1289:         - integer, to specify the number of folds.\n1290:         - An object to be used as a cross-validation generator.\n1291:         - An iterable yielding train/test splits.\n1292: \n1293:         Refer :ref:`User Guide <cross_validation>` for the various\n1294:         cross-validation strategies that can be used here.\n1295: \n1296:     class_weight : dict or 'balanced', optional\n1297:         Weights associated with classes in the form ``{class_label: weight}``.\n1298:         If not given, all classes are supposed to have weight one.\n1299: \n1300:         The \"balanced\" mode uses the values of y to automatically adjust\n1301:         weights inversely proportional to class frequencies in the input data\n1302:         as ``n_samples / (n_classes * np.bincount(y))``\n1303: \n1304:     Attributes\n1305:     ----------\n1306:     cv_values_ : array, shape = [n_samples, n_alphas] or \\\n1307:     shape = [n_samples, n_responses, n_alphas], optional\n1308:         Cross-validation values for each alpha (if `store_cv_values=True` and\n1309:     `cv=None`). After `fit()` has been called, this attribute will contain \\\n1310:     the mean squared errors (by default) or the values of the \\\n1311:     `{loss,score}_func` function (if provided in the constructor).\n1312: \n1313:     coef_ : array, shape = [n_features] or [n_targets, n_features]\n1314:         Weight vector(s).\n1315: \n1316:     intercept_ : float | array, shape = (n_targets,)\n1317:         Independent term in decision function. Set to 0.0 if\n1318:         ``fit_intercept = False``.\n1319: \n1320:     alpha_ : float\n1321:         Estimated regularization parameter\n1322: \n1323:     See also\n1324:     --------\n1325:     Ridge : Ridge regression\n1326:     RidgeClassifier : Ridge classifier\n1327:     RidgeCV : Ridge regression with built-in cross validation\n1328: \n1329:     Notes\n1330:     -----\n1331:     For multi-class classification, n_class classifiers are trained in\n1332:     a one-versus-all approach. Concretely, this is implemented by taking\n1333:     advantage of the multi-variate response support in Ridge.\n1334:     \"\"\"\n1335:     def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,\n1336:                  normalize=False, scoring=None, cv=None, class_weight=None):\n1337:         super(RidgeClassifierCV, self).__init__(\n1338:             alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,\n1339:             scoring=scoring, cv=cv)\n1340:         self.class_weight = class_weight\n1341: \n1342:     def fit(self, X, y, sample_weight=None):\n1343:         \"\"\"Fit the ridge classifier.\n1344: \n1345:         Parameters\n1346:         ----------\n1347:         X : array-like, shape (n_samples, n_features)\n1348:             Training vectors, where n_samples is the number of samples\n1349:             and n_features is the number of features.\n1350: \n1351:         y : array-like, shape (n_samples,)\n1352:             Target values. Will be cast to X's dtype if necessary\n1353: \n1354:         sample_weight : float or numpy array of shape (n_samples,)\n1355:             Sample weight.\n1356: \n1357:         Returns\n1358:         -------\n1359:         self : object\n1360:         \"\"\"\n1361:         check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n1362:                   multi_output=True)\n1363: \n1364:         self._label_binarizer = LabelBinarizer(pos_label=1, neg_label=-1)\n1365:         Y = self._label_binarizer.fit_transform(y)\n1366:         if not self._label_binarizer.y_type_.startswith('multilabel'):\n1367:             y = column_or_1d(y, warn=True)\n1368: \n1369:         if self.class_weight:\n1370:             if sample_weight is None:\n1371:                 sample_weight = 1.\n1372:             # modify the sample weights with the corresponding class weight\n1373:             sample_weight = (sample_weight *\n1374:                              compute_sample_weight(self.class_weight, y))\n1375: \n1376:         _BaseRidgeCV.fit(self, X, Y, sample_weight=sample_weight)\n1377:         return self\n1378: \n1379:     @property\n1380:     def classes_(self):\n1381:         return self._label_binarizer.classes_\n\n",
  "line_level_localization": [
    {
      "filename": "/sklearn/linear_model/ridge.py",
      "suspect_lines": [
        1215,
        1216,
        1223,
        1224,
        1225,
        1226,
        1306,
        1307,
        1308,
        1309,
        1310,
        1311,
        1336,
        1339
      ]
    }
  ]
}