{
  "instance_id": "scikit-learn__scikit-learn-14894",
  "problem_statement": "ZeroDivisionError in _sparse_fit for SVM with empty support_vectors_\n#### Description\r\nWhen using sparse data, in the case where the support_vectors_ attribute is be empty, _fit_sparse gives a ZeroDivisionError\r\n\r\n#### Steps/Code to Reproduce\r\n```\r\nimport numpy as np\r\nimport scipy\r\nimport sklearn\r\nfrom sklearn.svm import SVR\r\nx_train = np.array([[0, 1, 0, 0],\r\n[0, 0, 0, 1],\r\n[0, 0, 1, 0],\r\n[0, 0, 0, 1]])\r\ny_train = np.array([0.04, 0.04, 0.10, 0.16])\r\nmodel = SVR(C=316.227766017, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\r\n  \t    gamma=1.0, kernel='linear', max_iter=15000,\r\n  \t    shrinking=True, tol=0.001, verbose=False)\r\n# dense x_train has no error\r\nmodel.fit(x_train, y_train)\r\n\r\n# convert to sparse\r\nxtrain= scipy.sparse.csr_matrix(x_train)\r\nmodel.fit(xtrain, y_train)\r\n\r\n```\r\n#### Expected Results\r\nNo error is thrown and  `self.dual_coef_ = sp.csr_matrix([])`\r\n\r\n#### Actual Results\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py\", line 209, in fit\r\n    fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\r\n  File \"/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py\", line 302, in _sparse_fit\r\n    dual_coef_indices.size / n_class)\r\nZeroDivisionError: float division by zero\r\n```\r\n\r\n#### Versions\r\n```\r\n>>> sklearn.show_versions() \r\n\r\nSystem:\r\nexecutable: /usr/bin/python3\r\n    python: 3.5.2 (default, Nov 12 2018, 13:43:14)  [GCC 5.4.0 20160609]\r\n   machine: Linux-4.15.0-58-generic-x86_64-with-Ubuntu-16.04-xenial\r\n\r\nPython deps:\r\n     numpy: 1.17.0\r\n    Cython: None\r\n       pip: 19.2.1\r\n    pandas: 0.22.0\r\n   sklearn: 0.21.3\r\n     scipy: 1.3.0\r\nsetuptools: 40.4.3\r\n```\n",
  "localized_code": "[start of sklearn/svm/base.py]\n1: import numpy as np\n2: import scipy.sparse as sp\n3: import warnings\n4: from abc import ABCMeta, abstractmethod\n5: \n6: from . import libsvm, liblinear\n7: from . import libsvm_sparse\n8: from ..base import BaseEstimator, ClassifierMixin\n9: from ..preprocessing import LabelEncoder\n10: from ..utils.multiclass import _ovr_decision_function\n11: from ..utils import check_array, check_random_state\n12: from ..utils import column_or_1d, check_X_y\n13: from ..utils import compute_class_weight\n14: from ..utils.extmath import safe_sparse_dot\n15: from ..utils.validation import check_is_fitted, _check_large_sparse\n16: from ..utils.validation import _check_sample_weight\n17: from ..utils.multiclass import check_classification_targets\n18: from ..exceptions import ConvergenceWarning\n19: from ..exceptions import NotFittedError\n20: \n21: \n22: LIBSVM_IMPL = ['c_svc', 'nu_svc', 'one_class', 'epsilon_svr', 'nu_svr']\n23: \n24: \n25: def _one_vs_one_coef(dual_coef, n_support, support_vectors):\n26:     \"\"\"Generate primal coefficients from dual coefficients\n27:     for the one-vs-one multi class LibSVM in the case\n28:     of a linear kernel.\"\"\"\n29: \n30:     # get 1vs1 weights for all n*(n-1) classifiers.\n31:     # this is somewhat messy.\n32:     # shape of dual_coef_ is nSV * (n_classes -1)\n33:     # see docs for details\n34:     n_class = dual_coef.shape[0] + 1\n35: \n36:     # XXX we could do preallocation of coef but\n37:     # would have to take care in the sparse case\n38:     coef = []\n39:     sv_locs = np.cumsum(np.hstack([[0], n_support]))\n40:     for class1 in range(n_class):\n41:         # SVs for class1:\n42:         sv1 = support_vectors[sv_locs[class1]:sv_locs[class1 + 1], :]\n43:         for class2 in range(class1 + 1, n_class):\n44:             # SVs for class1:\n45:             sv2 = support_vectors[sv_locs[class2]:sv_locs[class2 + 1], :]\n46: \n47:             # dual coef for class1 SVs:\n48:             alpha1 = dual_coef[class2 - 1, sv_locs[class1]:sv_locs[class1 + 1]]\n49:             # dual coef for class2 SVs:\n50:             alpha2 = dual_coef[class1, sv_locs[class2]:sv_locs[class2 + 1]]\n51:             # build weight for class1 vs class2\n52: \n53:             coef.append(safe_sparse_dot(alpha1, sv1)\n54:                         + safe_sparse_dot(alpha2, sv2))\n55:     return coef\n56: \n57: \n58: class BaseLibSVM(BaseEstimator, metaclass=ABCMeta):\n59:     \"\"\"Base class for estimators that use libsvm as backing library\n60: \n61:     This implements support vector machine classification and regression.\n62: \n63:     Parameter documentation is in the derived `SVC` class.\n64:     \"\"\"\n65: \n66:     # The order of these must match the integer values in LibSVM.\n67:     # XXX These are actually the same in the dense case. Need to factor\n68:     # this out.\n69:     _sparse_kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"]\n70: \n71:     @abstractmethod\n72:     def __init__(self, kernel, degree, gamma, coef0,\n73:                  tol, C, nu, epsilon, shrinking, probability, cache_size,\n74:                  class_weight, verbose, max_iter, random_state):\n75: \n76:         if self._impl not in LIBSVM_IMPL:  # pragma: no cover\n77:             raise ValueError(\"impl should be one of %s, %s was given\" % (\n78:                 LIBSVM_IMPL, self._impl))\n79: \n80:         if gamma == 0:\n81:             msg = (\"The gamma value of 0.0 is invalid. Use 'auto' to set\"\n82:                    \" gamma to a value of 1 / n_features.\")\n83:             raise ValueError(msg)\n84: \n85:         self.kernel = kernel\n86:         self.degree = degree\n87:         self.gamma = gamma\n88:         self.coef0 = coef0\n89:         self.tol = tol\n90:         self.C = C\n91:         self.nu = nu\n92:         self.epsilon = epsilon\n93:         self.shrinking = shrinking\n94:         self.probability = probability\n95:         self.cache_size = cache_size\n96:         self.class_weight = class_weight\n97:         self.verbose = verbose\n98:         self.max_iter = max_iter\n99:         self.random_state = random_state\n100: \n101:     @property\n102:     def _pairwise(self):\n103:         # Used by cross_val_score.\n104:         return self.kernel == \"precomputed\"\n105: \n106:     def fit(self, X, y, sample_weight=None):\n107:         \"\"\"Fit the SVM model according to the given training data.\n108: \n109:         Parameters\n110:         ----------\n111:         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n112:             Training vectors, where n_samples is the number of samples\n113:             and n_features is the number of features.\n114:             For kernel=\"precomputed\", the expected shape of X is\n115:             (n_samples, n_samples).\n116: \n117:         y : array-like, shape (n_samples,)\n118:             Target values (class labels in classification, real numbers in\n119:             regression)\n120: \n121:         sample_weight : array-like, shape (n_samples,)\n122:             Per-sample weights. Rescale C per sample. Higher weights\n123:             force the classifier to put more emphasis on these points.\n124: \n125:         Returns\n126:         -------\n127:         self : object\n128: \n129:         Notes\n130:         -----\n131:         If X and y are not C-ordered and contiguous arrays of np.float64 and\n132:         X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n133: \n134:         If X is a dense array, then the other methods will not support sparse\n135:         matrices as input.\n136:         \"\"\"\n137: \n138:         rnd = check_random_state(self.random_state)\n139: \n140:         sparse = sp.isspmatrix(X)\n141:         if sparse and self.kernel == \"precomputed\":\n142:             raise TypeError(\"Sparse precomputed kernels are not supported.\")\n143:         self._sparse = sparse and not callable(self.kernel)\n144: \n145:         X, y = check_X_y(X, y, dtype=np.float64,\n146:                          order='C', accept_sparse='csr',\n147:                          accept_large_sparse=False)\n148:         y = self._validate_targets(y)\n149: \n150:         sample_weight = np.asarray([]\n151:                                    if sample_weight is None\n152:                                    else sample_weight, dtype=np.float64)\n153:         solver_type = LIBSVM_IMPL.index(self._impl)\n154: \n155:         # input validation\n156:         if solver_type != 2 and X.shape[0] != y.shape[0]:\n157:             raise ValueError(\"X and y have incompatible shapes.\\n\" +\n158:                              \"X has %s samples, but y has %s.\" %\n159:                              (X.shape[0], y.shape[0]))\n160: \n161:         if self.kernel == \"precomputed\" and X.shape[0] != X.shape[1]:\n162:             raise ValueError(\"Precomputed matrix must be a square matrix.\"\n163:                              \" Input is a {}x{} matrix.\"\n164:                              .format(X.shape[0], X.shape[1]))\n165: \n166:         if sample_weight.shape[0] > 0 and sample_weight.shape[0] != X.shape[0]:\n167:             raise ValueError(\"sample_weight and X have incompatible shapes: \"\n168:                              \"%r vs %r\\n\"\n169:                              \"Note: Sparse matrices cannot be indexed w/\"\n170:                              \"boolean masks (use `indices=True` in CV).\"\n171:                              % (sample_weight.shape, X.shape))\n172: \n173:         if isinstance(self.gamma, str):\n174:             if self.gamma == 'scale':\n175:                 # var = E[X^2] - E[X]^2 if sparse\n176:                 X_var = ((X.multiply(X)).mean() - (X.mean()) ** 2\n177:                          if sparse else X.var())\n178:                 self._gamma = 1.0 / (X.shape[1] * X_var) if X_var != 0 else 1.0\n179:             elif self.gamma == 'auto':\n180:                 self._gamma = 1.0 / X.shape[1]\n181:             else:\n182:                 raise ValueError(\n183:                     \"When 'gamma' is a string, it should be either 'scale' or \"\n184:                     \"'auto'. Got '{}' instead.\".format(self.gamma)\n185:                 )\n186:         else:\n187:             self._gamma = self.gamma\n188: \n189:         kernel = self.kernel\n190:         if callable(kernel):\n191:             kernel = 'precomputed'\n192: \n193:         fit = self._sparse_fit if self._sparse else self._dense_fit\n194:         if self.verbose:  # pragma: no cover\n195:             print('[LibSVM]', end='')\n196: \n197:         seed = rnd.randint(np.iinfo('i').max)\n198:         fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\n199:         # see comment on the other call to np.iinfo in this file\n200: \n201:         self.shape_fit_ = X.shape\n202: \n203:         # In binary case, we need to flip the sign of coef, intercept and\n204:         # decision function. Use self._intercept_ and self._dual_coef_\n205:         # internally.\n206:         self._intercept_ = self.intercept_.copy()\n207:         self._dual_coef_ = self.dual_coef_\n208:         if self._impl in ['c_svc', 'nu_svc'] and len(self.classes_) == 2:\n209:             self.intercept_ *= -1\n210:             self.dual_coef_ = -self.dual_coef_\n211: \n212:         return self\n213: \n214:     def _validate_targets(self, y):\n215:         \"\"\"Validation of y and class_weight.\n216: \n217:         Default implementation for SVR and one-class; overridden in BaseSVC.\n218:         \"\"\"\n219:         # XXX this is ugly.\n220:         # Regression models should not have a class_weight_ attribute.\n221:         self.class_weight_ = np.empty(0)\n222:         return column_or_1d(y, warn=True).astype(np.float64, copy=False)\n223: \n224:     def _warn_from_fit_status(self):\n225:         assert self.fit_status_ in (0, 1)\n226:         if self.fit_status_ == 1:\n227:             warnings.warn('Solver terminated early (max_iter=%i).'\n228:                           '  Consider pre-processing your data with'\n229:                           ' StandardScaler or MinMaxScaler.'\n230:                           % self.max_iter, ConvergenceWarning)\n231: \n232:     def _dense_fit(self, X, y, sample_weight, solver_type, kernel,\n233:                    random_seed):\n234:         if callable(self.kernel):\n235:             # you must store a reference to X to compute the kernel in predict\n236:             # TODO: add keyword copy to copy on demand\n237:             self.__Xfit = X\n238:             X = self._compute_kernel(X)\n239: \n240:             if X.shape[0] != X.shape[1]:\n241:                 raise ValueError(\"X.shape[0] should be equal to X.shape[1]\")\n242: \n243:         libsvm.set_verbosity_wrap(self.verbose)\n244: \n245:         # we don't pass **self.get_params() to allow subclasses to\n246:         # add other parameters to __init__\n247:         self.support_, self.support_vectors_, self._n_support, \\\n248:             self.dual_coef_, self.intercept_, self.probA_, \\\n249:             self.probB_, self.fit_status_ = libsvm.fit(\n250:                 X, y,\n251:                 svm_type=solver_type, sample_weight=sample_weight,\n252:                 class_weight=self.class_weight_, kernel=kernel, C=self.C,\n253:                 nu=self.nu, probability=self.probability, degree=self.degree,\n254:                 shrinking=self.shrinking, tol=self.tol,\n255:                 cache_size=self.cache_size, coef0=self.coef0,\n256:                 gamma=self._gamma, epsilon=self.epsilon,\n257:                 max_iter=self.max_iter, random_seed=random_seed)\n258: \n259:         self._warn_from_fit_status()\n260: \n261:     def _sparse_fit(self, X, y, sample_weight, solver_type, kernel,\n262:                     random_seed):\n263:         X.data = np.asarray(X.data, dtype=np.float64, order='C')\n264:         X.sort_indices()\n265: \n266:         kernel_type = self._sparse_kernels.index(kernel)\n267: \n268:         libsvm_sparse.set_verbosity_wrap(self.verbose)\n269: \n270:         self.support_, self.support_vectors_, dual_coef_data, \\\n271:             self.intercept_, self._n_support, \\\n272:             self.probA_, self.probB_, self.fit_status_ = \\\n273:             libsvm_sparse.libsvm_sparse_train(\n274:                 X.shape[1], X.data, X.indices, X.indptr, y, solver_type,\n275:                 kernel_type, self.degree, self._gamma, self.coef0, self.tol,\n276:                 self.C, self.class_weight_,\n277:                 sample_weight, self.nu, self.cache_size, self.epsilon,\n278:                 int(self.shrinking), int(self.probability), self.max_iter,\n279:                 random_seed)\n280: \n281:         self._warn_from_fit_status()\n282: \n283:         if hasattr(self, \"classes_\"):\n284:             n_class = len(self.classes_) - 1\n285:         else:  # regression\n286:             n_class = 1\n287:         n_SV = self.support_vectors_.shape[0]\n288: \n289:         dual_coef_indices = np.tile(np.arange(n_SV), n_class)\n290:         dual_coef_indptr = np.arange(0, dual_coef_indices.size + 1,\n291:                                      dual_coef_indices.size / n_class)\n292:         self.dual_coef_ = sp.csr_matrix(\n293:             (dual_coef_data, dual_coef_indices, dual_coef_indptr),\n294:             (n_class, n_SV))\n295: \n... Code Truncated ...\n\n",
  "line_level_localization": [
    {
      "filename": "/sklearn/svm/base.py",
      "suspect_lines": [
        290,
        291,
        292,
        293,
        294
      ]
    }
  ]
}