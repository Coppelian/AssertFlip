{
  "instance_id": "django__django-12708",
  "problem_statement": "Migration crashes deleting an index_together if there is a unique_together on the same fields\nDescription\n\t\nHappens with Django 1.11.10\nSteps to reproduce:\n1) Create models with 2 fields, add 2 same fields to unique_together and to index_together\n2) Delete index_together -> Fail\nIt will fail at django/db/backends/base/schema.py, line 378, in _delete_composed_index(), ValueError: Found wrong number (2) of constraints for as this one will find two constraints, the _uniq and the _idx one. No way to get out of this...\nThe worst in my case is that happened as I wanted to refactor my code to use the \"new\" (Dj 1.11) Options.indexes feature. I am actually not deleting the index, just the way it is declared in my code.\nI think there are 2 different points here:\n1) The deletion of index_together should be possible alone or made coherent (migrations side?) with unique_together\n2) Moving the declaration of an index should not result in an index re-creation\n",
  "localized_code": "[start of django/db/backends/base/schema.py]\n1: import logging\n2: from datetime import datetime\n3: \n4: from django.db.backends.ddl_references import (\n5:     Columns, ForeignKeyName, IndexName, Statement, Table,\n6: )\n7: from django.db.backends.utils import names_digest, split_identifier\n8: from django.db.models import Index\n9: from django.db.transaction import TransactionManagementError, atomic\n10: from django.utils import timezone\n11: \n12: logger = logging.getLogger('django.db.backends.schema')\n13: \n14: \n15: def _is_relevant_relation(relation, altered_field):\n16:     \"\"\"\n17:     When altering the given field, must constraints on its model from the given\n18:     relation be temporarily dropped?\n19:     \"\"\"\n20:     field = relation.field\n21:     if field.many_to_many:\n22:         # M2M reverse field\n23:         return False\n24:     if altered_field.primary_key and field.to_fields == [None]:\n25:         # Foreign key constraint on the primary key, which is being altered.\n26:         return True\n27:     # Is the constraint targeting the field being altered?\n28:     return altered_field.name in field.to_fields\n29: \n30: \n31: def _all_related_fields(model):\n32:     return model._meta._get_fields(forward=False, reverse=True, include_hidden=True)\n33: \n34: \n35: def _related_non_m2m_objects(old_field, new_field):\n36:     # Filter out m2m objects from reverse relations.\n37:     # Return (old_relation, new_relation) tuples.\n38:     return zip(\n39:         (obj for obj in _all_related_fields(old_field.model) if _is_relevant_relation(obj, old_field)),\n40:         (obj for obj in _all_related_fields(new_field.model) if _is_relevant_relation(obj, new_field)),\n41:     )\n42: \n43: \n44: class BaseDatabaseSchemaEditor:\n45:     \"\"\"\n46:     This class and its subclasses are responsible for emitting schema-changing\n47:     statements to the databases - model creation/removal/alteration, field\n48:     renaming, index fiddling, and so on.\n49:     \"\"\"\n50: \n51:     # Overrideable SQL templates\n52:     sql_create_table = \"CREATE TABLE %(table)s (%(definition)s)\"\n53:     sql_rename_table = \"ALTER TABLE %(old_table)s RENAME TO %(new_table)s\"\n54:     sql_retablespace_table = \"ALTER TABLE %(table)s SET TABLESPACE %(new_tablespace)s\"\n55:     sql_delete_table = \"DROP TABLE %(table)s CASCADE\"\n56: \n57:     sql_create_column = \"ALTER TABLE %(table)s ADD COLUMN %(column)s %(definition)s\"\n58:     sql_alter_column = \"ALTER TABLE %(table)s %(changes)s\"\n59:     sql_alter_column_type = \"ALTER COLUMN %(column)s TYPE %(type)s\"\n60:     sql_alter_column_null = \"ALTER COLUMN %(column)s DROP NOT NULL\"\n61:     sql_alter_column_not_null = \"ALTER COLUMN %(column)s SET NOT NULL\"\n62:     sql_alter_column_default = \"ALTER COLUMN %(column)s SET DEFAULT %(default)s\"\n63:     sql_alter_column_no_default = \"ALTER COLUMN %(column)s DROP DEFAULT\"\n64:     sql_delete_column = \"ALTER TABLE %(table)s DROP COLUMN %(column)s CASCADE\"\n65:     sql_rename_column = \"ALTER TABLE %(table)s RENAME COLUMN %(old_column)s TO %(new_column)s\"\n66:     sql_update_with_default = \"UPDATE %(table)s SET %(column)s = %(default)s WHERE %(column)s IS NULL\"\n67: \n68:     sql_unique_constraint = \"UNIQUE (%(columns)s)\"\n69:     sql_check_constraint = \"CHECK (%(check)s)\"\n70:     sql_delete_constraint = \"ALTER TABLE %(table)s DROP CONSTRAINT %(name)s\"\n71:     sql_constraint = \"CONSTRAINT %(name)s %(constraint)s\"\n72: \n73:     sql_create_check = \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s CHECK (%(check)s)\"\n74:     sql_delete_check = sql_delete_constraint\n75: \n76:     sql_create_unique = \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s UNIQUE (%(columns)s)\"\n77:     sql_delete_unique = sql_delete_constraint\n78: \n79:     sql_create_fk = (\n80:         \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s FOREIGN KEY (%(column)s) \"\n81:         \"REFERENCES %(to_table)s (%(to_column)s)%(deferrable)s\"\n82:     )\n83:     sql_create_inline_fk = None\n84:     sql_create_column_inline_fk = None\n85:     sql_delete_fk = sql_delete_constraint\n86: \n87:     sql_create_index = \"CREATE INDEX %(name)s ON %(table)s (%(columns)s)%(extra)s%(condition)s\"\n88:     sql_create_unique_index = \"CREATE UNIQUE INDEX %(name)s ON %(table)s (%(columns)s)%(condition)s\"\n89:     sql_delete_index = \"DROP INDEX %(name)s\"\n90: \n91:     sql_create_pk = \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s PRIMARY KEY (%(columns)s)\"\n92:     sql_delete_pk = sql_delete_constraint\n93: \n94:     sql_delete_procedure = 'DROP PROCEDURE %(procedure)s'\n95: \n96:     def __init__(self, connection, collect_sql=False, atomic=True):\n97:         self.connection = connection\n98:         self.collect_sql = collect_sql\n99:         if self.collect_sql:\n100:             self.collected_sql = []\n101:         self.atomic_migration = self.connection.features.can_rollback_ddl and atomic\n102: \n103:     # State-managing methods\n104: \n105:     def __enter__(self):\n106:         self.deferred_sql = []\n107:         if self.atomic_migration:\n108:             self.atomic = atomic(self.connection.alias)\n109:             self.atomic.__enter__()\n110:         return self\n111: \n112:     def __exit__(self, exc_type, exc_value, traceback):\n113:         if exc_type is None:\n114:             for sql in self.deferred_sql:\n115:                 self.execute(sql)\n116:         if self.atomic_migration:\n117:             self.atomic.__exit__(exc_type, exc_value, traceback)\n118: \n119:     # Core utility functions\n120: \n121:     def execute(self, sql, params=()):\n122:         \"\"\"Execute the given SQL statement, with optional parameters.\"\"\"\n123:         # Don't perform the transactional DDL check if SQL is being collected\n124:         # as it's not going to be executed anyway.\n125:         if not self.collect_sql and self.connection.in_atomic_block and not self.connection.features.can_rollback_ddl:\n126:             raise TransactionManagementError(\n127:                 \"Executing DDL statements while in a transaction on databases \"\n128:                 \"that can't perform a rollback is prohibited.\"\n129:             )\n130:         # Account for non-string statement objects.\n131:         sql = str(sql)\n132:         # Log the command we're running, then run it\n133:         logger.debug(\"%s; (params %r)\", sql, params, extra={'params': params, 'sql': sql})\n134:         if self.collect_sql:\n135:             ending = \"\" if sql.endswith(\";\") else \";\"\n136:             if params is not None:\n137:                 self.collected_sql.append((sql % tuple(map(self.quote_value, params))) + ending)\n138:             else:\n139:                 self.collected_sql.append(sql + ending)\n140:         else:\n141:             with self.connection.cursor() as cursor:\n142:                 cursor.execute(sql, params)\n143: \n144:     def quote_name(self, name):\n145:         return self.connection.ops.quote_name(name)\n146: \n147:     def table_sql(self, model):\n148:         \"\"\"Take a model and return its table definition.\"\"\"\n149:         # Add any unique_togethers (always deferred, as some fields might be\n150:         # created afterwards, like geometry fields with some backends).\n151:         for fields in model._meta.unique_together:\n152:             columns = [model._meta.get_field(field).column for field in fields]\n153:             self.deferred_sql.append(self._create_unique_sql(model, columns))\n154:         # Create column SQL, add FK deferreds if needed.\n155:         column_sqls = []\n156:         params = []\n157:         for field in model._meta.local_fields:\n158:             # SQL.\n159:             definition, extra_params = self.column_sql(model, field)\n160:             if definition is None:\n161:                 continue\n162:             # Check constraints can go on the column SQL here.\n163:             db_params = field.db_parameters(connection=self.connection)\n164:             if db_params['check']:\n165:                 definition += ' ' + self.sql_check_constraint % db_params\n166:             # Autoincrement SQL (for backends with inline variant).\n167:             col_type_suffix = field.db_type_suffix(connection=self.connection)\n168:             if col_type_suffix:\n169:                 definition += ' %s' % col_type_suffix\n170:             params.extend(extra_params)\n171:             # FK.\n172:             if field.remote_field and field.db_constraint:\n173:                 to_table = field.remote_field.model._meta.db_table\n174:                 to_column = field.remote_field.model._meta.get_field(field.remote_field.field_name).column\n175:                 if self.sql_create_inline_fk:\n176:                     definition += ' ' + self.sql_create_inline_fk % {\n177:                         'to_table': self.quote_name(to_table),\n178:                         'to_column': self.quote_name(to_column),\n179:                     }\n180:                 elif self.connection.features.supports_foreign_keys:\n181:                     self.deferred_sql.append(self._create_fk_sql(model, field, '_fk_%(to_table)s_%(to_column)s'))\n182:             # Add the SQL to our big list.\n183:             column_sqls.append('%s %s' % (\n184:                 self.quote_name(field.column),\n185:                 definition,\n186:             ))\n187:             # Autoincrement SQL (for backends with post table definition\n188:             # variant).\n189:             if field.get_internal_type() in ('AutoField', 'BigAutoField', 'SmallAutoField'):\n190:                 autoinc_sql = self.connection.ops.autoinc_sql(model._meta.db_table, field.column)\n191:                 if autoinc_sql:\n192:                     self.deferred_sql.extend(autoinc_sql)\n193:         constraints = [constraint.constraint_sql(model, self) for constraint in model._meta.constraints]\n194:         sql = self.sql_create_table % {\n195:             'table': self.quote_name(model._meta.db_table),\n196:             'definition': ', '.join(constraint for constraint in (*column_sqls, *constraints) if constraint),\n197:         }\n198:         if model._meta.db_tablespace:\n199:             tablespace_sql = self.connection.ops.tablespace_sql(model._meta.db_tablespace)\n200:             if tablespace_sql:\n201:                 sql += ' ' + tablespace_sql\n202:         return sql, params\n203: \n204:     # Field <-> database mapping functions\n205: \n206:     def column_sql(self, model, field, include_default=False):\n207:         \"\"\"\n208:         Take a field and return its column definition.\n209:         The field must already have had set_attributes_from_name() called.\n210:         \"\"\"\n211:         # Get the column's type and use that as the basis of the SQL\n212:         db_params = field.db_parameters(connection=self.connection)\n213:         sql = db_params['type']\n214:         params = []\n215:         # Check for fields that aren't actually columns (e.g. M2M)\n216:         if sql is None:\n217:             return None, None\n218:         # Work out nullability\n219:         null = field.null\n220:         # If we were told to include a default value, do so\n221:         include_default = include_default and not self.skip_default(field)\n222:         if include_default:\n223:             default_value = self.effective_default(field)\n224:             column_default = ' DEFAULT ' + self._column_default_sql(field)\n225:             if default_value is not None:\n226:                 if self.connection.features.requires_literal_defaults:\n227:                     # Some databases can't take defaults as a parameter (oracle)\n228:                     # If this is the case, the individual schema backend should\n229:                     # implement prepare_default\n230:                     sql += column_default % self.prepare_default(default_value)\n231:                 else:\n232:                     sql += column_default\n233:                     params += [default_value]\n234:         # Oracle treats the empty string ('') as null, so coerce the null\n235:         # option whenever '' is a possible value.\n236:         if (field.empty_strings_allowed and not field.primary_key and\n237:                 self.connection.features.interprets_empty_strings_as_nulls):\n238:             null = True\n239:         if null and not self.connection.features.implied_column_null:\n240:             sql += \" NULL\"\n241:         elif not null:\n242:             sql += \" NOT NULL\"\n243:         # Primary key/unique outputs\n244:         if field.primary_key:\n245:             sql += \" PRIMARY KEY\"\n246:         elif field.unique:\n247:             sql += \" UNIQUE\"\n248:         # Optionally add the tablespace if it's an implicitly indexed column\n249:         tablespace = field.db_tablespace or model._meta.db_tablespace\n250:         if tablespace and self.connection.features.supports_tablespaces and field.unique:\n251:             sql += \" %s\" % self.connection.ops.tablespace_sql(tablespace, inline=True)\n252:         # Return the sql\n253:         return sql, params\n254: \n255:     def skip_default(self, field):\n256:         \"\"\"\n257:         Some backends don't accept default values for certain columns types\n258:         (i.e. MySQL longtext and longblob).\n259:         \"\"\"\n260:         return False\n261: \n262:     def prepare_default(self, value):\n263:         \"\"\"\n264:         Only used for backends which have requires_literal_defaults feature\n265:         \"\"\"\n266:         raise NotImplementedError(\n267:             'subclasses of BaseDatabaseSchemaEditor for backends which have '\n268:             'requires_literal_defaults must provide a prepare_default() method'\n269:         )\n270: \n271:     def _column_default_sql(self, field):\n272:         \"\"\"\n273:         Return the SQL to use in a DEFAULT clause. The resulting string should\n274:         contain a '%s' placeholder for a default value.\n275:         \"\"\"\n276:         return '%s'\n277: \n278:     @staticmethod\n279:     def _effective_default(field):\n280:         # This method allows testing its logic without a connection.\n281:         if field.has_default():\n282:             default = field.get_default()\n283:         elif not field.null and field.blank and field.empty_strings_allowed:\n284:             if field.get_internal_type() == \"BinaryField\":\n285:                 default = b''\n286:             else:\n287:                 default = ''\n288:         elif getattr(field, 'auto_now', False) or getattr(field, 'auto_now_add', False):\n289:             default = datetime.now()\n290:             internal_type = field.get_internal_type()\n291:             if internal_type == 'DateField':\n292:                 default = default.date()\n293:             elif internal_type == 'TimeField':\n294:                 default = default.time()\n295:             elif internal_type == 'DateTimeField':\n296:                 default = timezone.now()\n297:         else:\n298:             default = None\n299:         return default\n300: \n301:     def effective_default(self, field):\n302:         \"\"\"Return a field's effective database default value.\"\"\"\n303:         return field.get_db_prep_save(self._effective_default(field), self.connection)\n304: \n305:     def quote_value(self, value):\n306:         \"\"\"\n307:         Return a quoted version of the value so it's safe to use in an SQL\n308:         string. This is not safe against injection from user code; it is\n309:         intended only for use in making SQL scripts or preparing default values\n310:         for particularly tricky backends (defaults are not user-defined, though,\n311:         so this is safe).\n312:         \"\"\"\n313:         raise NotImplementedError()\n314: \n315:     # Actions\n316: \n317:     def create_model(self, model):\n318:         \"\"\"\n319:         Create a table and any accompanying indexes or unique constraints for\n320:         the given `model`.\n321:         \"\"\"\n322:         sql, params = self.table_sql(model)\n323:         # Prevent using [] as params, in the case a literal '%' is used in the definition\n324:         self.execute(sql, params or None)\n325: \n326:         # Add any field index and index_together's (deferred as SQLite _remake_table needs it)\n327:         self.deferred_sql.extend(self._model_indexes_sql(model))\n328: \n329:         # Make M2M tables\n330:         for field in model._meta.local_many_to_many:\n331:             if field.remote_field.through._meta.auto_created:\n332:                 self.create_model(field.remote_field.through)\n333: \n334:     def delete_model(self, model):\n335:         \"\"\"Delete a model from the database.\"\"\"\n336:         # Handle auto-created intermediary models\n337:         for field in model._meta.local_many_to_many:\n338:             if field.remote_field.through._meta.auto_created:\n339:                 self.delete_model(field.remote_field.through)\n340: \n341:         # Delete the table\n342:         self.execute(self.sql_delete_table % {\n343:             \"table\": self.quote_name(model._meta.db_table),\n344:         })\n345:         # Remove all deferred statements referencing the deleted table.\n346:         for sql in list(self.deferred_sql):\n347:             if isinstance(sql, Statement) and sql.references_table(model._meta.db_table):\n348:                 self.deferred_sql.remove(sql)\n349: \n350:     def add_index(self, model, index):\n351:         \"\"\"Add an index on a model.\"\"\"\n352:         self.execute(index.create_sql(model, self), params=None)\n353: \n354:     def remove_index(self, model, index):\n355:         \"\"\"Remove an index from a model.\"\"\"\n356:         self.execute(index.remove_sql(model, self))\n357: \n358:     def add_constraint(self, model, constraint):\n359:         \"\"\"Add a constraint to a model.\"\"\"\n360:         sql = constraint.create_sql(model, self)\n361:         if sql:\n362:             self.execute(sql)\n363: \n364:     def remove_constraint(self, model, constraint):\n365:         \"\"\"Remove a constraint from a model.\"\"\"\n366:         sql = constraint.remove_sql(model, self)\n367:         if sql:\n368:             self.execute(sql)\n369: \n370:     def alter_unique_together(self, model, old_unique_together, new_unique_together):\n371:         \"\"\"\n372:         Deal with a model changing its unique_together. The input\n373:         unique_togethers must be doubly-nested, not the single-nested\n374:         [\"foo\", \"bar\"] format.\n375:         \"\"\"\n376:         olds = {tuple(fields) for fields in old_unique_together}\n377:         news = {tuple(fields) for fields in new_unique_together}\n378:         # Deleted uniques\n379:         for fields in olds.difference(news):\n380:             self._delete_composed_index(model, fields, {'unique': True}, self.sql_delete_unique)\n381:         # Created uniques\n382:         for fields in news.difference(olds):\n383:             columns = [model._meta.get_field(field).column for field in fields]\n384:             self.execute(self._create_unique_sql(model, columns))\n385: \n386:     def alter_index_together(self, model, old_index_together, new_index_together):\n387:         \"\"\"\n388:         Deal with a model changing its index_together. The input\n389:         index_togethers must be doubly-nested, not the single-nested\n390:         [\"foo\", \"bar\"] format.\n391:         \"\"\"\n392:         olds = {tuple(fields) for fields in old_index_together}\n393:         news = {tuple(fields) for fields in new_index_together}\n394:         # Deleted indexes\n395:         for fields in olds.difference(news):\n396:             self._delete_composed_index(model, fields, {'index': True}, self.sql_delete_index)\n397:         # Created indexes\n... Code Truncated ...\n\n",
  "line_level_localization": [
    {
      "filename": "/django/db/backends/base/schema.py",
      "suspect_lines": [
        396
      ]
    }
  ]
}