{
  "instance_id": "scikit-learn__scikit-learn-11310",
  "problem_statement": "Retrieving time to refit the estimator in BaseSearchCV\nBasically, I'm trying to figure out how much time it takes to refit the best model on the full data after doing grid/random search. What I can so far do is retrieve the time it takes to fit and score each model:\r\n```\r\nimport sklearn.datasets\r\nimport sklearn.model_selection\r\nimport sklearn.ensemble\r\n\r\nX, y = sklearn.datasets.load_iris(return_X_y=True)\r\n\r\nrs = sklearn.model_selection.GridSearchCV(\r\n    estimator=sklearn.ensemble.RandomForestClassifier(),\r\n    param_grid={'n_estimators': [2, 3, 4, 5]}\r\n)\r\nrs.fit(X, y)\r\nprint(rs.cv_results_['mean_fit_time'])\r\nprint(rs.cv_results_['mean_score_time'])\r\n```\r\nIn case I run this on a single core, I could time the whole search procedure and subtract the time it took to fit the single folds during hyperparameter optimization. Nevertheless, this isn't possible any more when setting `n_jobs != 1`.\r\n\r\nThus, it would be great to have an attribute `refit_time_` which is simply the time it took to refit the best model.\r\n\r\nUsecase: for [OpenML.org](https://openml.org) we want to support uploading the results of hyperparameter optimization, including the time it takes to do the hyperparameter optimization. \n",
  "localized_code": "[start of sklearn/model_selection/_search.py]\n1: \"\"\"\n2: The :mod:`sklearn.model_selection._search` includes utilities to fine-tune the\n3: parameters of an estimator.\n4: \"\"\"\n5: from __future__ import print_function\n6: from __future__ import division\n7: \n8: # Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,\n9: #         Gael Varoquaux <gael.varoquaux@normalesup.org>\n10: #         Andreas Mueller <amueller@ais.uni-bonn.de>\n11: #         Olivier Grisel <olivier.grisel@ensta.org>\n12: #         Raghav RV <rvraghav93@gmail.com>\n13: # License: BSD 3 clause\n14: \n15: from abc import ABCMeta, abstractmethod\n16: from collections import Mapping, namedtuple, defaultdict, Sequence, Iterable\n17: from functools import partial, reduce\n18: from itertools import product\n19: import operator\n20: import warnings\n21: \n22: import numpy as np\n23: from scipy.stats import rankdata\n24: \n25: from ..base import BaseEstimator, is_classifier, clone\n26: from ..base import MetaEstimatorMixin\n27: from ._split import check_cv\n28: from ._validation import _fit_and_score\n29: from ._validation import _aggregate_score_dicts\n30: from ..exceptions import NotFittedError\n31: from ..externals.joblib import Parallel, delayed\n32: from ..externals import six\n33: from ..utils import check_random_state\n34: from ..utils.fixes import sp_version\n35: from ..utils.fixes import MaskedArray\n36: from ..utils.random import sample_without_replacement\n37: from ..utils.validation import indexable, check_is_fitted\n38: from ..utils.metaestimators import if_delegate_has_method\n39: from ..utils.deprecation import DeprecationDict\n40: from ..metrics.scorer import _check_multimetric_scoring\n41: from ..metrics.scorer import check_scoring\n42: \n43: \n44: __all__ = ['GridSearchCV', 'ParameterGrid', 'fit_grid_point',\n45:            'ParameterSampler', 'RandomizedSearchCV']\n46: \n47: \n48: class ParameterGrid(object):\n49:     \"\"\"Grid of parameters with a discrete number of values for each.\n50: \n51:     Can be used to iterate over parameter value combinations with the\n... Code Truncated ...\n\n",
  "line_level_localization": [
    {
      "filename": "/sklearn/model_selection/_search.py",
      "suspect_lines": []
    }
  ]
}